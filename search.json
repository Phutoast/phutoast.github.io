[
  {
    "objectID": "posts/3-yoneda-explore/index.html",
    "href": "posts/3-yoneda-explore/index.html",
    "title": "Yoneda Lemma: An Exploration (WIP)",
    "section": "",
    "text": "Required Knowledge: some conformation with what category is, for the latter section, you would need to understand the notion of functor and natural transformation.\n\nProbing Action"
  },
  {
    "objectID": "posts/2-kernel-test/index.html",
    "href": "posts/2-kernel-test/index.html",
    "title": "Kernel Statistical Test",
    "section": "",
    "text": "In this document, I would like to explain how the MMD and HSIC are derived (roughly). We will start with the brief introduction to RKHS, and then moving on to the statistical testing procedures. This is based on lecture note of the course Reproducing kernel Hilbert spaces in Machine Learning\n\n\nWe recall that the Hilbert space (HS) is a vector space that are equipped with an inner product between vectors and returns a scalar result. Reproducing Kernel HS (RKHS) is the Hilbert spaces that is equipped with the a kernel (that is constructed by the non-unique feature maps). Let’s unpack this, by starting from the definition of kernel\n\nDefinition (Kernel): Given the non-empty set \\(\\mathcal{X}\\), we define a kernel to be \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}\\) such that there is a Hilber space \\(\\mathcal{H}\\) and a function (called feature map) \\(\\phi:\\mathcal{X}\\rightarrow\\mathcal{H}\\) where: \\[k(x, y) = \\langle \\phi(x), \\phi(y)\\rangle_\\mathcal{H}\\] Noted that vector space \\(\\mathcal{H}\\) doesn’t need to be finite dimension (and so it can have infinite dimension i.e a function like object).\n\nThe good example of feature maps that doesn’t have to be unique is when:\n\\[\n\\phi_1(x) = x \\qquad \\phi_2(x) = \\begin{bmatrix}x/\\sqrt{2} \\\\ x/\\sqrt{2}\\end{bmatrix}\n\\]\nWe have the following way to construct a new kernel from the old one, given the fact that \\(k_1\\) and \\(k_2\\) are kernels, then we can show that, for any \\(x, y\\in\\mathcal{X}\\):\n\n\\(k_1(x, y)+k_2(x, y)\\)\n\\(k_1(x, y)*k_2(x, y)\\)\nFor \\(a\\in\\mathbb{R}\\), such that \\(ak_1(x,y)\\)\nFor any function \\(f:\\mathcal{X}\\rightarrow\\mathcal{X}'\\) (can be neural network or any kind of functions) and kernel \\(k':\\mathcal{X}'\\times\\mathcal{X}'\\rightarrow\\mathbb{R}\\), such that \\(k'(\\phi(x), \\phi(y))\\)\n\nare all kernel. With this would means that the following function \\(k(x, x') = (c + \\langle x, x'\\rangle)^m\\) is also a kernel, or if we have a function that admits Taylor series \\(f\\) (with convergences properties etc.), then \\(f(\\langle x, x'\\rangle)\\) is also a kernel.\nNow, we are ready to define the RKHS, in which it is a special Hilbert space with a special kind of kernel that satisfies additional\n\nDefinition (Reproducing Kernel Hilber Space): Given a Hilbert space \\(\\mathcal{H}\\) of \\(\\mathbb{R}\\) valued functions on non-empty set \\(\\mathcal{X}\\), the kernel \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}\\) is called reproducing and \\(\\mathcal{H}\\) is called RKHS if:\n\nFor all \\(x\\in\\mathcal{X}\\), \\(k(\\cdot, x) \\in\\mathcal{H}\\), then \\(k(\\cdot, x)\\in\\mathcal{H}\\)\nFor all \\(x\\in\\mathcal{X}\\), \\(\\langle{f(\\cdot), k(\\cdot,x)\\rangle}_\\mathcal{H} = f(x)\\)\n\n\nGiven the defintion, one can see that:\n\\[\n\\langle k(\\cdot, x), k(\\cdot, y)\\rangle_\\mathcal{H} = k(x, y)\n\\]\nwhich means that \\(k(\\cdot,x)\\) for any \\(x\\in\\mathcal{X}\\) can be seen as the feature map (recall that it doesn’t have to be unique), we will call this a canonical feature map.\nWe also have the follows result that illustrate why RKHS is preferable compared to the normal HS of functions.\n\nAdvanced Topics: Intuitively, we just say that the functions in RKHS acts “smoothly” and “predictably”, in the sense that: - If the distance between functions \\(\\|f-g\\|_\\mathcal{H}\\) is close to each other then its pointwise evaluation \\(|f(x)-g(x)|\\) for any \\(x\\) would also be close to each other. - This can be shown by the fact that the HS \\(\\mathcal{H}\\) has reproducing kernel iff the evaluation operator (defined as \\(\\delta_x : \\mathcal{H}\\rightarrow\\mathbb{R}\\) where \\(\\delta_x(f)=f(x)\\)) is bounded i.e \\(|\\delta_x(f)|\\le \\lambda_x\\|f\\|_\\mathcal{H}\\) for positive constant \\(\\lambda_x\\in\\mathbb{R}\\) (the proof uses the Riesz representation theorem)\n\nFurthermore, if the kernel satisfies the special property, then there is going to be an RHKS that is equipped with the given kernel as:\n\nTheorem (Moore-Aronszajn): A symmetric function \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow \\mathbb{R}\\) is positive definite if: for all \\(a_1,a_2,\\dots,a_n\\in \\mathbb{R}\\) and for all \\(x_1,x_2,\\dots,x_n\\in\\mathcal{X}\\): \\[\\sum^n_{i=1}\\sum^n_{j=1}a_ia_jk(x_i, x_j)\\ge0\\] If the kernel is positive definite, then there is a unique RKHS with the reproducing kernel \\(k\\).\n\n\n\n\nGiven the sample \\((x_i)^m_{i=1}\\sim p\\) and \\((y_i)^m_{i=1}\\sim q\\). Given any feature extraction function \\(\\phi\\), one can find related kernel \\(k(\\cdot,\\cdot)\\) to be: \\(k(a, b)=\\langle \\phi(a), \\phi(b)\\rangle\\). Therefore, the distance between their mean in a feature space of the kernel \\(k(\\cdot,\\cdot)\\) can be computed as:\n\\[\n\\begin{aligned}\n\\Bigg\\| \\frac{1}{m}&\\sum^m_{i=1}\\phi(x_i) - \\frac{1}{n}\\sum^n_{i=1}\\phi(y_i) \\Bigg\\|^2 \\\\\n&= \\frac{1}{m^2}\\sum^m_{i=1}\\sum^m_{j=1}k(x_i,x_j) + \\frac{1}{n^2}\\sum^n_{i=1}\\sum^n_{j=1}k(y_i, y_j) - \\frac{2}{mn}\\sum^m_{i=1}\\sum^n_{j=1}k(x_i, y_i)\n\\end{aligned}\n\\]\nWe can observe 2 things here: 1. If we set the feature extraction function to be \\(\\phi(a)=[a \\ a^2]\\), then we are able to compare both means and variance. 2. One can set the feature extraction function to be arbitrary, as long as one can find the appropriate corresponding kernel (that should be easier to compute than just an inner product of each other). For instance, with RBF, one can have feature extraction function with infinite features! (via Taylor series).\nTherefore, intuitively, we can perform a more power/non-linear relationship between samples. Let’s now move to the actual formulation of the statistical testing.\n\n\n\nIn this section, we are going to given the description of 2 main statistical testing technique that relies on the kernel method: MMD and HSIC (together with its variations). Let’s start with some operators that will be useful for both.\n\n\nGiven the example above in the interlude, we can generalizes the mean of the features map given an element \\(x\\sim P\\), as follows.\n\nDefinition (Mean Embedding): Given positive definite kernel \\(k(x,x')\\) with probability distribution \\(P\\) and \\(Q\\), we define \\(\\mu_P\\) and \\(\\mu_Q\\) such that: \\[\\langle{\\mu_P, \\mu_Q\\rangle} = \\mathbb{E}_{P, Q}[k(x, y)]\\] where \\(x\\sim P\\) and \\(y \\sim Q\\). We can consider the expectation in an RKHS as \\(\\mathbb{E}_P[f(x)] = \\langle{f, \\mu_P}\\rangle_\\mathcal{H}\\) for any function \\(f\\in\\mathcal{H}\\), the function in the corresponding RKHS\n\nWith this, one can see that the empirical mean embedding can be given in the form of:\n\\[\n\\hat{\\mu}_P = \\frac{1}{m}\\sum^m_{i=1}\\phi(x_i) \\qquad \\text{ where } \\qquad x_i\\sim P\n\\]\nIn which, one can show that this element exists.\n\nTheorem: The element \\(\\mu_P\\in\\mathcal{F}\\) defined as \\[\\mathbb{E}_P[f(x)] = \\langle{f, \\mu_P}\\rangle_\\mathcal{H}\\] if the kernel \\(k\\) of RKHS has the property that \\(\\mathbb{E}_P[\\sqrt{k(x, x)}]< \\infty\\)\n\nProof Sketch: We can use the Riesz representation theorem by showing that the operator \\(T_Pf=\\mathbb{E}_P[f(x)]\\) is boudned, and thus there is \\(\\mu_P\\) such that \\(T_Pf=\\langle f,\\mu_P\\rangle\\)).\n\n\n\nLet’s formally define the notion of MMD, which tries to answer the question, does the samples \\(\\{x_i\\}^n_{i=1}\\) and \\(\\{y_i\\}^n_{i=1}\\) comes from the same distribution or not ?\n\nDefinition (MMD): Now, we define the quantity of MMD being the distance between \\(2\\) probability distributions \\(P\\) and \\(Q\\) as (together with its, more computable form) \\[ \\begin{aligned} \\operatorname{MMD}^2&(P, Q) = \\|\\mu_P-\\mu_Q\\|^2_\\mathcal{F} \\\\ &= \\mathbb{E}_P[k(x, x')] + \\mathbb{E}_Q[k(y, y')] - 2\\mathbb{E}_{P, Q}[k(x, y)] \\end{aligned} \\] whereby, we have the following unbiased estimate of its quantity: \\[ \\widehat{\\operatorname{MMD}}^2(P, Q) = \\frac{1}{n(n-1)}\\sum_{i\\ne j}k(x_i, x_j) + \\frac{1}{n(n-1)}\\sum_{i\\ne j}k(y_i, y_j) - \\frac{2}{n^2}\\sum_{i,j}k(x_i, y_j) \\] for \\(x_i\\sim P\\) and \\(y_i\\sim Q\\)\n\nYou may wonder, why does MMD is called maximum mean discrepancy ? One can show MMD can be written in an alternative form of:\n\nTheorem: We can show that the MMD can be written in an alternative form of: \\[\\operatorname{MMD}(P, Q) = \\sup_{\\|f\\|\\le1}\\big(\\mathbb{E}_P[f(x)] - \\mathbb{E}_Q[f(x)]\\big)\\]\n\nThis can be interpreted as, given “smooth” function within a ball (therefore not being too extream), we find such a function that maximally distingush the sample of \\(P\\) and \\(Q\\), and the maximum disagreement is the MMD value.\nNow, back to the statistical testing, we can show that the value of MMD will have the following asympototics distribution of:\n\nTheorem: We have the following distribution of the empirical MMD statistics as follows: - When \\(P\\ne Q\\), we have: \\[\\frac{\\widehat{\\operatorname{MMD}}^2 - \\operatorname{MMD}(P, Q)^2}{\\sqrt{V_n(P, Q)}} \\xrightarrow{D} \\mathcal{N}(0, 1)\\]where the variance \\(V_n(P, Q) = \\mathcal{O}(n^{-1})\\) but depends on the chosen kernel. - When \\(P=Q\\), we have: \\[n\\widehat{\\operatorname{MMD}}^2 \\sim \\sum^\\infty_{l=1} \\lambda_l[z^2_l - 2] \\qquad \\text{ where } \\qquad \\lambda_i\\phi_i(x) = \\int_\\mathcal{X}\\widetilde{k}(x,\\widetilde{x})\\phi_i(x)\\text{ d}P(x)\\]where \\(\\widetilde{k}\\) is a centered kernel and \\(z_l\\sim\\mathcal{N}(0, 2)\\)\n\nHowever, to compute such a distribution with null-hypothesis \\(P=Q\\) in closed form is hard, therefore:\n\nWe have to rely on using a boostrap method which is done by permuting the set \\(X\\) and \\(Y\\) before testing (i.e mixing them up)\nThis would gives us the estimate of the MMD statistics when \\(P=Q\\), which can them be used to compute the threshold for statistical test.\n\nNow, to find a best kernel, we have that:\n\nRemark (Finding a best kernek): Given the distribution when \\(P=Q\\), one can see that the power of the test is given to be: \\[\\text{Pr}_1\\left({n\\widehat{\\operatorname{MMD}} > \\hat{c}_\\alpha }\\right) \\rightarrow  \\Phi\\left({\\frac{\\operatorname{MMD}^2(P, Q)}{\\sqrt{V_n(P, Q)}} - \\frac{c_\\alpha}{n\\sqrt{V_n(P, Q)}} }\\right)\\] To find the best kernel, we can find the kernel that maximize the test power. We would like to note the following: \\[\\frac{\\operatorname{MMD}^2(P, Q)}{\\sqrt{V_n(P, Q)}} = \\mathcal{O}(\\sqrt{n}) \\qquad \\frac{c_\\alpha}{n\\sqrt{V_n(P, Q)}} =\\mathcal{O}(n^{-1/2})\\] therefore, we can ignore the second term, and we can maximize the first term only, by setting this to be the objective of the neural network (that perform the feature extraction of the kernel). Note that we can derive the estimator for \\(V_n\\) too.\n\nFor example, one can use the following kernel from\n\\[\nk_\\theta(x, y) = \\big[(1-\\varepsilon)\\kappa(\\Phi_\\theta(x), \\Phi_\\theta(y))+\\varepsilon\\big]q(x, y)\n\\]\nwhere \\(\\Phi_\\theta\\) is a neural network and \\(\\kappa\\) and \\(q\\) are Gaussian kernel, which is able to distinguish between CIFAR-10 vs CIFAR-10 (image dataset).\nNow come the more important question, can we use any kernel to give us the appropriate MMD test ? The answer is obviously no, but what kind of kernel would be approriate ? Starting with a defintion of a good kernel (or we will call it characteristic):\n\nDefinition (Charateristic kernel): A RKHS (with corresponding kernel) is called characteristic if \\(\\operatorname{MMD}(P, Q; \\mathcal{F}) = 0\\) iff \\(P = Q\\)\n\nThat is when \\(P\\) and \\(Q\\) are the same, the value of MMD should be zero. What would be an appropriate kernel ? In this case, we would like to assume that kernel that we are working on is Translation Invariance i.e\n\nDefinition (Translation Invariance): The kernel \\(k\\) is called Translation Invariance if there is a function \\(f\\) such that: \\[k(x,y)=f(x-y)\\] for any \\(x\\) and \\(y\\)\n\nThen, one can have a fourier representation/coefficient of the kernel to be (assume we are within the domain of \\([-\\pi,\\pi]\\)) the multiple within the fourier series expansion:\n\\[\nk(x, y) = \\sum^\\infty_{l=-\\infty} \\hat{k}_l \\exp(il(x-y)) = \\sum^\\infty_{l=-\\infty}\\underbrace{\\left[{\\sqrt{\\hat{k}_l} \\exp(ilx) }\\right]}_{\\phi_l(x)}\\underbrace{\\left[{\\sqrt{\\hat{k}_l}\\exp(-ily)}\\right]}_{\\overline{\\phi_l(y)}}\n\\]\n\\(\\hat{k}_l\\) is called the fourier coefficient of the kernel. For the probability distribution, one can also have a similar way to find the fourier coefficient of them. We have the following result.\n\nTheorem: The value of MMD can be written as: \\[\\operatorname{MMD}^2(P, Q;\\mathcal{F}) = \\sum^\\infty_{l=-\\infty} |\\phi_{P,l} - \\phi_{Q, l}|^2\\hat{k}_l\\] for \\(\\hat{k}_l\\) being the fourier coefficient of the kernel, \\(\\phi_{P,l}\\) and \\(\\phi_{Q,l}\\) are fourier coefficient of the probability distributions \\(P\\) and \\(Q\\), respectively.\n\nTherefore, the kernel is characterisic iff none of the \\(\\hat{k}_l\\) is equal to zero.\nOn the other hand, instead of considering within specific range \\([\\pi,-\\pi]\\), one can also define the RKHS to be universal, which is when:\n\nDefinition (Universal RKHS): Given RKHS, it is universal if when: - \\(k(x, x')\\) is continuous - \\(\\mathcal{X}\\) is compact. - \\(\\mathcal{F}\\) is dense in \\(C(\\mathcal{X})\\) wrt. \\(L_\\infty\\) i.e for \\(\\varepsilon>0\\) and \\(f\\in C(\\mathcal{X})\\), there is \\(g\\in\\mathcal{F}\\) such that: \\[\\|f-g\\|_\\infty\\le\\varepsilon\\]\n\nin which we can show that:\n\nTheorem: If \\(\\mathcal{F}\\) is universal then \\(\\operatorname{MMD}(P, Q;\\mathcal{F}) = 0\\) iff \\(P = Q\\)\n\n\n\n\n\nNow, we are interested in given a pair of variables \\(\\{(x_i, y_i)\\}^n_{i=1}\\sim P_{XY}\\) are they dependent of each other ? - Usually one can use the MMD to find the differences whether this sample is sampled from the \\(P_XP_Y\\) (i.e product of marginal distribution). However, we don’t have an access to this. - Another question is: which kind of kernel would we be use ? is it a product kernel ? or different kind of kernels\n\n\nWe start off by defining the tensor product between elements in the Hilber space.\n\nDefinition (Tensor Product): Given element \\(a,b,c\\in\\mathcal{H}\\) of the Hilbert space, the tensor product between \\(a\\) and \\(b\\) is denoted as \\(a\\otimes b\\) such that: \\[(a\\otimes b)c = \\langle b,c\\rangle_\\mathcal{H}a\\] Note that this is analogous to when \\(a,b\\) and \\(c\\) are vector, then \\((ab^\\top)c=b^\\top ca\\)\n\nNow, we would like to extends the notion of the inner product (and norm) to the linear transformation between Hilbert space. This would gives us Hilbert-Schmidt Operators i.e\n\nDefinition (Hilbert-Schmidt Operators): Given a separable (countable orthonormal basis Hilbert spaces \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\) with orthonormal basis \\((f_i)_{i\\in I}\\) and \\((g_j)_{j\\in I}\\), respectively and 2 linear transformation between them: \\(L:\\mathcal{G}\\rightarrow\\mathcal{F}\\) and \\(M:\\mathcal{G}\\rightarrow\\mathcal{F}\\): \\[\\langle{L, M}\\rangle_{\\operatorname{HS}} = \\sum_{j\\in J}\\langle{Lg_j, Mg_j}\\rangle_\\mathcal{F}\\]\n\nNow, we can define the covariance operator (in similar manners to the mean embedding) as:\n\nDefinition (Covariance Operator): The covariance operators \\(C_{xy} : \\mathcal{G} \\rightarrow \\mathcal{F}\\) is given by: \\[\\langle{f, C_{xy}g}\\rangle_\\mathcal{F} = \\mathbb{E}_{xy}[f(x)g(y)]\\] which we can show to exists if the kernel associated \\(\\mathcal{G}\\) and \\(\\mathcal{F}\\): \\(k_1\\) and \\(k_2\\), respectively, are such that \\(k_1(x,x) < \\infty\\) and \\(k_2(y,y)<\\infty\\)\n\nThe existances can be proven by observe that, for any linear operator \\(A:\\mathcal{G} \\rightarrow \\mathcal{F}\\), we have:\n\\[\n\\langle{C_{xy}, A}\\rangle_{\\operatorname{HS}} = \\mathbb{E}_{xy}\\big[\\langle\\psi(x)\\otimes\\phi(y), A\\rangle_{\\operatorname{HS}}\\big]\n\\]\nand so we can use Riesz representation thoerem to proof the existence. Then, we are ready to define the HSIC\n\n\n\n\nDefinition (Hilbert-Schmidt Indepdent Criterion): The HSIC can be seen as the norm of the centered covariance operator i.e: \\[\\operatorname{HSIC}(P_{XY};\\mathcal{F}, \\mathcal{G}) = \\|{C_{xy} - \\mu_x\\otimes\\mu_y}\\|_{\\operatorname{HS}} = \\|{\\widetilde{C}_{xy}}\\|_{\\operatorname{HS}}\\]\n\nIn relation to MMD, we can show that"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html",
    "href": "posts/1-GiHKAL/index.html",
    "title": "Gaussian That I Have Known and Loved",
    "section": "",
    "text": "This write is based from the book: Pattern Recognition and Machine Learning, partly from CS229 notes on Guassian, and The Principles of Deep Learning Theory: An Effective Theory Approach to Understanding Neural Networks (especially on the chapter on Wick’s theorem, in which in this written we have provided the exposition on the proof too), where we aim to give and expands most proofs and interesting results regarding Gaussian distributions, together with some results that makes the complete picture."
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#gaussian-definition-and-fundamental-integral",
    "href": "posts/1-GiHKAL/index.html#gaussian-definition-and-fundamental-integral",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Gaussian Definition and Fundamental Integral",
    "text": "Gaussian Definition and Fundamental Integral\nDefinition (Single Variable Gaussian Distribution): The Gaussian distribution is defined as\n\n\\[\\begin{equation*}\n\\newcommand{\\dby}{\\ \\mathrm{d}}\\newcommand{\\argmax}[1]{\\underset{#1}{\\arg\\max \\ }}\\newcommand{\\argmin}[1]{\\underset{#1}{\\arg\\min \\ }}\\newcommand{\\const}{\\text{const.}}\\newcommand{\\bracka}[1]{\\left( #1 \\right)}\\newcommand{\\brackb}[1]{\\left[ #1 \\right]}\\newcommand{\\brackc}[1]{\\left\\{ #1 \\right\\}}\\newcommand{\\brackd}[1]{\\left\\langle #1 \\right\\rangle}\\newcommand{\\correctquote}[1]{``#1''}\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\\newcommand{\\abs}[1]{\\left|#1\\right|}\n    p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{ -\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right\\}\n\\end{equation*}\\]\n\nwhere \\(\\mu\\) is a parameter called mean, while \\(\\sigma\\) is a parameter called standard derivation. However, we also define \\(\\sigma^2\\) as variance. Finally, the standard normal distribution is Gaussian distribution with mean \\(0\\) and variance \\(1\\).\nProposition (Gaussian integral): The integration of \\(\\exp(-x^2)\\) is equal to\n\n\\[\\begin{equation*}\n    \\int^\\infty_{-\\infty}\\exp(-x^2)\\dby x = \\sqrt{\\pi}\n\\end{equation*}\\]\n\nWe can use this identity to find the normalizing factor of Gaussian distribution.\nProof: We will transform the problem into \\(2\\)D polar coordinate system, which make the integration easier.\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\bracka{\\int^\\infty_{-\\infty}\\exp(-x^2)\\dby x}^2 &= \\bracka{\\int^\\infty_{-\\infty}\\exp(-x^2)\\dby x}\\bracka{\\int^\\infty_{-\\infty}\\exp(-y^2)\\dby y} \\\\\n&= \\int^\\infty_{-\\infty}\\int^\\infty_{-\\infty}\\exp\\bracka{-(x^2+y^2)}\\dby x\\dby y \\\\\n&= \\int^{2\\pi}_{0}\\int^\\infty_{0}\\exp\\bracka{-r^2}r\\dby r\\dby\\theta \\\\\n&= 2\\pi\\int^\\infty_{0}\\exp\\bracka{-r^2}r\\dby r = \\pi\\int^{0}_{-\\infty} \\exp\\bracka{u}\\dby u = \\pi\n\\end{aligned}\n\\end{equation*}\\]\n\nPlease note that we use \\(u\\)-substution with \\(-r^2\\), in the last step.\n\n□\n\nCorollary (Normalization of Gaussian Distribution): We consider the integration\n\n\\[\\begin{equation*}\n    \\int^\\infty_{-\\infty}\\exp\\left\\{ -\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\} \\dby x = \\sqrt{2\\pi\\sigma^2}\n\\end{equation*}\\]\n\nProof: Starting by setting \\(z=x-\\mu\\), which we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\int^\\infty_{-\\infty}\\exp\\left\\{ -\\frac{z^2}{2\\sigma^2}\\right\\} \\dby z &= \\sigma\\sqrt{2} \\int^\\infty_{-\\infty}\\frac{1}{\\sqrt{2}\\sigma}\\exp\\left\\{ -\\frac{z^2}{2\\sigma^2}\\right\\} \\dby z \\\\\n&= \\sigma\\sqrt{2} \\int^\\infty_{-\\infty} \\exp(-y^2)\\dby = \\sigma\\sqrt{2\\pi}\n\\end{aligned}\n\\end{equation*}\\]\n\nwhere we have \\(y = z/(\\sqrt{2}\\sigma)\\), and we finishes the proof.\n\n□"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#statistics-of-single-variable-gaussian-distribution",
    "href": "posts/1-GiHKAL/index.html#statistics-of-single-variable-gaussian-distribution",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Statistics of Single Variable Gaussian Distribution",
    "text": "Statistics of Single Variable Gaussian Distribution\nDefinition (Odd Function): The function \\(f(x)\\) is an odd function iff \\(f(-x) = -x\\)\nLemma (Odd Function Integration): The integral over \\([-a, a]\\), where \\(a\\in\\mathbb{R}^+\\) of an odd function \\(f(x)\\) is \\(0\\) i.e\n\n\\[\\begin{equation*}\\int^a_{-a}f(x)\\dby x = 0\\end{equation*}\\]\n\nProof: We can see that:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\int^a_{-a}f(x)\\dby x &= \\int^0_{-a}f(x)\\dby x + \\int^a_{0}f(x)\\dby x \\\\\n    &= \\int^a_{0}f(-x)\\dby x + \\int^a_{0}f(x)\\dby x \\\\\n    &= -\\int^a_{0}f(x)\\dby x + \\int^a_{0}f(x)\\dby x = 0\n\\end{aligned}\n\\end{equation*}\\]\n\nThus complete the proof\n\n□\n\nProposition (Mean of Gaussian): The expectation \\(\\mathbb{E}_{x}[x]\\) of nornally distributed Gaussian is \\(\\mu\\)\nProof: Let’s consider the following integral\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{(x-\\mu)^2}{2\\sigma^2}}x\\dby x\n\\end{aligned}\n\\end{equation*}\\]\n\nWe will set \\(z = x - \\mu\\), where we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\frac{1}{\\sqrt{2\\pi\\sigma^2}}&\\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}(z+\\mu)\\dby x \\\\\n    &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\Bigg[ \\underbrace{\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z\\dby z}_{I_1} + \\underbrace{\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\mu\\dby z}_{I_2} \\Bigg]\n\\end{aligned}\n\\end{equation*}\\]\n\nLet’s consider \\(I_1\\), where it is clear that\n\n\\[\\begin{equation*}\n    g(x) = \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z = -\\bracka{-\\exp\\bracka{-\\frac{(-z)^2}{2\\sigma^2}}z} = -g(-x)\n\\end{equation*}\\]\n\nThus the function \\(g(x)\\) is and odd function. Therefore, making the integration \\(I_1\\) vanishes to \\(0\\). Please see the Lemma above for the proof. Now, for the second integration, we can simply recall the normalization result of the Gaussian, where\n\n\\[\\begin{equation*}\n    \\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\mu\\dby z = \\mu\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\dby z = \\mu\\sqrt{2\\pi\\sigma^2}\n\\end{equation*}\\]\n\nFinally, we have\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}(z+\\mu)\\dby x = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\Bigg[ 0 + \\mu\\sqrt{2\\pi\\sigma^2} \\Bigg] = \\mu\n\\end{aligned}\n\\end{equation*}\\]\n\nThus complete the proof.\n\n□\n\nLemma: The variance \\(\\operatorname{var}(x) = \\mathbb{E}[(x - \\mu)^2]\\) is equal to \\(\\mathbb{E}[x^2] - \\mathbb{E}[x]^2\\)\nProof: This is an application of expanding the definition\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\mathbb{E}[(x - \\mu)^2] &= \\mathbb{E}[x^2 -2x\\mu + \\mu^2] \\\\\n    &= \\mathbb{E}[x^2] - 2\\mathbb{E}[x]\\mu + \\mathbb{E}[\\mu^2] \\\\\n    &= \\mathbb{E}[x^2] - 2\\mathbb{E}[x]^2 + \\mathbb{E}[x]^2 \\\\\n    &= \\mathbb{E}[x^2] - \\mathbb{E}[x]^2 \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\n\n□\n\nProposition: The variance of normal distribution is \\(\\sigma^2\\)\nProof: Let’s consider the following equation, where we set \\(z = x-\\mu\\):\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\frac{1}{\\sqrt{2\\pi\\sigma^2}}& \\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{(x-\\mu)^2}{2\\sigma^2}}x^2\\dby x  = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}(z+\\mu)^2\\dby z \\\\\n    &=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\brackb{\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z^2\\dby z + \\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}2\\mu z\\dby z + \\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\mu^2\\dby z } \\\\\n    &=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\brackb{\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z^2\\dby z + 0 + \\mu^2\\sqrt{2\\pi\\sigma^2} \\dby z }\n\\end{aligned}\n\\end{equation*}\\]\n\nNow let’s consider the first integral, please note that\n\n\\[\\begin{equation*}\n    \\frac{d}{dz} \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}} = \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\bracka{-\\frac{z}{\\sigma^2}}\n\\end{equation*}\\]\n\nSo we can perform an integration by-part considering \\(u=z\\) and \\(dv = \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\bracka{-\\frac{z}{\\sigma^2}}\\):\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\int^\\infty_{-\\infty}&\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z^2\\dby z = -\\sigma^2\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\bracka{-\\frac{z}{\\sigma^2}}z\\dby z \\\\\n    &= -\\sigma^2\\brackb{\\left.z\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\right|^\\infty_{-\\infty} - \\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\dby z} \\\\\n    &= -\\sigma^2[0 - \\sqrt{2\\pi\\sigma^2}] = \\sigma^2\\sqrt{2\\pi\\sigma^2}\n\\end{aligned}\n\\end{equation*}\\]\n\nTo show that the evaluation on the left-hand side is zero, we have\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\lim_{z\\rightarrow\\infty} z\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}} &- \\lim_{z\\rightarrow-\\infty} z\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}} \\\\\n    &= 0 - \\lim_{z\\rightarrow-\\infty}1\\cdot\\exp\\bracka{-\\frac{x^2}{2\\sigma^2}}\\bracka{-\\frac{\\sigma^2}{z}} \\\\\n    &= 0-0 = 0\n\\end{aligned}\n\\end{equation*}\\]\n\nThe first equality comes from L’Hospital’s rule. Combinding the results:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\mathbb{E}[x^2] &- \\mathbb{E}[x]^2 = \\sigma^2 + \\mu^2 - \\mu^2 = \\sigma^2\n\\end{aligned}\n\\end{equation*}\\]\n\nThus complete the proof.\n\n□\n\nThis second part is to introduce some of the mathematical basics such as linear algebra. Further results of linear Gaussian models and others will be presented in the next part."
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#useful-backgrounds",
    "href": "posts/1-GiHKAL/index.html#useful-backgrounds",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Useful Backgrounds",
    "text": "Useful Backgrounds\n\nCovariance and Covariance Matrix\nDefinition (Covariance): Given 2 random variables \\(X\\) and \\(Y\\), the covariance is defined as:\n\n\\[\\begin{equation*}\n\\newcommand{\\dby}{\\ \\mathrm{d}}\\newcommand{\\argmax}[1]{\\underset{#1}{\\arg\\max \\ }}\\newcommand{\\argmin}[1]{\\underset{#1}{\\arg\\min \\ }}\\newcommand{\\const}{\\text{const.}}\\newcommand{\\bracka}[1]{\\left( #1 \\right)}\\newcommand{\\brackb}[1]{\\left[ #1 \\right]}\\newcommand{\\brackc}[1]{\\left\\{ #1 \\right\\}}\\newcommand{\\brackd}[1]{\\left\\langle #1 \\right\\rangle}\\newcommand{\\correctquote}[1]{``#1''}\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\operatorname{cov}(X, Y) = \\mathbb{E}\\Big[ (X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y]) \\Big]\n\\end{equation*}\\]\n\nIt is clear that $(X, Y) = (Y, X) $\nDefinition (Covariance Matrix): Now, if we have the collection of random variables in the random vector \\(\\boldsymbol x\\) (of size \\(n\\)), then the collection of covariance between its elements are collected in covariance matrix\n\n\\[\\begin{equation*}\n\\operatorname{cov}(\\boldsymbol x) =\n\\begin{bmatrix}\n    \\operatorname{cov}(x_1, x_1) & \\operatorname{cov}(x_1, x_2) & \\cdots & \\operatorname{cov}(x_1, x_n)  \\\\\n    \\operatorname{cov}(x_2, x_1) & \\operatorname{cov}(x_2, x_2) & \\cdots & \\operatorname{cov}(x_2, x_n)  \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\operatorname{cov}(x_n, x_1) & \\operatorname{cov}(x_n, x_2) & \\cdots & \\operatorname{cov}(x_n, x_n)  \\\\\n\\end{bmatrix} = \\mathbb{E}\\Big[ (\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])(\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])^T \\Big]\n\\end{equation*}\\]\n\nThe equivalent is clear when we perform the matrix multiplication over this.\nRemark (Property of Covariance Matrix): It is clear that the covariance matrix is (from its defintion):\n\nSymmetric, as \\(\\operatorname{cov}(x_a, x_b) = \\operatorname{cov}(x_b, x_a)\\)\nPositive semidefinite, if we consider arbitary constant vector \\(\\boldsymbol a \\in \\mathbb{R}^n\\), then by the linearity of expectation, we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol a^T \\operatorname{cov}(\\boldsymbol x)\\boldsymbol a\n&= \\boldsymbol a^T \\mathbb{E}\\Big[ (\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])(\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])^T \\Big]\\boldsymbol a  \\\\\n&= \\mathbb{E}\\Big[ \\boldsymbol a^T (\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])(\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])^T\\boldsymbol a \\Big]  \\\\\n&= \\mathbb{E}\\Big[ \\big(\\boldsymbol a^T (\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])\\big)^2 \\Big] > 0 \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\n\n\n\nLinear Algebra\nDefinition (Eigenvalues and Eigenvectors): Given the matrix \\(\\boldsymbol X \\in \\mathbb{C}^{n\\times n}\\), then the pair of vector \\(\\boldsymbol v \\in \\mathbb{C}^n\\) and number \\(\\lambda \\in \\mathbb{C}\\) are called eigenvector and eigenvalue iff\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol A \\boldsymbol v = \\lambda\\boldsymbol v\n\\end{aligned}\n\\end{equation*}\\]\n\nProposition (Eigenvalue of Symmetric Matrix): One can show that the eigenvalue of symmetric (real) matrix is always real.\nProof (From here): Let’s consider the pairs of eigenvalues/eigenvectors \\(\\boldsymbol v \\in \\mathbb{C}^n\\) and \\(\\lambda \\in \\mathbb{C}\\) of symmetric (real) matrix \\(\\boldsymbol A\\) i.e \\(\\boldsymbol A\\boldsymbol v = \\lambda\\boldsymbol v\\). Please note that \\((x+yi)(x-yi) = x^2 + y^2 \\ge 0\\), this means that \\(\\bar{\\boldsymbol v}^T\\boldsymbol v \\ge 0\\) (\\(\\bar{\\boldsymbol v}\\) is vector that contains conjugate element of \\(\\boldsymbol v\\)). Now, see that:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\bar{\\boldsymbol v}^T\\boldsymbol A\\boldsymbol v\n&= \\bar{\\boldsymbol v}^T(\\boldsymbol A\\boldsymbol v) = \\lambda\\bar{\\boldsymbol v}^T\\boldsymbol v  \\\\\n&= (\\bar{\\boldsymbol v}^T\\boldsymbol A^T)\\boldsymbol v = \\bar{\\lambda}\\bar{\\boldsymbol v}^T\\boldsymbol v  \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nNote that \\(\\overline{\\boldsymbol A\\boldsymbol v} = \\bar{\\lambda}\\bar{\\boldsymbol v}\\) Since \\(\\bar{\\boldsymbol v}^T\\boldsymbol v > 0\\) we see that \\(\\bar{\\lambda} = \\lambda\\), which implies that \\(\\bar{\\lambda}\\) is real.\n\n□\n\nProposition (Eigenvalue of Positive Definite Matrix): One can show that the eigenvalue of positive definite matrix is non-negative.\nProof: We consider the following equations:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol v^T\\boldsymbol A\\boldsymbol v = \\lambda\\boldsymbol v^T\\boldsymbol v > 0\n\\end{aligned}\n\\end{equation*}\\]\n\nAnd so the eigenvector must be \\(\\lambda > 0\\).\n\n□\n\nDefinition (Linear Transformation): Given the function \\(A: V \\rightarrow W\\), where \\(V\\) and \\(W\\) are vector spaces. Then, for \\(\\boldsymbol v \\in V, \\boldsymbol w \\in W\\) and \\(a, b \\in \\mathbb{R}\\)\n\n\\[\\begin{equation*}\n\\begin{aligned}\nA(a\\boldsymbol v + b\\boldsymbol w) = aA(\\boldsymbol v) + bA(\\boldsymbol w)\n\\end{aligned}\n\\end{equation*}\\]\n\nRemark (Matrix Multipliacation and Linear Transformation): We can represent the linear transformation \\(L : V \\rightarrow W\\) in terms of matrix. Let’s consider the vector \\(\\boldsymbol v \\in V\\) (of dimension \\(n\\)) together with basis vectors \\(\\brackc{\\boldsymbol b_1,\\dots,\\boldsymbol b_n}\\) of \\(V\\) and basis vectors \\(\\brackc{\\boldsymbol c_1, \\dots, \\boldsymbol c_m}\\) of \\(W\\). Then we can represen the vector \\(\\boldsymbol v\\) as:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol v = v_1\\boldsymbol b_1 + \\dots + v_n\\boldsymbol b_n\n\\end{aligned}\n\\end{equation*}\\]\n\nThis means that we can represent the vector \\(\\boldsymbol v\\) as: \\((\\boldsymbol v_1, \\boldsymbol v_2, \\dots, \\boldsymbol v_n)^T\\) Furthermore, we can characterized the transformation of the basis vector:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    &L(\\boldsymbol b_i) = l_{1i} \\boldsymbol c_1 + l_{2i}\\boldsymbol c_2 + \\cdots + l_{mi}\\boldsymbol c_m \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nfor \\(i=1,\\dots,n\\). Then we can see that the definition of linear transformation together with the linear transformation of basis as we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    L(\\boldsymbol v) &= v_1L(\\boldsymbol b_1) + \\dots + v_nL(\\boldsymbol v_n) \\\\\n    &= \\begin{aligned}[t]\n        &v_1\\Big( l_{11} \\boldsymbol c_1 + l_{21}\\boldsymbol c_2 + \\cdots + l_{m1}\\boldsymbol c_m \\Big) \\\\\n        &+v_2\\Big( l_{12} \\boldsymbol c_1 + l_{22}\\boldsymbol c_2 + \\cdots + l_{m2}\\boldsymbol c_m \\Big) \\\\\n        &+v_n\\Big( l_{1n} \\boldsymbol c_1 + l_{2n}\\boldsymbol c_2 + \\cdots + l_{mn}\\boldsymbol c_m \\Big)\\\\\n    \\end{aligned} \\\\\n    &= \\begin{bmatrix}\n        \\sum^n_{i=1}v_il_{1i} \\\\ \\sum^n_{i=1}v_il_{2i} \\\\ \\vdots \\\\ \\sum^n_{i=1}v_il_{ni}\n    \\end{bmatrix} = \\begin{bmatrix}\n        l_{11} & l_{12} & \\cdots & l_{1n} \\\\\n        l_{21} & l_{22} & \\cdots & l_{2n} \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        l_{m1} & l_{m2} & \\cdots & l_{mn} \\\\\n    \\end{bmatrix}\n    \\begin{bmatrix}\n        v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n\n    \\end{bmatrix}\n\\end{aligned}\n\\end{equation*}\\]\n\nand so we have show that the linear transformation can be represented as matrix multiplication (in finite space).\nTheorem (Spectral Theorem) (follows from here): Let \\(n \\le N\\) and \\(W\\) be an n-dimensional subspace of \\(\\mathbb{R}^n\\). Given a linear transformation \\(A:W\\rightarrow W\\) that is symmetric. There are eigenvectors \\(\\boldsymbol v_1,\\dots,\\boldsymbol v_n\\in W\\) of \\(A\\) such that \\(\\brackc{\\boldsymbol v_1,\\dots,\\boldsymbol v_n}\\) is an orthonormal basis for \\(W\\). For normal matrix, we let \\(n=N\\) and \\(W=\\mathbb{R}^n\\).\nRemark (Eigendecomposition): Let’s consider the matrix of eigenvectors \\(\\boldsymbol v_1,\\boldsymbol v_2\\dots,\\boldsymbol v_n \\in \\mathbb{R}^n\\) of symmetric matrix \\(\\boldsymbol A \\in \\mathbb{R}^{n\\times n}\\) together with eigenvalues \\(\\lambda_1,\\lambda_2,\\dots,\\lambda_n\\), then we have :\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\boldsymbol A\n    \\begin{bmatrix}\n        \\kern.6em\\vline & \\kern.2em\\vline\\kern.2em & & \\vline\\kern.6em \\\\\n        \\kern.6em\\boldsymbol v_1 & \\kern.2em\\boldsymbol v_2\\kern.2em & \\kern.2em\\cdots\\kern.2em &  \\boldsymbol v_n\\kern.6em  \\\\\n        \\kern.6em\\vline & \\kern.2em\\vline\\kern.2em & & \\vline\\kern.6em \\\\\n    \\end{bmatrix} &=\n    \\begin{bmatrix}\n        \\kern.6em\\vline & \\kern.2em\\vline\\kern.2em & & \\vline\\kern.6em \\\\\n        \\kern.6em\\boldsymbol v_1 & \\kern.2em\\boldsymbol v_2\\kern.2em & \\kern.2em\\cdots\\kern.2em &  \\boldsymbol v_n\\kern.6em  \\\\\n        \\kern.6em\\vline & \\kern.2em\\vline\\kern.2em & & \\vline\\kern.6em \\\\\n    \\end{bmatrix}\\begin{bmatrix}\n        \\lambda_1 & 0 & \\cdots & 0 \\\\\n        0 & \\lambda_2 & \\cdots & 0 \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & \\lambda_n \\\\\n    \\end{bmatrix} \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nThis is equivalent to \\(\\boldsymbol A\\boldsymbol Q = \\boldsymbol Q\\boldsymbol \\Lambda\\), which mean that if we right multiply by \\(\\boldsymbol Q^T\\) (as we have orthogonal eigenvectors), then we have (or in vectorized format):\n\n\\[\\begin{equation*}\n\\boldsymbol A = \\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^T = \\sum^n_{i=1}\\lambda_i\\boldsymbol v_i\\boldsymbol v_i^T\n\\end{equation*}\\]\n\nPlease note that \\(\\boldsymbol A^{-1}\\) can be represented as:\n\n\\[\\begin{equation*}\n\\boldsymbol A^{-1} = \\boldsymbol Q\\boldsymbol \\Lambda^{-1}\\boldsymbol Q^T = \\sum^n_{i=1}\\frac{1}{\\lambda_i}\\boldsymbol v_i\\boldsymbol v_i^T\n\\end{equation*}\\]\n\nas it is clear that \\(\\boldsymbol A\\boldsymbol A^{-1} = \\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^T\\boldsymbol Q\\boldsymbol \\Lambda^{-1}\\boldsymbol Q^T = \\boldsymbol I\\).\nProposition (Determinant and Eigenvalues): Give matrix \\(\\boldsymbol A\\) together with eigenvalues of \\(\\lambda_1,\\lambda_2,\\dots,\\lambda_n\\), then we can show that\n\n\\[\\begin{equation*}\n    \\abs{\\boldsymbol A} = \\prod^n_{i=1}\\lambda_i\n\\end{equation*}\\]\n\nProof (Diagonalizable Matrix): We consider the eigendecomposition of \\(\\boldsymbol A\\) (if it exists, which is most of the cases here. For more general proof, see linear algebra notes), as we have:\n\n\\[\\begin{equation*}\n    \\abs{\\boldsymbol A} = \\abs{\\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^{-1}} = \\abs{\\boldsymbol Q}\\abs{\\boldsymbol \\Lambda}\\abs{\\boldsymbol Q^{-1}} = \\frac{\\abs{\\boldsymbol Q}}{\\abs{\\boldsymbol Q}}\\abs{\\boldsymbol \\Lambda} =  \\prod^n_{i=1}\\lambda_i\n\\end{equation*}\\]\n\n\n□\n\nProposition (Trace and Eigenvalues): Given a matrix \\(\\boldsymbol A\\) together with eigenvalues of \\(\\lambda_1,\\lambda_2,\\dots,\\lambda_n\\), then we can show that:\n\n\\[\\begin{equation*}\n    \\operatorname{Tr}(\\boldsymbol A) = \\sum^n_{i=1}\\lambda_i\n\\end{equation*}\\]\n\nProof (Diagonalizable Matrix): We consider the eigendecomposition of \\(\\boldsymbol A\\). Consider the trace over it, as we have:\n\n\\[\\begin{equation*}\n    \\operatorname{Tr}(\\boldsymbol A) = \\operatorname{Tr}(\\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^{-1}) = \\operatorname{Tr}(\\boldsymbol \\Lambda\\boldsymbol Q\\boldsymbol Q^{-1}) = \\operatorname{Tr}(\\boldsymbol \\Lambda) = \\sum^n_{i=1}\\lambda_i\n\\end{equation*}\\]\n\n\n\nMiscellaneous\nProposition (Change of Variable): If we consider the transformation of the variables where \\(T : \\mathbb{R}^k \\supset X \\rightarrow \\mathbb{R}^k\\). Then we can show that:\n\n\\[\\begin{equation*}\n\\int_{\\mathbb{R}^k} f(\\boldsymbol y)\\dby \\boldsymbol y = \\int_{\\mathbb{R}^k} f(\\boldsymbol T(\\boldsymbol x))\\abs{\\boldsymbol J_T(\\boldsymbol x)}\\dby \\boldsymbol x\n\\end{equation*}\\]\n\nwhere \\(\\boldsymbol J_T(\\boldsymbol x)\\) is the Jacobian of the transformation \\(T(\\cdot)\\), which is defined to be:\n\n\\[\\begin{equation*}\n\\boldsymbol J_T(\\boldsymbol x) = \\begin{pmatrix}\n    \\cfrac{\\partial T_1(\\cdot)}{\\partial x_1} & \\cfrac{\\partial T_1(\\cdot)}{\\partial x_2}  & \\cdots & \\cfrac{\\partial T_1(\\cdot)}{\\partial x_n} \\\\\n    \\cfrac{\\partial T_2(\\cdot)}{\\partial x_1} & \\cfrac{\\partial T_2(\\cdot)}{\\partial x_2}  & \\cdots & \\cfrac{\\partial T_2(\\cdot)}{\\partial x_n} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\cfrac{\\partial T_n(\\cdot)}{\\partial x_1} & \\cfrac{\\partial T_n(\\cdot)}{\\partial x_2}  & \\cdots & \\cfrac{\\partial T_n(\\cdot)}{\\partial x_n} \\\\\n\\end{pmatrix}\n\\end{equation*}\\]"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#multivariate-guassian-distribution-introduction",
    "href": "posts/1-GiHKAL/index.html#multivariate-guassian-distribution-introduction",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Multivariate Guassian Distribution: Introduction",
    "text": "Multivariate Guassian Distribution: Introduction\nDefinition (Multivariate Gaussian): It is defined as:\n\n\\[\\begin{equation*}\n\\mathcal{N}(\\boldsymbol x | \\boldsymbol \\mu, \\boldsymbol \\Sigma) = \\frac{1}{\\sqrt{\\abs{2\\pi\\boldsymbol \\Sigma}}}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu)\\right\\}\n\\end{equation*}\\]\n\nwhere we call \\(\\boldsymbol \\mu \\in \\mathbb{R}^n\\) a mean and \\(\\boldsymbol \\Sigma \\in \\mathbb{R}^{n\\times n}\\) covariance, which should be symmetric and positive semidefinite (since the covariance is always positive semidefinite and symmetric).\nRemark (2D Independent Gaussian): Now, let’s consider, multivariate Gaussian but in the case that both variables are independent to each other with difference variances, as we define the parameters to be:\n\n\\[\\begin{equation*}\n\\boldsymbol \\mu = \\begin{bmatrix}\n    \\mu_1 \\\\ \\mu_2\n\\end{bmatrix} \\qquad \\boldsymbol \\Sigma =\n\\begin{bmatrix}\n    \\sigma_1^2 & 0 \\\\\n    0 & \\sigma_2^2 \\\\\n\\end{bmatrix}\n\\end{equation*}\\]\n\nNow, let’s expand the multivariate Guassian, please note that \\(\\abs{\\boldsymbol \\Sigma} = \\sigma_1^2\\sigma_2^2\\):\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\mathcal{N}(\\boldsymbol x | \\boldsymbol \\mu, \\boldsymbol \\Sigma) &= \\frac{1}{\\sqrt{\\abs{2\\pi\\boldsymbol \\Sigma}}}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu)\\right\\} \\\\\n&= \\frac{1}{4\\pi^2\\sigma_1^2\\sigma_2^2} \\exp\\brackc{-\\frac{1}{2} \\begin{bmatrix} x_1-\\mu_1 \\\\ x_2-\\mu_2 \\end{bmatrix}^T\n\\begin{bmatrix}\n    \\sigma_1^2 & 0 \\\\\n    0 & \\sigma_2^2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix} x_1-\\mu_1 \\\\ x_2-\\mu_2 \\end{bmatrix}} \\\\\n&= \\frac{1}{4\\pi^2\\sigma_1^2\\sigma_2^2} \\exp\\brackc{-\\frac{1}{2} \\brackb{ \\bracka{\\frac{x_1-\\mu_1}{\\sigma_1}}^2 + \\bracka{\\frac{x_2-\\mu_2}{\\sigma_2}}^2 }}\\\\\n&= \\frac{1}{2\\pi\\sigma_1^2} \\exp\\brackc{-\\frac{1}{2} \\frac{(x_1-\\mu_1)^2}{\\sigma_1^2}} \\frac{1}{2\\pi\\sigma_2^2} \\exp\\brackc{-\\frac{1}{2} \\frac{(x_2-\\mu_2)^2}{\\sigma_2^2}}\\\\\n&= \\mathcal{N}(x_1 | \\mu_1, \\sigma_1^2)\\mathcal{N}(x_2 | \\mu_2, \\sigma_2^2)\n\\end{aligned}\n\\end{equation*}\\]\n\nRemark (Shape of Gaussian): Let’s consider the eigendecomposition of the inverse covariance matrix (which is positive semidefinite and symmetric), as we have\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol \\Sigma^{-1} = \\sum^n_{i=1}\\frac{1}{\\lambda_i}\\boldsymbol u_i\\boldsymbol u_i^T\n\\end{aligned}\n\\end{equation*}\\]\n\nwhere \\(\\lambda_1,\\dots,\\lambda_n\\) and \\(\\boldsymbol u_1,\\dots,\\boldsymbol u_n\\) are the eigenvalues and eigenvectors, repectively of \\(\\boldsymbol \\Sigma\\). Let’s consider the terms inside the exponential to be:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x - \\boldsymbol \\mu)\n= \\sum^n_{i=1}\\frac{(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol u_i\\boldsymbol u_i^T(\\boldsymbol x - \\boldsymbol \\mu)}{\\lambda_i}\n= \\sum^n_{i=1}\\frac{y_i^2}{\\lambda_i}\n\\end{aligned}\n\\end{equation*}\\]\n\nwhere we have \\(y_i = (\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol u_i\\). Consider the vector to be \\(\\boldsymbol y = (y_1,y_2,\\dots,y_n)^T = \\boldsymbol U(\\boldsymbol x - \\boldsymbol \\mu)\\). This gives us the linear transformation over \\(\\boldsymbol x\\), which implies the following shape of Gaussian: - Ellipsoids with the center \\(\\boldsymbol \\mu\\) - Axis is in the direction of eigenvector \\(\\boldsymbol u_i\\) - Scaling of each direction is the eigenvector \\(\\lambda_i\\) associated with \\(\\boldsymbol u_i\\)\nProposition (Normalization of Gaussian): We can show that:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\int \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu)\\right\\} \\dby \\boldsymbol x = \\sqrt{\\abs{2\\pi\\boldsymbol \\Sigma}}\n\\end{aligned}\n\\end{equation*}\\]\n\nProof: Let’s consider the change of variable, in which we will change the variable from \\(x_i\\) to \\(y_i\\) where \\(\\boldsymbol y = \\boldsymbol U(\\boldsymbol x - \\boldsymbol \\mu)\\). To do this we have the find the Jacobian of the transformation, which is:\n\n\\[\\begin{equation*}\n\\begin{aligned}\nJ_{ij} = \\frac{\\partial x_i}{\\partial y_j} = U_{ji}\n\\end{aligned}\n\\end{equation*}\\]\n\nConsider its determinant, as we have: \\(\\abs{\\boldsymbol J}^2 = \\abs{\\boldsymbol U^T}^2 = \\abs{\\boldsymbol U^T}\\abs{\\boldsymbol U} = \\abs{\\boldsymbol U^T\\boldsymbol U} = \\abs{I} = 1\\). Consider the integration as we have (and use the Gaussian integrations):\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\int \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu)\\right\\} \\dby \\boldsymbol x\n&= \\int \\exp\\brackc{\\sum^n_{i=1}-\\frac{y_i^2}{2\\lambda_i}} |\\boldsymbol J| \\dby \\boldsymbol y \\\\\n&= \\int \\prod^n_{i=1}\\exp\\brackc{-\\frac{y_i^2}{2\\lambda_i}} \\dby \\boldsymbol y \\\\\n&= \\prod^n_{i=1}\\int \\exp\\brackc{-\\frac{y_i^2}{2\\lambda_i}} \\dby y_i \\\\\n&= \\prod^n_{i=1}\\sqrt{2\\pi\\lambda_i} = \\sqrt{\\abs{2\\pi\\boldsymbol \\Sigma}}\n\\end{aligned}\n\\end{equation*}\\]\n\nNote that we have shown that the determinant is the product of eigenvalues. Thus the prove is completed.\n\n□"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#useful-backgrounds-1",
    "href": "posts/1-GiHKAL/index.html#useful-backgrounds-1",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Useful Backgrounds",
    "text": "Useful Backgrounds\nProposition (Inverse of Partition Matrix): The block matrix can be inversed as:\n\n\\[\\begin{equation*}\n\\require{color}\n\\newcommand{\\dby}{\\ \\mathrm{d}}\\newcommand{\\argmax}[1]{\\underset{#1}{\\arg\\max \\ }}\\newcommand{\\argmin}[1]{\\underset{#1}{\\arg\\min \\ }}\\newcommand{\\const}{\\text{const.}}\\newcommand{\\bracka}[1]{\\left( #1 \\right)}\\newcommand{\\brackb}[1]{\\left[ #1 \\right]}\\newcommand{\\brackc}[1]{\\left\\{ #1 \\right\\}}\\newcommand{\\brackd}[1]{\\left\\langle #1 \\right\\rangle}\\newcommand{\\correctquote}[1]{``#1''}\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\definecolor{red}{RGB}{244, 67, 54}\n\\definecolor{pink}{RGB}{233, 30, 99}\n\\definecolor{purple}{RGB}{103, 58, 183}\n\\definecolor{yellow}{RGB}{255, 193, 7}\n\\definecolor{grey}{RGB}{96, 125, 139}\n\\definecolor{blue}{RGB}{33, 150, 243}\n\\definecolor{green}{RGB}{0, 150, 136}\n\\begin{bmatrix}\n    \\boldsymbol A & \\boldsymbol B \\\\\n    \\boldsymbol C & \\boldsymbol D \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n    \\boldsymbol M & -\\boldsymbol M\\boldsymbol B\\boldsymbol D^{-1} \\\\\n    -\\boldsymbol D^{-1}\\boldsymbol C\\boldsymbol M & \\boldsymbol D^{-1}+ \\boldsymbol D^{-1}\\boldsymbol C\\boldsymbol M\\boldsymbol B\\boldsymbol D^{-1} \\\\\n\\end{bmatrix}\n\\end{equation*}\\]\n\nwhere we set \\(\\boldsymbol M = (\\boldsymbol A-\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C)^{-1}\\)\nProposition (Inverse Matrix Identity): We can show that\n\n\\[\\begin{equation*}\n    (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1} = \\boldsymbol P\\boldsymbol B^T(\\boldsymbol B\\boldsymbol P\\boldsymbol B^T + \\boldsymbol R)^{-1}\n\\end{equation*}\\]\n\nProof: The can be proven by right multiply the inverse on the right hand-side:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    (\\boldsymbol P^{-1} + &\\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1}(\\boldsymbol B\\boldsymbol P\\boldsymbol B^T + \\boldsymbol R) \\\\\n    &= (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol B\\boldsymbol P\\boldsymbol B^T + (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol R \\\\\n    &= (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol B\\boldsymbol P\\boldsymbol B^T + (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B \\\\\n    &= (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\Big[\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol B\\boldsymbol P + \\boldsymbol P^{-1} \\boldsymbol P\\Big]\\boldsymbol B^T\\\\\n    &= (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\Big[\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol B+ \\boldsymbol P^{-1} \\Big]\\boldsymbol P\\boldsymbol B^T \\\\\n    &= \\boldsymbol P\\boldsymbol B^T \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\n\n□\n\nProposition (Woodbury Identity): We can show that\n\n\\[\\begin{equation*}\n    (\\boldsymbol A + \\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C)^{-1} = \\boldsymbol A^{-1} - \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\n\\end{equation*}\\]\n\nProof: The can be proven by right multiply the inverse on the right hand-side:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\Big[ \\boldsymbol A^{-1} &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1} \\Big](\\boldsymbol A + \\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C) \\\\\n    &= \\boldsymbol A^{-1}(\\boldsymbol A + \\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C) - \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}(\\boldsymbol A + \\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C) \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol A^{-1}\\boldsymbol A + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol A \\\\\n        &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol I + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C \\\\\n        &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol I + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B\\Big[(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol D + (\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B\\Big]\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol I + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B\\Big[(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)\\Big]\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol I + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} = \\boldsymbol I\n\\end{aligned}\n\\end{equation*}\\]\n\n\n□"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#conditional-marginalisation",
    "href": "posts/1-GiHKAL/index.html#conditional-marginalisation",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Conditional & Marginalisation",
    "text": "Conditional & Marginalisation\nRemark (Settings): We consider the setting where we consider the partition of random variables:\n\n\\[\\begin{equation*}\n\\begin{bmatrix}\n    \\boldsymbol x_a \\\\ \\boldsymbol x_b\n\\end{bmatrix}\n\\sim \\mathcal{N}\\bracka{\n\\begin{bmatrix}\n    \\boldsymbol \\mu_a \\\\ \\boldsymbol \\mu_b\n\\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol \\Sigma_{aa} & \\boldsymbol \\Sigma_{ab} \\\\\n    \\boldsymbol \\Sigma_{ba} & \\boldsymbol \\Sigma_{bb} \\\\\n\\end{bmatrix}}\n\\end{equation*}\\]\n\nDue to the symmetric of covariance, we have \\(\\boldsymbol \\Sigma_{ab} = \\boldsymbol \\Sigma_{ba}^T\\). Furthermore, we denote\n\n\\[\\begin{equation*}\n\\boldsymbol \\Sigma^{-1} = \\begin{bmatrix}\n    \\boldsymbol \\Sigma_{aa} & \\boldsymbol \\Sigma_{ab} \\\\\n    \\boldsymbol \\Sigma_{ba} & \\boldsymbol \\Sigma_{bb} \\\\\n\\end{bmatrix}^{-1} =\n\\begin{bmatrix}\n    \\boldsymbol \\Lambda_{aa} & \\boldsymbol \\Lambda_{ab} \\\\\n    \\boldsymbol \\Lambda_{ba} & \\boldsymbol \\Lambda_{bb} \\\\\n\\end{bmatrix} = \\boldsymbol \\Lambda\n\\end{equation*}\\]\n\nwhere \\(\\boldsymbol \\Lambda\\) is called precision matrix. One can consider the inverse of partition matrix to find such a value of \\(\\boldsymbol \\Lambda\\) (will be useful afterward), thus we note that \\(\\boldsymbol \\Sigma_{aa} \\ne \\boldsymbol \\Lambda^{-1}_{aa}\\), and so on.\nRemark (Complete the Square): To find the conditional and marginalision (together with other kinds of Gaussian manipulation), we relies on a method called completing the square. Let’s consider the quadratic form expansion:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    -\\frac{1}{2}&(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu) \\\\\n    &= {\\color{blue}{-\\frac{1}{2}\\boldsymbol x^T\\boldsymbol \\Sigma^{-1}\\boldsymbol x + \\boldsymbol \\mu^T\\boldsymbol \\Sigma^{-1}\\boldsymbol x}} -\\frac{1}{2} \\boldsymbol \\mu^T\\boldsymbol \\Sigma^{-1}\\boldsymbol \\mu \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nNote that the blue term are the term that depends on \\(x\\). This means that to find the Gaussian, we will have to find the first and second order of \\(\\boldsymbol x\\) only. Or, we can consider each individual elements of partitioned random variable:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    -\\frac{1}{2}&(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu) \\\\\n    &= \\begin{aligned}[t]\n        {\\color{green}{-\\frac{1}{2}(\\boldsymbol x_a - \\boldsymbol \\mu_a)^T\\boldsymbol \\Lambda_{aa}(\\boldsymbol x_a - \\boldsymbol \\mu_a)}}{\\color{yellow}{ - \\frac{1}{2}(\\boldsymbol x_a - \\boldsymbol \\mu_a)^T\\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b)}} \\\\\n        {\\color{purple}{-\\frac{1}{2}(\\boldsymbol x_b - \\boldsymbol \\mu_b)^T\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a)}}{\\color{grey}{ - \\frac{1}{2}(\\boldsymbol x_b - \\boldsymbol \\mu_b)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol x_b - \\boldsymbol \\mu_b)}} \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        &{\\color{green} -\\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} -\\frac{1}{2} \\boldsymbol \\mu^T\\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu {\\color{yellow} -\\frac{1}{2}\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b + \\frac{1}{2} \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b} \\\\\n        &{\\color{yellow}+\\frac{1}{2}\\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b} - \\frac{1}{2}\\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b {\\color{purple} -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba} \\boldsymbol \\mu_a}  \\\\\n        &- \\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol \\mu_a {\\color{grey} -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b + \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b} -\\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b\n    \\end{aligned} \\\\\n\\end{aligned}\n\\label{eqn:1}\\tag{1}\n\\end{equation*}\\]\n\nCompleting the square is to match this pattern into our formula in order to get new Gaussian distribution. There are \\(2\\) ways to complete the squre that depends on the scenario: - When we want to find the Gaussian in difference form (but still being Gaussian) i.e conditional - When we want to marginalise some variables out or when we have to find the true for of the distribution without relying on knowing the final form (or when we are not really sure about the final form) i.e marginalisation, posterior\nLet’s just show how it works with examples.\nProposition (Conditional): Consider the following Gaussian\n\n\\[\\begin{equation*}\n\\begin{bmatrix}\n    \\boldsymbol x_a \\\\ \\boldsymbol x_b\n\\end{bmatrix}\n\\sim p(\\boldsymbol x_a, \\boldsymbol x_b) = \\mathcal{N}\\bracka{\n\\begin{bmatrix}\n    \\boldsymbol \\mu_a \\\\ \\boldsymbol \\mu_b\n\\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol \\Sigma_{aa} & \\boldsymbol \\Sigma_{ab} \\\\\n    \\boldsymbol \\Sigma_{ba} & \\boldsymbol \\Sigma_{bb} \\\\\n\\end{bmatrix}}\n\\end{equation*}\\]\n\nWe can show that: \\(p(\\boldsymbol x_a | \\boldsymbol x_b) = \\mathcal{N}(\\boldsymbol x | \\boldsymbol \\mu_{a|b}, \\boldsymbol \\Lambda^{-1}_{aa})\\), where we have - \\(\\boldsymbol \\mu_{a\\lvert b} = \\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{aa}^{-1}\\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b)\\) - Or, we can set \\(\\boldsymbol K = \\boldsymbol \\Sigma_{ab}\\boldsymbol \\Sigma_{bb}^{-1}\\), where we have\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    &\\boldsymbol \\mu_{a|b} = \\boldsymbol \\mu_a + \\boldsymbol K(\\boldsymbol x_b - \\boldsymbol \\mu_b)  \\qquad \\begin{aligned}[t]\n        \\boldsymbol \\Sigma_{a|b} &= \\boldsymbol \\Sigma_{aa} - \\boldsymbol K\\boldsymbol \\Sigma_{bb}\\boldsymbol K^T \\\\\n        &= \\boldsymbol \\Sigma_{aa} - \\boldsymbol \\Sigma_{ab}\\boldsymbol \\Sigma_{bb}^{-1}\\boldsymbol \\Sigma_{ba} \\\\\n    \\end{aligned}\n\\end{aligned}\n\\end{equation*}\\]\n\nPlease note that this follows from block-matrix inverse result (you can try plugging the results in).\nProof: We consider the expansion of the conditional distribution, as we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    -\\frac{1}{2}(\\boldsymbol x_a - \\boldsymbol \\mu_{a|b})^T\\boldsymbol \\Sigma_{a|b}^{-1}(\\boldsymbol x_a - \\boldsymbol \\mu_{a|b}) = {\\color{red}-\\frac{1}{2}\\boldsymbol x_a^T\\boldsymbol \\Sigma_{a|b}^{-1}\\boldsymbol x_a} + {\\color{blue}\\boldsymbol x_a^T\\boldsymbol \\Sigma_{a|b}^{-1}\\boldsymbol \\mu_{a|b}} + \\text{const}\n\\end{aligned}\n\\end{equation*}\\]\n\nLet’s consider the values, which should be equal to equation \\(\\eqref{eqn:1}\\), as we can see that: - The red term: we set \\(\\boldsymbol \\Sigma_{a\\lvert b}^{-1} = \\boldsymbol \\Lambda_{aa}\\) (consider the first green term of the equation \\(\\eqref{eqn:1}\\)) - The blue term., we will have to consider \\((\\dots)^T\\boldsymbol x_a\\). We have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n{\\color{green} \\boldsymbol \\mu_{a}^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} &{\\color{yellow} - \\frac{1}{2}\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b + \\frac{1}{2} \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b} {\\color{purple} - \\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a} \\\\\n&= \\boldsymbol x_a^T\\Big[ \\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{ab}\\boldsymbol x_b + \\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b \\Big] = \\boldsymbol x_a^T\\Big[ \\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b) \\Big] \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nPlease note that \\(\\boldsymbol \\Lambda_{ab}^T = \\boldsymbol \\Lambda_{ba}\\). Now, let’s do “pattern” matching, as we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol x_a^T&\\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_{a|b} = \\boldsymbol x_a^T\\Big[ \\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b) \\Big] \\\\\n\\implies&\\boldsymbol \\mu_{a|b} \\begin{aligned}[t]\n    &= \\boldsymbol \\Lambda_{aa}^{-1}\\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{aa}^{-1}\\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b) \\\\\n    &= \\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{aa}^{-1}\\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b) \\\\\n\\end{aligned}\n\\end{aligned}\n\\end{equation*}\\]\n\nThus the proof is complete.\n\n□\n\nProposition (Marginalisation): Consider the following Gaussian\n\n\\[\\begin{equation*}\n\\begin{bmatrix}\n    \\boldsymbol x_a \\\\ \\boldsymbol x_b\n\\end{bmatrix}\n\\sim p(\\boldsymbol x_a, \\boldsymbol x_b) = \\mathcal{N}\\bracka{\n\\begin{bmatrix}\n    \\boldsymbol \\mu_a \\\\ \\boldsymbol \\mu_b\n\\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol \\Sigma_{aa} & \\boldsymbol \\Sigma_{ab} \\\\\n    \\boldsymbol \\Sigma_{ba} & \\boldsymbol \\Sigma_{bb} \\\\\n\\end{bmatrix}}\n\\end{equation*}\\]\n\nWe can show that: \\(p(\\boldsymbol x_a) = \\mathcal{N}(\\boldsymbol x | \\boldsymbol \\mu_{a}, \\boldsymbol \\Sigma_{aa})\\)\nProof: We collect the terms that contains \\(\\boldsymbol x_b\\) so that we can integrate it out, as we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n{\\color{grey} -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b} &{\\color{grey} +} {\\color{grey}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b}\n{\\color{purple} -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba} \\boldsymbol \\mu_a}\n{\\color{yellow} -\\frac{1}{2}\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b+\\frac{1}{2}\\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b} \\\\\n&=  -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b + \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b -\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba} \\boldsymbol \\mu_a \\\\\n&=  -\\frac{1}{2}\\Big[\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b - 2\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\Lambda_{bb}^{-1}\\underbrace{\\Big(\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b -\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a)\\Big)}_{\\boldsymbol m}\\Big] \\\\\n&=  -\\frac{1}{2}\\Big[\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b - 2\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m + (\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m) \\Big] + \\frac{1}{2}(\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)  \\\\\n&=  -\\frac{1}{2}(\\boldsymbol x_b - \\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol x_b - \\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m) + {\\color{blue}\\frac{1}{2}\\boldsymbol m^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m}  \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nIf we integrate our the quantity, to be:\n\n\\[\\begin{equation*}\n\\int \\exp\\brackc{-\\frac{1}{2}(\\boldsymbol x_b - \\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol x_b - \\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)}\\dby \\boldsymbol x_b\n\\end{equation*}\\]\n\nWe can use the Gaussian integration, like in part 1 and part 2. Now, we consider the other terms that doesn’t depends on \\(\\boldsymbol x_b\\) i.e all terms that depends on \\(\\boldsymbol x_a\\) together with the blue term that been left out.\n\n\\[\\begin{equation*}\n\\begin{aligned}\n{\\color{blue}\\frac{1}{2}\\boldsymbol m^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m} &{\\color{green} -}{\\color{green} \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} {\\color{yellow} + \\frac{1}{2} \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b}{\\color{purple}+ \\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a} \\\\\n&= \\begin{aligned}[t]\n    \\frac{1}{2}\\Big(&\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b -\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a)\\Big)^T\\boldsymbol \\Lambda_{bb}^{-1}\\Big(\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b -\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a)\\Big) \\\\\n    &{\\color{green} - \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} + \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    \\frac{1}{2}\\Big[ \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b &- (\\boldsymbol x_a - \\boldsymbol \\mu_a)^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\mu_b \\\\\n    &- \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a) + (\\boldsymbol x_a - \\boldsymbol \\mu_a)^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a) \\Big] \\\\\n    &{\\color{green} - \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} + \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    \\frac{1}{2}\\Big[ \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b &- \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\mu_b + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\mu_b - \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol \\mu_a \\\\\n    &+ \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a - \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol\\mu_a - \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a \\\\\n    &+ \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol \\mu_a \\Big] {\\color{green} - \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} + \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    \\frac{1}{2}\\Big[-2\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\mu_b &+ \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a - 2\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol\\mu_a\\Big] \\\\\n    &{\\color{green} - \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} + \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\boldsymbol x_a^T\\Big[\\boldsymbol \\Lambda_{aa} - \\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\Big]\\boldsymbol x_a &+ \\boldsymbol x_a^T\\Big[\\boldsymbol \\Lambda_{aa} - \\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\Big]\\boldsymbol\\mu_a + \\text{const}\n\\end{aligned} \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nIf we comparing this form to the normal quadratic expanision of Gaussian, we can set the \\(\\boldsymbol \\mu\\) of marginalised Gaussian is \\(\\boldsymbol \\mu_a\\), while the covariance is \\((\\boldsymbol \\Lambda_{aa} - \\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba})^{-1}\\). If we compare this to the inverse matrix partition, we can see that this is equal to \\(\\boldsymbol \\Sigma_{aa}\\). Thus complete the proof.\n\n□\n\nProposition (Linear Gaussian Model): Consider the distribution to be: \\(p(\\boldsymbol x) = \\mathcal{N}(\\boldsymbol x \\vline {\\color{yellow} \\boldsymbol \\mu} , {\\color{blue} \\boldsymbol \\Lambda^{-1}})\\) and \\(p(\\boldsymbol y \\vline \\boldsymbol x) = \\mathcal{N}(\\boldsymbol y \\vline {\\color{purple}\\boldsymbol A}\\boldsymbol x + {\\color{green} \\boldsymbol b}, {\\color{red} \\boldsymbol L^{-1}})\\). We can show that the following holds:\n\n\\[\\begin{equation*}\np(\\boldsymbol y) = \\mathcal{N}(\\boldsymbol y \\vline {\\color{purple}\\boldsymbol A}{\\color{yellow} \\boldsymbol \\mu} + {\\color{green} \\boldsymbol b}, {\\color{red} \\boldsymbol L^{-1}} + {\\color{purple}\\boldsymbol A}{\\color{blue} \\boldsymbol \\Lambda^{-1}}{\\color{purple}\\boldsymbol A^T}) \\qquad p(\\boldsymbol x \\vline \\boldsymbol y) = \\mathcal{N}\\bracka{ \\boldsymbol x \\vline {\\color{grey} \\boldsymbol \\Sigma}\\brackc{ {\\color{purple}\\boldsymbol A^T} {\\color{red} \\boldsymbol L}(\\boldsymbol y-{\\color{green} \\boldsymbol b}) + {\\color{blue} \\boldsymbol \\Lambda}{\\color{yellow} \\boldsymbol \\mu}}, {\\color{grey} \\boldsymbol \\Sigma} }\n\\end{equation*}\\]\n\nwhere we have \\({\\color{grey} \\boldsymbol \\Sigma} = ({\\color{blue} \\boldsymbol \\Lambda} + {\\color{purple}\\boldsymbol A^T}{\\color{red} \\boldsymbol L}{\\color{purple}\\boldsymbol A})^{-1}\\)\nProof: We will consider the joint random variable \\(\\boldsymbol z = (\\boldsymbol x, \\boldsymbol y)^T\\). Let’s consider the joint distribution and the inside of exponential:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n-\\frac{1}{2}&(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Lambda(\\boldsymbol x - \\boldsymbol \\mu) - \\frac{1}{2}(\\boldsymbol y - \\boldsymbol A\\boldsymbol x - \\boldsymbol b)^T\\boldsymbol L(\\boldsymbol y - \\boldsymbol A\\boldsymbol x - \\boldsymbol b) + \\text{const} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\Big[ \\boldsymbol x^T\\boldsymbol \\Lambda\\boldsymbol x &- 2\\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol x + \\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol \\mu + \\boldsymbol y^T\\boldsymbol L\\boldsymbol y - \\boldsymbol y^T\\boldsymbol L\\boldsymbol A\\boldsymbol x - \\boldsymbol y^T\\boldsymbol L\\boldsymbol b\\\\\n    &-\\boldsymbol x^T\\boldsymbol A^T\\boldsymbol L\\boldsymbol y + \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol A\\boldsymbol x + \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol b - \\boldsymbol b^T\\boldsymbol L\\boldsymbol y +\\boldsymbol b^T\\boldsymbol L\\boldsymbol A\\boldsymbol x + \\boldsymbol b^T\\boldsymbol L\\boldsymbol b \\Big] + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\Big[ \\boldsymbol x^T\\boldsymbol \\Lambda\\boldsymbol x &- 2\\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol x + \\boldsymbol y^T\\boldsymbol L\\boldsymbol y - \\boldsymbol y^T\\boldsymbol L\\boldsymbol A\\boldsymbol x - \\boldsymbol y^T\\boldsymbol L\\boldsymbol b\\\\\n    &-\\boldsymbol x^T\\boldsymbol A^T\\boldsymbol L\\boldsymbol y + \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol A\\boldsymbol x + \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol b - \\boldsymbol b^T\\boldsymbol L\\boldsymbol y +\\boldsymbol b^T\\boldsymbol L\\boldsymbol A\\boldsymbol x  \\Big] + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\Big[ \\boldsymbol x^T\\Big(\\boldsymbol \\Lambda + \\boldsymbol A^T \\boldsymbol L \\boldsymbol A\\Big)\\boldsymbol x &+ \\boldsymbol y^T\\boldsymbol L\\boldsymbol y - \\boldsymbol y^T\\boldsymbol L\\boldsymbol A\\boldsymbol x - \\boldsymbol x^T\\boldsymbol A^T\\boldsymbol L\\boldsymbol y\\\\\n    &+ 2\\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol b  - 2\\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol x - 2\\boldsymbol y^T\\boldsymbol L\\boldsymbol b \\Big] + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\Big[ \\boldsymbol x^T\\Big(\\boldsymbol \\Lambda + \\boldsymbol A^T \\boldsymbol L \\boldsymbol A\\Big)\\boldsymbol x &+ \\boldsymbol y^T\\boldsymbol L\\boldsymbol y - \\boldsymbol y^T\\boldsymbol L\\boldsymbol A\\boldsymbol x - \\boldsymbol x^T\\boldsymbol A^T\\boldsymbol L\\boldsymbol y\\Big]\\\\\n    &- \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol b + \\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol x + \\boldsymbol y^T\\boldsymbol L\\boldsymbol b  + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\n    \\begin{pmatrix}\n        \\boldsymbol x \\\\ \\boldsymbol y\n    \\end{pmatrix}^T\n    \\begin{pmatrix}\n        \\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A & -\\boldsymbol A^T\\boldsymbol L \\\\\n        -\\boldsymbol L\\boldsymbol A & \\boldsymbol L\n    \\end{pmatrix}\n    \\begin{pmatrix}\n        \\boldsymbol x \\\\ \\boldsymbol y\n    \\end{pmatrix}  +\n    \\begin{pmatrix}\n        \\boldsymbol x \\\\ \\boldsymbol y\n    \\end{pmatrix}^T\n    \\begin{pmatrix}\n        \\boldsymbol \\Lambda\\boldsymbol \\mu - \\boldsymbol A^T\\boldsymbol L\\boldsymbol b \\\\\n        \\boldsymbol L\\boldsymbol b\n    \\end{pmatrix} + \\text{const}\n\\end{aligned} \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nWe can use the block-matrix inverse result, to show that:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\begin{pmatrix}\n    \\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A & -\\boldsymbol A^T\\boldsymbol L \\\\\n    -\\boldsymbol L\\boldsymbol A & \\boldsymbol L\n\\end{pmatrix}^{-1} =\n\\begin{pmatrix}\n    \\boldsymbol \\Lambda^{-1} & \\boldsymbol \\Lambda^{-1}\\boldsymbol A^T \\\\\n    \\boldsymbol A\\boldsymbol \\Lambda^{-1} & \\boldsymbol L^{-1} + \\boldsymbol A\\boldsymbol \\Lambda^{-1}\\boldsymbol A^T\n\\end{pmatrix}\n\\end{aligned}\n\\end{equation*}\\]\n\nRecall the Gaussian pattern matching, we can see that the mean is equal to:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\begin{pmatrix}\n    \\boldsymbol \\Lambda^{-1} & \\boldsymbol \\Lambda^{-1}\\boldsymbol A^T \\\\\n    \\boldsymbol A\\boldsymbol \\Lambda^{-1} & \\boldsymbol L^{-1} + \\boldsymbol A\\boldsymbol \\Lambda^{-1}\\boldsymbol A^T\n\\end{pmatrix}\n\\begin{pmatrix}\n    \\boldsymbol \\Lambda\\boldsymbol \\mu - \\boldsymbol A^T\\boldsymbol L\\boldsymbol b \\\\\n    \\boldsymbol L\\boldsymbol b\n\\end{pmatrix} = \\begin{pmatrix}\n    \\boldsymbol \\mu \\\\ \\boldsymbol A\\boldsymbol \\mu + \\boldsymbol b\n\\end{pmatrix}\n\\end{aligned}\n\\end{equation*}\\]\n\nPlease note that with marginalisation result, it is obvious to see how this leads to the final result. Now, for the conditional result, we have the usual result that \\(\\boldsymbol \\Sigma= \\boldsymbol \\Lambda_{xx} = (\\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A)^{-1}\\), for the mean:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\boldsymbol \\mu - &(\\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A)^{-1}(\\boldsymbol A^T\\boldsymbol L)(-\\boldsymbol y + A\\boldsymbol \\mu + \\boldsymbol b) \\\\\n    &= \\boldsymbol \\Sigma\\boldsymbol A^T\\boldsymbol L(\\boldsymbol y - \\boldsymbol b) + \\boldsymbol \\mu - \\boldsymbol \\Sigma\\boldsymbol A^T\\boldsymbol L\\boldsymbol A\\boldsymbol \\mu\n\\end{aligned}\n\\end{equation*}\\]\n\nWe want to show that \\(\\boldsymbol \\mu - \\boldsymbol \\Sigma\\boldsymbol A^T\\boldsymbol L\\boldsymbol A\\boldsymbol \\mu = \\boldsymbol \\Sigma\\boldsymbol \\Lambda\\boldsymbol \\mu\\).\n\n\nLHS: Apply inverse indentity, where we consider the section highlight in pink:\n\\[\n\\begin{aligned}\n    \\boldsymbol \\mu &- \\boldsymbol \\Sigma\\boldsymbol A^T\\boldsymbol L\\boldsymbol A\\boldsymbol \\mu \\\\\n    &= \\boldsymbol \\mu - {\\color{pink}(\\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A)^{-1}\\boldsymbol A^T\\boldsymbol L}\\boldsymbol A\\boldsymbol \\mu \\\\\n    &= \\boldsymbol \\mu - \\boldsymbol \\Lambda^{-1}\\boldsymbol A^T(\\boldsymbol A\\boldsymbol \\Lambda^{-1}\\boldsymbol A^T + \\boldsymbol L^{-1})^{-1}\\boldsymbol A\\boldsymbol \\mu\n\\end{aligned}\n\\]\n\nRHS: We consider the section highlight in blue and use Woodbury Identity:\n\\[\n\\begin{aligned}\n    \\boldsymbol \\Sigma\\boldsymbol \\Lambda\\boldsymbol \\mu &= {\\color{blue}(\\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A)^{-1}}\\boldsymbol \\Lambda\\boldsymbol \\mu \\\\\n    &= \\boldsymbol \\mu - \\boldsymbol \\Lambda^{-1}\\boldsymbol A^T(\\boldsymbol A\\boldsymbol \\Lambda^{-1}\\boldsymbol A^T + \\boldsymbol L^{-1})^{-1}\\boldsymbol A\\boldsymbol \\mu\n\\end{aligned}\n\\]\n\n\nNow we have show that both are equal, thus we conclude the proof.\n\n□"
  },
  {
    "objectID": "works.html",
    "href": "works.html",
    "title": "Works",
    "section": "",
    "text": "Apart from blogs, I also works on larger scale projects, here are the list of them."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mysite",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "miscellaneous/1-plato/index.html",
    "href": "miscellaneous/1-plato/index.html",
    "title": "Introduction to Plato",
    "section": "",
    "text": "This post is a distillation of “Introduction to the study of Plato”, which is the first chapter of The Cambridge Companion to Plato. We will try to add other contents as well, for example: Stanford Encyclopedia of Philosophy (SEP) article on Plato 1. The main contents is to provides an overview/introduction of Plato’s works and development of his thought. Finally, the traslation will comes from  Perseus Digital Library.\n\nWho is Plato ?\nPlato was borned around 427 (approximately)-347 BCE. He has contributed to many areas of philosophy that includes: Metaphysics, Epistemology, Ethics, etc. Or even, outside philosophy (according to our way to categorizing subjects), including Art, Mathematics, Science, Religions and more.\nHe was the first person to start treating philosophical subjects together in a unified framework while believing that philosophy is a subject that has unique methods of investigation - via dialogues or conversation. Although he is innovative in his own right, he still built on top of past philosophers and his contemporaries.\n\n\nPlato Influences\nThere are many influences on Plato thought and its development, which includes:\n\nPolitical Development in Athens and Sparta\nIntellectual movement in 4th and 5th century, including:\n\nSophistic Movement\nWorks in Mathematics\nTheory of Flux by Heraclitus and Cratylus\nUnchaining and Unitary Being ideas of Parmenides\n\n\nBut the biggest influences on Plato was his teacher Socrates, who was excecuted by people of Athens from Trial in 399 BCE (from a charge because they believe Socrates corrupting his fellows/youth by creating a new gods 2.) There are some examples of Socrates’ influence on Plato:\n\nPlato has writting most (if not all) of his works after Socrates death.\nSocrates’ method of philosophy is displayed in Plato works, as it is a form of adversarial dialogues (although his main interest is on how man should live one life, see later on this distinction).\nHence Plato works are mostly dialogues form, while his teacher is the main subject in most of Plato’s dialogues.\n\n\n\nPlato Development\nThe common consensus on phases of Plato works can be divided into 3 parts: Early, Middle and Late 3 works for instance:\n\n\n\n\n\n\n\n\n\nEarly\nEarly (2)\nMiddle\nLate\n\n\n\n\nApology\nLaches\nMeno\nTimaeus\n\n\nCharmides\nProtagoras\nPhaedo\nCritias\n\n\nCrito\n Euthydemus \nSymposium\nSophist\n\n\nEuthyphro\n Hippias Major \nRepublic\nStatesman\n\n\nGorgias\nLysis \nParmenides\nPhilebus\n\n\nHippias Minor\n Mennexenus \nTheaetetus\nLaws\n\n\nIon\n Replublic Book I \nPhaedrus\n\n\n\n\nThere are some further notes on this:\n\nThe exact order of Early works isn’t clear, so it is displayed in alphabetical order.\nThe general consensus that from Euthydemus to Republic Book I are later of for Early period since they are closer to the middle dialogues, while Gorgias should also be latter work compares to the others.\nThe order in middle and late dialogues is one of the plausible order, as there are still some disagreements. However, it is quite agree upon that Laws should be in one of the latest Plato’s writing.\n\nNow, we will provide an overviews of Early Plato works.\n\n\nPlato Earlier Works\nPlato’s early works, while on high influences of Socrates, sought to answer some of the Socrates question on how to live one life. Consider the part of Apology that contains the defending speech of Socrates’ in his trial, where Socreates described his position as 4:\n\n\n\nThis one of you, O human beings, is wisest, who, like Socrates, recognizes that he is in truth of no account in respect to wisdom.\n\n\n—Apology 23b\n\n\n\nwhile Socrates declared that knowledge belong to the gods and incomparable to human’s wisdom (Apology 23a). In Plato’s earlier works, such as: Lache, Charmides, Hippias Major, and Euthyphro. There are several theme that are in common with each other, such as:\n\nSocrates declared that he doesn’t know anything\nMost of the works focuses on the nature of virtue or ethics (by finding the definition) almost exclusively, but ends with unsuccessful outcomes 5.\n\nFinally, Plato is still attached to certain doctrines for instance: One is immune from misfortune as long as they are “good”. To posess virtue is the same as knowing a certain subject matter, while the knowledge can be acquired by philosophizing (conversation). Note that, Plato will eventually change some of his views in the later works, which leads to some change of style, as see in: Republic (a Middle dialogue), where\n\nSocrates (the character) is trying to define and defend justice rather than claiming that he doesn’t have such a knowledge.\nOr, when he argued that training in virtue also required emotional training and not necessary restricted to reason alone.\n\nNonetheless, Plato still keep Socrates as a main interlocutors throughtout his works (excepting some late dialogues).\n\n\n\nTransitional Dialogues\n\nMeno\nMeno, which we believed to be one of the earliest dialogue, is written around 386-382 BCE after Socrates death for 13 years. In this work, we can see that Plato has broken from his earlier themes, in which the dialogue can be split into 2 sections:\n\nFirst Section (70a-79e): Meno (the character) ask Socrates “whether virtue can be taught”, which leads to a question of “what is meaning of being good” (Meno 71a). This brief examination is similar to the early dialogues also ended with unsuccessful search.\nSecond Section (80b-100c): This is where the changes is taking place: Meno questions Socrates on his method of investigation as Socrates claimed to know nothing beforehand, so how can one come to know if he/she doesn’t have a knowledge before (Meno 80d)\n\nThis second section is a shift in Plato’s work as he investigated in such a topic for the first time. To answer the question, Socrates proposed a theory of recollection, which states that “learning” is actually an act of remembering from an knowledges that has been accumulated from our past lives 6. To prove this, Socrates demonstrated to Meno by teaching one of the slaves (or make him remember again) mathematics 7 by asking a series of questions (Meno 81b-86c).\n\n\nTheory of Form in Phaedo\nThe Phaedo is written, it is Plato dialogue that is installed as part of “Last days or Socrates”, which is the last one in the series of dialogues (following from Euthyphro, Apology, and Crito). Although the dialogues in the series are all earlier works, Phaedo is in transitional dialogue. Regarding position relative to Meno, Phaedo shoud be after, since there is a clear reference to Meno (see Phaedo 72e-73a).\nThe main topic of Phaedo is on the immortality of the soul and the first proposal of the “theory of forms”, where Plato speculated about the realm of objects, differ from what we have seen (74a), where they are:\n\nChangeless (Phaedo 78d)\nCan be revealed by thought rather than sensation (Phaedo 79a)\nDifference from both body and soul (Phaedo 79b-79c)\nEverlasting (Phaedo 79d)\n\nPlato’s Example: In Phaedo, Plato gave the example of the form of “Equality”. He shows that we can’t make a mistake on Equality, but we can make mistake on 2 equal sticks. This means that 2 equal sticks is inferior to the Equality (Phaedo 74d-74e) 8.\nPlato proposed the Form as an object that (the historical) Socrates tried to investigate. However, in Phaedo, he didn’t address other questions about the nature of Form, such as:\n\nIs what we are looking for exists independently of human thought ?\nIs it something that can be detected by our senses ?\nCan it change or perish ?\nHow can we able to learn about it ?\nWhat is its relationship with people and action that are consider virtuous but are not identical to what virtue is ?\nHow are these objects of thought related to each other ?\n\nThe theory of Form is where Plato attempts to answer these questions.\n\n\nTheory of Form Development\nHis thought on the theory is developed throughout his works in Middle till the Later periods. Let’s consider 3 differences dialogues: Meno, Republic, and Stateman and Plato’s opinion on the Forms:\n\nMeno: Plato didn’t give us the definition of the forms. However, he proposed an instance of it, for example, Equality, Beauty, Goodness, Justice and Piety.\nRepublic: Plato gives us the definition of the forms, as:\n\n\n\nI take it, of positing a single idea or form in the case of the various multiplicities to which we give the same name.\n\n\n—Republic 596a\n\n\n\nHowever, this definition is incomplete as there are some concerning question such as:\n\nWhat happen to the word that is invented ?\nIs there is any justification to introduce such word so that it corresponds to the Form ?\n\nStateman: Plato clarify that Form can’t exists without justified classification of reality into groups. For instance, we don’t have the form for non-Greek as there is nothing that unify them together apart from Greek itself (262c-262e).\n\n\n\n\n\nMiddle Dialogues\n\nFrom Phaedo to Republic\nPlato has developed his idea further in Republic, where there are several changes in his idea regarding the theory of Form and achieving knowledge:\n\nPhaedo: Our attempts to understand the form is blocked by the bodily needs, and so philosopher wishes to die and to be free from these needs 9 (Phaedo 65e-67b)\nRepublic: However, Plato suggested that there are 3 components of the soul and there is a part of the soul that prevent us from achieving knowledge in philosophy (not the body). Furthermore, Plato proposed that one can attain knowledge of the Form if he/she persue the right track.\n\nPlato has placed the Form of Good is the greatest importance. However, he noted that the path of understnading the form is unfit from political life. Finally, the biggest difference of Phaedo and Republic is Plato’s attitude toward how philosopher lives:\n\nIn Phaedo, Plato tried to prove immortality of the soul for the sake of happiness afterlife.\nOn the other hand, in Republic, Plato proves immortality of the soul for the sake of living philosphoical life and fullu virtuous life, regardless of immortality of the soul.\n\n\n\nMain Theme of The Republic\nThe Republic can be seen as the corner stone of Plato works because it contains many philsophical topics that he will explore in his later works. The main purpose of this work is an attempt to answer the question that Socrates has been persuing. It is divided into 9 books, let’s see how the themes are divided into:\n\nBook 1 and Beginning of Book 2: Book 1 is similar in the style of Plato’s early dialogues where the characters tried to answer the definition of justices and ended in failure near the end of the books. However, at the start of book 2, one of the characters (Glaucon) ask Socrates to justify being just, which sets the tone for the books afterward.\nBook 2 Onward: Plato has taken a new route by exploring and unifying multiple fields such as: Epistimology, Ethical Theory, Political Thoery and Psychological Theory. As Plato will, subsequentially, develop these topics together with theory of Form further in his later works, his work (of these topics) in The Republic is still immature.\n\nLooking further, in his later dialogues, we can see that Plato examine each philosophical topics that has been started in The Republic for instance:\n\nMetaphysics: Parmenides and Sophist\nEpsitimology: Theaetetus\nForm and Sensible World: Timaeus\nHow to live one’s life: Philebus\nPolitical Theory: Laws\nHuman Psychology: Phaedrus, Philebus and Laws\n\n\n\nPlato’s Allegory of the Cave\nIn book 7, we see Plato’s famous Allegory of the Cave, which captures the human condition and philosophical life:\n\nPeople who are not touched by philosophy are prisoners who gazes at the shadow casted on the wall bu various objects by unknown manipulators (The Republic 514a-519a).\nThey are fooled to think that the shadow is real, while they are accustomed to the normal way of living, and so they disregard any kind of interruption.\nWe can see this as an analogy to psychological response when people are questioned by Socrates (The Republic 517a).\n\nThis is also connected to the theory of Form as Plato claimed that the shadow is reality degraded, thus being less real (The Republic 515a-515e). The story goes on as one of the prisoners escaped the cave, he encountered the realm of objects that is more real, which can be seen as he learn to understand the realm of Forms (The Republic 518c).\n\n\nPlato’s Hierarchy of Objects\nIn Book 10 Plato gave the example of “Bed” where he gives the hierarchy of objects related to Bed (in ascending order): A painting of a bed, a bed created by carpenter and a form of bed. We have the following observation:\n\nPainter’s image deviated from the real bed, but it has the saame relationship as between the real bed and the Form of bed.\nWhen we say that the painting has “bed”, we mean that the image in the painting has correct relationship to the real bed. In this case, it has the visual similarity.\nHowever, the relationship between the Form and the bed isn’t visual similarity as there\nSimilarly, Plato suggested that there should be some relationship between the visible bed and the Form of bed. By correctly calling some object to be bed, there should be a right relationship between it and the Form of bed (which should be difference property).\nWhen Plato declared that the Form is completely real, he means that the Form is situated on the highest hierarchy of objects (ranking via degree of ontological dependencies). Thus the form doesn’t depend on anything even its name.\n\n\n\nPlato’s Political Philosophy in The Republic\nThe allegory of the cave also contains political philosophy of Plato, as it provided us the psychological state of ordinary people. Plato suggested that people who are limited in their view of the world aren’t the best judge of their own interst, and might even made mistake on what good for them is:\n\nThus, Plato disagree with political system that comes from people concents, as people will choose the system that they agree but don’t necessary be good.\nGood system, in his view, should be one that promotes their well being. If the citizens fail to understand, then it is the system’s job to educate them.\nPlato, therefore, suggests giving the power to one ruler, who have philosophical understanding of human good (Philosophy king). However, he doesn’t consider much about how this power can be misused 10.\nAfterall the ultimate goal of the city is to promote happiness for and benefits everyone, not just only certain group of people.\n\nBy consider a relationship between human in political community, Plato suggested an analogous relationship that exists between difference Form (they form an kosmos, Republic 500c), which implies that we might have to study them not in isolation but a harmonized unified political order. And, in turns, we should strives to be in such order.\n\n\nPlato’s Theory of Form, onward\nPlato, however, in Republic, paid little attention to the relationship of the Forms, and suggested that the Form of good is the most important Form that is cruitial to understand the other Forms (Republic 505a-509c), as the Forms are constituted in hierarchy inwith the form of Good at the top. Nonetheless, in later dialogues, Plato returns to study multiple kinds of Form, notably:\n\nPhaedus: Plato investigate into difference kinds of form such as unity and diversity together with the concept of love.\nParmenides: Plato studies relationship between Form of unity and other kinds of Forms (such as Sameness, etc.) as a continuation to his studies in the Form of Goodness.\nTimaeus: Plato stated that the sensible work is constructed by divine craftman based on difference kinds of Forms (Timaeus 29a-30b)\n\nThis also includes other Late works such as Stateman, Sophist, and Philebus\n\n\n\n\nAfter Republic\n\nOrder of Late Dialogues\nThere is many consensus on the theme and content of the dialogues after The Republic. One of the main problems is how the theory of Form is developed in his later works, as there are conflicting theories:\n\nPlato has abandoned the theory of Form.\nPlato aware of the shortcoming of the theory of Form but couldn’t fix because he can’t find the source of issues.\nPlato slightly change the content by modify some of its claimed.\n\nThis also ties to the debate about chronological sequence of works after The Republic. However, there is a wide consensus that Laws is the later works, as we can find 5 other works that are related to Laws, which are: Critias, Philebus, Sophist, Stateman and Timaeus\nFurthermore, there is a consensus that Stateman was written after Sophist and Timaeus before Critias (It is obvious that Critias’s subject is discussed in Timaeus). The rest of the order is still on going debate; for instance:\n\nDiogenes Laertius believed that Laws is the last work, while the Critas is incomplete which means that it can potentially be the last position.\nFor stylistic studies suggested the order: Timaeus, Critias, Sophist, Stateman, Philebus, and Laws\n\nNow, if we consider the role of Socrates in these Late dialogues, we can see that in most works, execept Philebus, his role is minor. We may see this as the departure from Plato’s middle periods 11.\n\n\nStylistic Comparision\nG.E.L. Owen disagreed with the stylistic studies claiming that the Timaeus is in middle periods, right after the composition of The Republic 12. The major shift in Plato’s thought happened in Parmenides and Theaetetus. Note that Socrates’ position in these 2 dialogues is unsual. Both of the dialogues are most in critical with the doctrinces put forth in The Republic as:\n\nParmenides: The theory of Form is critized by Parmenides, but Socrates is too young to gives an answer. (Parmenides 126a-135d)\nTheatetus: Unsuccessful search for the definition of knowledge which is taken for granted in The Republic and earlier works. Socrates acts only as a guide as he claims that he can’t produce a positive view of his own (Theatetus 148e-151d)\n\nThis unusual role can be see as the Plato’s new theory of Metaphysics and Epsitimology associated with theory of Form, while the theories are under hard questionings, making Socrates to be a passive role. This carries on to Socrates role in the latter works.\nIn addition, given this observation, it implies that Plato has modified his idea (to unknown degree), or change in Socrates role might not make sense (unless there is another explanation). Note that Plato might change Socrates’ role even with a small changes in his idea; thus, this might not be a strong signal on Plato’s intellectual development. Therefore, we will have to look into the contents of Plato’s work.\n\n\nTheory of Form Criticism and Afters\nConsider Parmenides opening, where it contains the arguments against the theory of Form, and afterward, there are no objection to this criticism in any of Plato works. The argument goes as follows:\n\nIf there is a form of largeness, then we can consider multiple number of such a form.\nThe form of largeness (in our sense) is when the number of things are large.\nConsider the Form of largeness together with large things, we then create a Form of largeness.\nThis can be repeated infinitely, and therefore, there is no form of largeness, leading to a contradiction.\n\nThe plausible rejection is whether the Form of largeness is a large thing in itself or not ? This would leads to many follow-up questions, regarding the assumptions that Plato has to make:\n\nHas Plato use this assumption throughout his middle periods works ?\nDid he plan to examine this assumption more clearly in the later part of Parmenides ?\nHow this objective affect Plato’s development of theory of Form ?\n\nThis also have a consequences on how we place some of the dialogues in relative to each others. If we consider Timaeus to be in the late periods, then it must be written after Parmenides. However, in Timaeus, Plato still relies on the middle periods theory of Form (as he claimed that the Form is object that a craftman ued to creating the sensible world). Thus need to mean that, Plato hasn’t extensively modify his theory of Form into late periods.\n\n\nDevelopment in Moral/Political Theory\nLet’s compare/constract the proposals of ideal society in both The Republic (a Middle period) and Laws (a Late period) as we have:\n\n\n\n\n\n\n\nSimilarity\nDifferences (In Laws)\n\n\n\n\nOne of the roles of political community is to premotes/takes part in moral education\nNo specialized philosophical training for elite groups and delegate the decision making to small group of people\n\n\nThe state shouldn’t untermine its citizens’ value\nThe function of government is distributed to prevent the abuse of power, and no citizens is completely deprived of legislative or judical role\n\n\n\nGiven the limited democracy description in Laws, Plato seem to prefer the philosophical kings as he described in Laws as the second best political community (Laws 739a-740a)\n\n\nPlato’s Attitude for Writing\nMost of the works from the early Greek and Hellenistic philosophy are lost to us. There might be some part of Plato’s writing that were loss. We will now consider what this Plato’s view toward writing. In Phaedrus, Plato suggests 2 reasons to favour speaking rather than writing when doing philosophy:\n\nSpeaking allow the listener to response immediately (Phaedrus 275c-276a)\nOne can practice his or her philosophy more regulary and therefor being fresh all the time. Writing can have a negative effect, as one can write it and forgot the contents (Phaedrus 275a-275b)\n\nThis doesn’t mean that Plato abandoned on writing as he did more of it in his latter works. He suggested that writing can be useful when use properly (i.e when memory deterorate with age or helping student when one discuss philosophy). Nonetheless, some scholar believed that Plato’s important ideas may not be written down and passes via oral mean only.\n\n\n\n\n\n\nFootnotes\n\n\nThe author of most of the articles is Richard Kraut, where he is the prominant scholar on greek philosophy.↩︎\nThis comes from Eutyphro 2c and 3b. Plato has writting a series of dialogues about the trial and death of Socrates, which are (in chronological order): Eutyphro, Apology, Crito and Phaedo. All of them will be closely examined in later blog posts.↩︎\nThis is based on 19th century works and Aristotle.↩︎\nThis is a quotation from the Orcacle (see Apology 23a) given as the explanation to the riddle that Socrates tried to understand, stated as: Delphi (the Orcale) declared, when Chaerephon asked who is the wisest, that no one was wiser [than Socrates] (Apology 20e-21a). This puzzled Socrates since he is conscious that I am not wise either much or little (Apology 21b). We will explore this in more detail in later posts.↩︎\nFor More details, in Apology Socrates sought to under the Oracle’s statement (see the note above) by examine people who claim to posess some wisdom (Apology 21b) in order to invalidate the Oracle.↩︎\nThis relies on the fact that the soul is immortal, which is yet to be proven. The immortality will be main topic in Phaedo.↩︎\nPlato used mathematics as a tool for explaination. His love for mathematics isn’t presented in the eariler works, while appearing regulary in this middle and late dialogues.↩︎\nPlato also combined the theory of recollection and theory of form, as when we observe 2 equal sticks, as in our mind, we think about Equality. Since both 2 objects are not the same, the act of thinking should be an act of recollection. (Phaedo 74d)↩︎\nHowever, Plato doesn’t encorage truth-seeker to commit suicide in order to be free from the body. He gives the reason that we belong to Gods and thus it is best for us to be free when our owner said so (Phaedo 62d)↩︎\nThat is why he suggested abolishing the private wealth in ruling part as a partial solution, and of course to preven the lower class to be exploited. Furthermore, in Plato’s ideal city should have the sense of community, even if, they don’t share an equal understanding of human good.↩︎\nThe topic discussed in Philebus is concerning about the place of pleasure in the best human life, so it is logical to bring Socrates back as the main interlocutors.↩︎\nThe openning of the of Timaeus alludes to the conversation about the best city (talked to The Republic), but this might holds any weight.↩︎"
  },
  {
    "objectID": "miscellaneous.html",
    "href": "miscellaneous.html",
    "title": "Phu's Website",
    "section": "",
    "text": "Here is the collection of writing/exposition/summary of philosophy (and other humanities subjects) notes/talks/books/chapters, please proceed with causion…\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 2, 2022\n\n\nIntroduction to Plato\n\n\nauthor-name-here\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "writings.html",
    "href": "writings.html",
    "title": "Phu's Website",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 16, 2024\n\n\nYoneda Lemma: An Exploration (WIP)\n\n\nauthor-name-here\n\n\n\n\nNov 28, 2023\n\n\nKernel Statistical Test\n\n\nauthor-name-here\n\n\n\n\nNov 27, 2022\n\n\nGaussian That I Have Known and Loved\n\n\nauthor-name-here\n\n\n\n\n\n\nNo matching items"
  }
]