[
  {
    "objectID": "writings.html",
    "href": "writings.html",
    "title": "Phu's Website",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nMay 16, 2024\n\n\nYoneda Lemma: An Exploration (WIP)\n\n\nCategory Theory\n\n\n\n\nNov 28, 2023\n\n\nKernel Statistical Test\n\n\nMachine Learning\n\n\n\n\nNov 27, 2022\n\n\nGaussian That I Have Known and Loved\n\n\nMachine Learning\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hey!",
    "section": "",
    "text": "When we first begin to believe anything, what we believe is not a single proposition, it is a whole system of propositions. (Light dawns gradually over the whole.)1\n—Ludwig Wittgenstein’s On Certainty §141\nNice to meet you. I’m Phu Sakulwongtana (ภูร์ สกุลวงศ์ธนา) from Thailand. I’m generally interested in machine learning and mathematics (especially category theory and its application).\nIf you find any mistakes and/or wanted to contact me, please send an email to:\nwithout any separation between first and last name"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Hey!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI first heard this quote from The Pittsburgh School of Philosophy↩︎"
  },
  {
    "objectID": "miscellaneous.html",
    "href": "miscellaneous.html",
    "title": "Phu's Website",
    "section": "",
    "text": "Here is the collection of writing/exposition/summary of philosophy (and other humanities subjects) notes/talks/books/chapters, please proceed with causion…\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nAug 2, 2022\n\n\nIntroduction to Plato\n\n\nphilosophers\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/3-yoneda-explore/index.html",
    "href": "posts/3-yoneda-explore/index.html",
    "title": "Yoneda Lemma: An Exploration (WIP)",
    "section": "",
    "text": "Required Knowledge: some conformation with what category is, for the latter section, you would need to understand the notion of functor and natural transformation.\nThe notes are adapted from The Dao of Functional Programming by Bartosz Milewski and Notes on Category Theory with examples from basic mathematics by Paolo Perrone"
  },
  {
    "objectID": "posts/3-yoneda-explore/index.html#terminal-object",
    "href": "posts/3-yoneda-explore/index.html#terminal-object",
    "title": "Yoneda Lemma: An Exploration (WIP)",
    "section": "Terminal Object",
    "text": "Terminal Object\nLet’s go back to the Remark 4, and generalize the notion of singleton.\n\n\n\n\n\n\nDefinition\n\n\n\n\nDefinition 2 (Initial and Terminal Objects): We will consider 2 kinds of objects that have the similar way of defining:\n\nInitial object, denoted \\(0\\), is an object that have unique morphism from itself to every object (including itself). We see it as an object that expands toward all the other objects\nOn the other hand, terminal object, denoted \\(1\\), is an object that have unique morphism from every object to itself (including itself). We can see it as an object that all arrows converges.\n\nWe have the following illustration for this kind of objects:\n\n\n\nNote that we can still have arrow coming into initial object or arrow coming out of terminal object, but these won’t enjoy the special properties.\n\n\n\nBefore we move on, we want to note that the uniqueness of arrow (part of what is called universal property) is of extreme importance, and related directly to the probing action (Yoneda lemma), as we will see. In the case of Set, we have that:\n\n\n\n\n\n\nRemark\n\n\n\n\nRemark 2. In \\(\\textbf{Set}\\), an empty set \\(\\emptyset\\) is an initial object, and singleton set (which we denote collectively as \\(1\\)) is a terminal object:\n\nThus, in general, one can generalize the selection of elements in the object \\(A\\) using the morphism from a terminal object to \\(A\\). Therefore, with \\(f:A\\to B\\), the notion \\(f(A)\\) is actually a composition with arrow \\(A:1\\to A\\) i.e \\(f\\circ A\\)\nIt may be quite clear from now that there can be “multiple” initial and terminal object, for example, in \\(\\textbf{Set}\\), the terminal objects are \\(\\{1\\}, \\{2\\}, \\{\\blacksquare\\},\\dots\\). But we can show that it is unique .\n\n\n\n\nAgain, there can be multiple number of intial/terminal objects, but, rest-assure, they are all isomorphic to each other, as we have:\n\n\n\n\n\n\nPropositions\n\n\n\n\nProposition 3 : A terminal (initial) object are unique up to isomorphism. In other words, given 2 objects \\(A\\) and \\(A'\\) both having a property of terminal (initial) object, then \\(A\\cong A'\\)\n\n\n\n\nWe will provide the proof for terminal object only, but the proof for initial object is similar. We note that by definition, there is an unique arrow from \\(f : A\\to A'\\) and \\(g: A'\\to A\\):\n\nWe see that when both of them are composed i.e \\(g\\circ f:A\\to A\\) and \\(f\\circ g:A'\\to A'\\), they will give rise to identities functions \\(\\operatorname{id}_A\\) and \\(\\operatorname{id}_{A'}\\)\nThat is because identity is the only morphism that maps both object to itself by the definition of the terminal objects.\n\nThus \\(f\\) and \\(g\\) are inverse of each other and \\(f\\) is an isomorphism, as needed.\n\n□\n\n\nProof"
  },
  {
    "objectID": "posts/3-yoneda-explore/index.html#product",
    "href": "posts/3-yoneda-explore/index.html#product",
    "title": "Yoneda Lemma: An Exploration (WIP)",
    "section": "Product",
    "text": "Product\nNow, we are ready to start defining the product, so let’s start with the product in \\(\\textbf{Set}\\), first:\n\n\n\n\n\n\nRemark\n\n\n\n\nRemark 3. If we remember well, given set \\(X\\) and \\(Y\\), one can define the cartesian product between them as:\n\\[\nX\\times Y = \\big\\{ (x, y) : x\\in X \\text{ and } y \\in Y \\big\\}\n\\]\nThe interesting part of the cartesian product is that its element, one can recover the element of both sets via the projecton map \\(\\pi_1:X\\times Y\\to X\\) and \\(\\pi_2:X\\times Y\\to Y\\), where both \\(p_1((x', y'))=x'\\) and \\(p_2((x',y'))=y'\\). One can view cartesian product as accurate way to mix of elements of \\(X\\) and \\(Y\\), while we can recover the original information using the projection.\n\n\n\nWith this in mind, we can define a product in general as:\n\n\n\n\n\n\nDefinition\n\n\n\n\nDefinition 3 (Categorical Product): The product between \\(A\\) and \\(B\\) in category \\(\\textbf{C}\\) is denoted as \\(A\\times B\\) together with morphism \\(p_1:A\\times B\\to A\\) and \\(p_2:A\\times B\\to B\\) i.e \\((A\\times B, p_1, p_2)\\) such that given any object \\(X\\) with arrow \\(f_1:X\\to A\\) and \\(f_2:X\\to B\\), there is unique arrow \\(h:X\\to A\\times B\\) (and given unique \\(h\\) there is unique pair of arrow) such that the following commutative diagram holds:\n\n\n\n\n\n\nNow, we can recover the cartesian product from the categorical product on \\(\\textbf{Set}\\) category.\n\n\n\n\n\n\nPropositions\n\n\n\n\nProposition 4 : Categortical product between \\(A\\) and \\(B\\) in \\(\\textbf{Set}\\) is the Cartesian product \\(A\\times B\\), and Cartesian product \\(A\\times B\\) is the categorical product.\n\n\n\n\n(Part 1): We can set the object \\(X\\) with a terminal object \\(1\\) (see the figure below, which is the singleto set). Recall that map from a terminal object selects an element of the target.\n\n\n\nThen, we can have the \\(f_1\\) selects one object \\(a\\) from \\(A\\) and \\(f_2\\) selects one object \\(b\\) from \\(B\\), then \\(h\\), in order to enforce the uniqueness can be set to selecting a pair of \\((a, b)\\). Finally, by the commutativity, the map \\(p_1\\circ h=p_1((a, b)):1\\to A\\) should select the same object as \\(f_1\\) which is \\(a\\). Thus \\(p_1\\) is a projection of first element. The same can be shown with \\(p_2\\). So, the first part has been proven.\n(Part 2): To show that cartesian product is a categorical product, we will have to construct \\(h\\) given \\((f_1,f_2)\\) and vice versa with any set \\(X\\). Let’s consider such construction:\n\nGiven the function \\(f_1:X\\to A\\) and \\(f_2:X\\to B\\), then \\(h:X\\to A\\times B\\) can be set to be \\(h(x)=(f_1(x), f_2(x))\\)\nOn the other hand, with a function \\(h\\) one can use the commutativity to define both \\(f_1\\) and \\(f_2\\) i.e \\(f_1 = p_1\\circ h\\) and \\(f_2 = p_2\\circ h\\)\n\n\n□\n\n\nProof"
  },
  {
    "objectID": "posts/3-yoneda-explore/index.html#results-on-products",
    "href": "posts/3-yoneda-explore/index.html#results-on-products",
    "title": "Yoneda Lemma: An Exploration (WIP)",
    "section": "Results on Products",
    "text": "Results on Products\nIn the next few results, we are also going to proof a simple arthemtics for the product (and to show how the universal construction can be used).\n\n\n\n\n\n\nPropositions\n\n\n\n\nProposition 5 : Given object \\(A\\) in category \\(\\textbf{C}\\) that has terminal object \\(1\\), we can show that \\(A\\times 1\\cong A\\)\n\n\n\n\nWe consider the following diagram:\n\n\n\nOne can see clearly that \\(p_1\\circ h=\\operatorname{id}_A\\), we are left to show that \\(h\\circ p_1=\\operatorname{id}_{A\\times1}\\). To do this, we have the following comparision diagram (every thing commutes):\n\n\n\nPlease note that by definition of terminal object, \\(p_2=!_{A\\times1}\\) and it is unique (hence the \\(A\\times1\\to1\\) on the right edge of LHS diagram is correct). Since the definition of product holds that there is a unique pair of arrows, we can see that \\(h\\circ p_1=\\operatorname{id}_{A\\times1}\\)\n\n□\n\n\nProof\n\n\nand we can also prove the commutativity of a categorical product.\n\n\n\n\n\n\nPropositions\n\n\n\n\nProposition 6 : Given object \\(A\\) and \\(B\\) in category \\(\\textbf{C}\\), we can show that \\(A\\times B\\cong B\\times A\\)\n\n\n\n\nWe consider the following diagrams:\n\n\n\nWe will claim that \\(h\\) and \\(h'\\) are inverse of each other i.e \\(h'\\circ h=\\operatorname{id}_{B\\times A}\\) and \\(h\\circ h'=\\operatorname{id}_{A\\times B}\\), thus \\(h\\) is isomorphism. To show this, we can stack diagram above up (to get the LHS version and it is clear that it commutes).\n\n\n\nOn the RHS, we have the obvious commutative diagram, but please note that by definition of categorical product (the universal construction), given a pair \\((p_1',p_2')\\), we have the unique correspondance to the map \\(\\operatorname{id}_{B\\times A}\\). Thus, \\(h'\\circ h=\\operatorname{id}_{B\\times A}\\) by the uniquenes. The proof that \\(h\\circ h'=\\operatorname{id}_{A\\times B}\\) follows in similar manners.\n\n□\n\n\nProof\n\n\nFinally, we can consider the functoriality of the product, in which the “lifting of the morphism can be performed. (we will discuss the actual notion of functor in later section).\n\n\n\n\n\n\nRemark\n\n\n\n\nRemark 4. (Parallel Application of Product) Given two morphisms \\(f:A\\to A'\\) and \\(g:B\\to B'\\), then we can construct the map \\(f\\times g :A\\times B\\to A'\\times B'\\) based on these 2 function as follows:"
  },
  {
    "objectID": "posts/3-yoneda-explore/index.html#hom-functor",
    "href": "posts/3-yoneda-explore/index.html#hom-functor",
    "title": "Yoneda Lemma: An Exploration (WIP)",
    "section": "Hom-Functor",
    "text": "Hom-Functor\nFrom the construction that we have seen so far (and might as well be the central lession of category theory), everything should be defined based on the relationship.\n\n\n\n\n\n\nRemark\n\n\n\n\nRemark 6. (Information in Hom-Functor) With this, it is crucial to consider the hom-functor i.e \\(\\operatorname{Hom}_{\\textbf{C}}(-, X):\\textbf{C}^\\text{op}\\to\\textbf{Set}\\) i.e a functor that given the object in \\(\\textbf{C}\\) will returns the information about arrows that has \\(X\\) as the target. In other words, the information about arrows into object \\(X\\) can be stored within this functor.\n\n\n\nLet’s define the functor formally:\n\n\n\n\n\n\nDefinition\n\n\n\n\nDefinition 4 (Hom-Functor): The hom-functor denoted as \\(\\operatorname{Hom}_{\\textbf{C}}(-, X):\\textbf{C}^\\text{op}\\to\\textbf{Set}\\), where it has the following action on object and morphism:\n\nObject: Given object \\(Y\\) of \\(\\textbf{C}\\) (as \\(\\textbf{C}^\\text{op}\\) have the same object as \\(\\textbf{C}\\)), then \\(\\operatorname{Hom}_{\\textbf{C}}(Y, X)\\) is the set.\nMorphism: Given the morphism \\(f^\\text{op}:Y\\to Z\\) (in \\(\\textbf{C}^\\text{op}\\)), then we can create (lift) the morphism of \\(\\textbf{C}\\) to get a morphism \\(\\textbf{Set}\\) as follows: \\[\n\\begin{aligned}\n\\operatorname{Hom}_{\\textbf{C}}(-, X)[f^\\text{op}] : \\operatorname{Hom}_{\\textbf{C}}(Y, X)&\\to\\operatorname{Hom}_{\\textbf{C}}(Z, X) \\\\\ng&\\mapsto g\\circ f\n\\end{aligned}\n\\] Please note that the direction of the lifted map is \\(Y\\to Z\\), as the same as \\(f^\\text{op}\\) (and note that \\(f:Z\\to Y\\))\n\n\n\n\nTo push the metaphor further, we can try to do the following\n\n\n\n\n\n\nRemark\n\n\n\n\nRemark 7. (Usage of Hom-Functor): Given the hom-functor, we can compare the objects based on the arrow into it i.e we can study the natural transformation between 2 hom-functors. For example, if we want to compare between \\(X\\) and \\(Y\\), then we can consider the following natural transformation \\(\\operatorname{Hom}_{\\textbf{C}}(-, X)\\Rightarrow\\operatorname{Hom}_{\\textbf{C}}(-, Y)\\).\nBut, of course, we can generalize a general case, in which we consider the natural transformation between hom-functor and an arbitary functor i.e \\(F:\\textbf{C}^\\text{op}\\to \\textbf{Set}\\).\n\n\n\nIn the best case, we can get a natural isomorphism between hom-functor and any functor. This leads to the notion of representable functors\n\n\n\n\n\n\nDefinition\n\n\n\n\nDefinition 5 (Representable Presheaf): Given category \\(\\textbf{C}\\), the functor \\(F:\\textbf{C}^\\text{op}\\rightarrow\\textbf{Set}\\) with is naturally isomorphic to \\(\\operatorname{Hom}_\\textbf{C}(-, X):\\textbf{C}\\rightarrow \\textbf{Set}\\), then we have representable presheaf and \\(X\\) is called representing object.\n\n\n\nWith the notion of representable, we are going to stick to the concept of reconstruction1. That is, with a representable presheaf, we only have to inspect the arrows from \\(Y\\to X\\) just to get what \\(F\\) does i.e a single snap shot on \\(X\\) from \\(Y\\) to get what we need.\nIn a less ideal case, we would need every angles possibles just to reconstruct the action of functor on one object. This is how Yoneda lemma works."
  },
  {
    "objectID": "posts/3-yoneda-explore/index.html#the-lemma",
    "href": "posts/3-yoneda-explore/index.html#the-lemma",
    "title": "Yoneda Lemma: An Exploration (WIP)",
    "section": "The Lemma",
    "text": "The Lemma\nNow, we are ready to state the Yoneda lemma.\n\n\n\n\n\n\nTheorem\n\n\n\n\nTheorem 1 (Yoneda Lemma): With category \\(\\textbf{C}\\), an its object \\(X\\) and presheaf \\(F:\\textbf{C}^\\text{op}\\rightarrow\\textbf{Set}\\), then the map2:\n\\[\nよ:\\operatorname{Hom}_{[\\textbf{C}^\\text{op}, \\textbf{Set}]}\\Big( \\operatorname{Hom}_\\textbf{C}(-, X), F \\Big) \\xrightarrow{\\cong} FX\n\\]\nassigning a natural transformation \\(\\alpha:\\operatorname{Hom}_\\textbf{C}(-,X)\\Rightarrow F\\) to an element \\(\\alpha_X(\\operatorname{id}_X)\\in FX\\). The assignment is bijective and it is natural both in \\(X\\) and \\(F\\).\n\n\n\nThe proof will be presented in the next section. Before that, there are several observations that we can make here:\n\nWe take information about how \\(FY\\) looks like3, and the information about how \\(Y\\) is related to \\(X\\) to inform ourselves how to use the some aspect of \\(FY\\) to reconstruct an element in \\(FX\\) that is selected via natural transformation \\(\\alpha_X(\\operatorname{id}_X)\\).\n\nFurthermore, we see that:\n\nThis natural transformation of identity can be taken as a self-representation of that particular element of \\(FX\\).\nObserve that \\(\\operatorname{id}_X\\) is seen in the proof of Proposition 1.\n\nLet’s consider things in a more concrete example:\n\n\n\n\n\n\nRemark\n\n\n\n\nRemark 8. In this example, we are interested in reconstructing the \\(FX\\) based on the information of the others. We have the following diagram:\n\n\n\n\n\nFirstly, we want to point out that the functor \\(F:\\textbf{C}^\\text{op}\\to\\textbf{Set}\\) can be seen as “rendering” the objects and morphism to a more human readable format, as we are more familiar working with sets.\n(Meaning of Morphism): With functor \\(F\\), the object is rendered as sets of objects with different boarder, color and shape. On the other hand, the morphism is rendered as a relation between elements of objects if they have the same values of the properties4:\n\nThe morphism \\(F\\operatorname{Shape}^\\text{op}\\) (\\(F\\) adds meanings to the morphism) acts as follows: the LHS indicates that the shapes don’t match: \\[\nF\\operatorname{Shape}^\\text{op}(\\Box, \\bigcirc)=\\texttt{F} \\qquad \\quad F\\operatorname{Shape}^\\text{op}(\\blacksquare, \\Box)=\\texttt{T}\n\\] On the RHS, there is a relationship because the shape maps.\nFurthermore, the composition of the relationship will be true if every components are true. For example, \\(F\\operatorname{Shape}^\\text{op}\\circ F\\operatorname{Color}^\\text{op}\\) is true with \\(\\bigcirc\\mapsto\\Box\\mapsto\\blacksquare\\). Finally, \\(F\\operatorname{id}\\) is always true regardless of any relationship, as we have a self-indentity.\n\n(Meaing of Natural Transformation): For the natural transformation, with component \\(\\alpha_Y:\\operatorname{Hom}_{\\textbf{C}}(Y, X)\\to FX\\). Since we are working on the relation (rather than function), it relates an fixed element (and only varies when we have different natural transformation i.e aspect of \\(FY\\)), in our case it refers to the top left object at \\(Y\\) (see how orange arrow selects an object), to the relations (morphism), and output the property of tha object:\n\\[\n\\alpha_Y(\\operatorname{Color}) = \\textcolor{ForestGreen}{\\text{Green}} \\qquad \\quad \\alpha(\\operatorname{Boarder}) = \\text{Solid}\n\\]\nThere are only 2 arrows from \\(X\\) to \\(Y\\). To see what the natural transformation relates element of \\(FX\\) by taking some aspect of \\(FY\\), we have to consider the following commutative diagram, representing the naturality of \\(\\alpha\\):\n\n\n\nThen, given \\(F\\operatorname{Color}^\\text{op}(\\alpha_X(\\operatorname{id}_X))=\\alpha_Y(\\operatorname{id}_X\\circ\\operatorname{Color})=\\alpha_Y(\\operatorname{Color})\\), or:\n\\[\nF\\operatorname{Color}^\\text{op}\\Big( \\alpha_X(\\operatorname{id}_X), \\textcolor{ForestGreen}{\\text{Green}} \\Big) = \\texttt{T}\n\\]\nSince \\(\\alpha_X(\\operatorname{id}_X)\\) represents the self-identity of the whole selected element of \\(FX\\) (see arrow), the naturality condition forces its color to be green.\n\nThis can be repeated for other properties such as boarder types and different property i.e shape from \\(Z\\), and repeated for every element in \\(FX\\) via considering all natural transformation.\nAnd this is repeated for other selection of elements as we consider a set of natural transformations.\nTherefore, we can reconstruct elements of \\(FX\\) by inherited the color and boarder (relationship between \\(X\\) and \\(Y\\)) from the element within the “render” image \\(FY\\) of \\(Y\\), while inherited the shape from \\(FZ\\) (and so on).\n\nThus, we arrived at the intuition we have above: we need to know the relations between objects \\(X\\) and the other objects (hom-functor) and how these relations translates to each element of the set (natural transformation between hom-functor and \\(F\\)), in order to reconstruct the element of \\(FX\\).\n\n\n\nIn fact, we can state the intuitive that we have more formally in the following corollaries:\n\n\n\n\n\n\nCorollary\n\n\n\n\n(Yoneda Embedding): With category \\(\\textbf{C}\\), and its object \\(X\\) and \\(Y\\) there is a bijection between the following sets:\n\\[\n\\operatorname{Hom}_\\textbf{C}(X, Y) \\cong \\operatorname{Hom}_{[\\textbf{C}^\\text{op}, \\textbf{Set}]}\\Big( \\operatorname{Hom}_{\\textbf{C}}(-, X), \\operatorname{Hom}_{\\textbf{C}}(-, Y) \\Big)\n\\]\n\n\n\n\nUsing Yoneda lemma, we replace presheaf \\(F\\) by \\(\\operatorname{Hom}_\\textbf{C}(-, Y)\\)\n\n□\n\n\nProof\n\n\n\n\n\n\n\n\nCorollary\n\n\n\n\nWith category \\(\\textbf{C}\\), and its object \\(X\\) and \\(Y\\), then \\(X\\cong Y\\) iff hom-functors that they represent are naturally isomorphic.\n\n\n\n\nGiven morphism between \\(X\\) and \\(Y\\) to be \\(f:X\\to Y\\), then by Yoneda embedding there is an associated natural transformation \\(\\alpha:\\operatorname{Hom}_\\textbf{C}(-, X)\\Rightarrow\\operatorname{Hom}_\\textbf{C}(-, Y)\\) such that \\(\\alpha_X(\\operatorname{id}_X)=f\\).\n\\(\\boldsymbol{(\\implies):}\\) Since \\(X\\) and \\(Y\\) are isomorphic, we can let \\(f\\) above be that isomorphism between both objects. By using the natural transformation properties, for any map \\(g:Z\\to X\\), we set \\(\\alpha_Z(g)=f\\circ g\\) because the following diagram has to commutes:\n\n\n\nAnd, it is clear that each component of the natural transformation is inverse by the virtue of \\(f\\) being isomorphism i.e given \\(\\alpha^{-1}_Z(h)=f^{-1}\\circ g\\) for \\(h:Z\\to Y\\)\n\\(\\boldsymbol{(\\impliedby):}\\) We claim that \\(\\alpha_X(\\operatorname{id}_X):X\\to Y\\) is an isomorphism with its inverse being \\(\\alpha_Y^{-1}(\\operatorname{id}_Y):Y\\to X\\), which exists because \\(\\alpha\\) is natural isomorphism. Then by the observation above:\n\\[\n\\operatorname{id}_Y =\\alpha_Y(\\alpha_Y^{-1}(\\operatorname{id}_Y)) = \\alpha_X(\\operatorname{id}_X)\\circ\\alpha_Y^{-1}(\\operatorname{id}_Y)\n\\]\nOn the other hand, we consider the following commutative diagram being the natural isomorphism action on \\(-\\circ\\alpha_X(\\operatorname{id}_X)\\) as:\n\n\n\nAnd because \\(\\alpha_Z\\) is isomorphism, we have that:\n\\[\n\\alpha_Y^{-1}(\\operatorname{id}_Y)\\circ\\alpha_X(\\operatorname{id}_X) = \\operatorname{id}_X\n\\]\n\n□\n\n\nProof\n\n\n\n\n\n\n\n\nRemark\n\n\n\n\nRemark 9. (Interpretation of Result) When looking at the component of natural isomorphism, with any object \\(S\\) of \\(\\textbf{C}\\), and \\(X\\cong Y\\), we have:\n\\[\n\\operatorname{Hom}_\\textbf{C}(S, X)\\cong\\operatorname{Hom}_\\textbf{C}(S, Y)\n\\] That is, if \\(X\\) and \\(Y\\) are indistinguishable by \\(S\\), for every \\(S\\) in \\(\\textbf{C}\\). This also applied to \\(\\operatorname{Hom}_\\textbf{C}(X, S)\\cong\\operatorname{Hom}_\\textbf{C}(Y, S)\\), where we would consider the dual case of Yoneda lemma."
  },
  {
    "objectID": "posts/3-yoneda-explore/index.html#footnotes",
    "href": "posts/3-yoneda-explore/index.html#footnotes",
    "title": "Yoneda Lemma: An Exploration (WIP)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis intuition is related to the Tannakian reconstruction (see The Dao of Functional Programming chapter 18) for more details.↩︎\nよ is read as “yo”, in Hiragana (Japanese writing system).↩︎\nIn the formulation (and symbol) of Ends formulation of ninja Yoneda lemma, this metaphors even more clear in which we takes into account all \\(Y\\): \\[\n\\int_{Y:\\textbf{C}}\\operatorname{Hom}_\\textbf{Set}\\Big( \\operatorname{Hom}_\\textbf{C}(Y, X), FY \\Big) \\cong FX\n\\] (Ibid, chapter 17).↩︎\nThe notion of morphism doesn’t restricted to only function that takes input and output. Nonetheless, the relation can also be seen as function.↩︎"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html",
    "href": "posts/1-GiHKAL/index.html",
    "title": "Gaussian That I Have Known and Loved",
    "section": "",
    "text": "This write is based from the book: Pattern Recognition and Machine Learning, partly from CS229 notes on Guassian, and The Principles of Deep Learning Theory: An Effective Theory Approach to Understanding Neural Networks (especially on the chapter on Wick’s theorem, in which in this written we have provided the exposition on the proof too), where we aim to give and expands most proofs and interesting results regarding Gaussian distributions, together with some results that makes the complete picture."
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#gaussian-definition-and-fundamental-integral",
    "href": "posts/1-GiHKAL/index.html#gaussian-definition-and-fundamental-integral",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Gaussian Definition and Fundamental Integral",
    "text": "Gaussian Definition and Fundamental Integral\nDefinition (Single Variable Gaussian Distribution): The Gaussian distribution is defined as\n\n\\[\\begin{equation*}\n\\newcommand{\\dby}{\\ \\mathrm{d}}\\newcommand{\\argmax}[1]{\\underset{#1}{\\arg\\max \\ }}\\newcommand{\\argmin}[1]{\\underset{#1}{\\arg\\min \\ }}\\newcommand{\\const}{\\text{const.}}\\newcommand{\\bracka}[1]{\\left( #1 \\right)}\\newcommand{\\brackb}[1]{\\left[ #1 \\right]}\\newcommand{\\brackc}[1]{\\left\\{ #1 \\right\\}}\\newcommand{\\brackd}[1]{\\left\\langle #1 \\right\\rangle}\\newcommand{\\correctquote}[1]{``#1''}\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\\newcommand{\\abs}[1]{\\left|#1\\right|}\n    p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{ -\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right\\}\n\\end{equation*}\\]\n\nwhere \\(\\mu\\) is a parameter called mean, while \\(\\sigma\\) is a parameter called standard derivation. However, we also define \\(\\sigma^2\\) as variance. Finally, the standard normal distribution is Gaussian distribution with mean \\(0\\) and variance \\(1\\).\nProposition (Gaussian integral): The integration of \\(\\exp(-x^2)\\) is equal to\n\n\\[\\begin{equation*}\n    \\int^\\infty_{-\\infty}\\exp(-x^2)\\dby x = \\sqrt{\\pi}\n\\end{equation*}\\]\n\nWe can use this identity to find the normalizing factor of Gaussian distribution.\nProof: We will transform the problem into \\(2\\)D polar coordinate system, which make the integration easier.\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\bracka{\\int^\\infty_{-\\infty}\\exp(-x^2)\\dby x}^2 &= \\bracka{\\int^\\infty_{-\\infty}\\exp(-x^2)\\dby x}\\bracka{\\int^\\infty_{-\\infty}\\exp(-y^2)\\dby y} \\\\\n&= \\int^\\infty_{-\\infty}\\int^\\infty_{-\\infty}\\exp\\bracka{-(x^2+y^2)}\\dby x\\dby y \\\\\n&= \\int^{2\\pi}_{0}\\int^\\infty_{0}\\exp\\bracka{-r^2}r\\dby r\\dby\\theta \\\\\n&= 2\\pi\\int^\\infty_{0}\\exp\\bracka{-r^2}r\\dby r = \\pi\\int^{0}_{-\\infty} \\exp\\bracka{u}\\dby u = \\pi\n\\end{aligned}\n\\end{equation*}\\]\n\nPlease note that we use \\(u\\)-substution with \\(-r^2\\), in the last step.\n\n□\n\nCorollary (Normalization of Gaussian Distribution): We consider the integration\n\n\\[\\begin{equation*}\n    \\int^\\infty_{-\\infty}\\exp\\left\\{ -\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\} \\dby x = \\sqrt{2\\pi\\sigma^2}\n\\end{equation*}\\]\n\nProof: Starting by setting \\(z=x-\\mu\\), which we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\int^\\infty_{-\\infty}\\exp\\left\\{ -\\frac{z^2}{2\\sigma^2}\\right\\} \\dby z &= \\sigma\\sqrt{2} \\int^\\infty_{-\\infty}\\frac{1}{\\sqrt{2}\\sigma}\\exp\\left\\{ -\\frac{z^2}{2\\sigma^2}\\right\\} \\dby z \\\\\n&= \\sigma\\sqrt{2} \\int^\\infty_{-\\infty} \\exp(-y^2)\\dby = \\sigma\\sqrt{2\\pi}\n\\end{aligned}\n\\end{equation*}\\]\n\nwhere we have \\(y = z/(\\sqrt{2}\\sigma)\\), and we finishes the proof.\n\n□"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#statistics-of-single-variable-gaussian-distribution",
    "href": "posts/1-GiHKAL/index.html#statistics-of-single-variable-gaussian-distribution",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Statistics of Single Variable Gaussian Distribution",
    "text": "Statistics of Single Variable Gaussian Distribution\nDefinition (Odd Function): The function \\(f(x)\\) is an odd function iff \\(f(-x) = -x\\)\nLemma (Odd Function Integration): The integral over \\([-a, a]\\), where \\(a\\in\\mathbb{R}^+\\) of an odd function \\(f(x)\\) is \\(0\\) i.e\n\n\\[\\begin{equation*}\\int^a_{-a}f(x)\\dby x = 0\\end{equation*}\\]\n\nProof: We can see that:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\int^a_{-a}f(x)\\dby x &= \\int^0_{-a}f(x)\\dby x + \\int^a_{0}f(x)\\dby x \\\\\n    &= \\int^a_{0}f(-x)\\dby x + \\int^a_{0}f(x)\\dby x \\\\\n    &= -\\int^a_{0}f(x)\\dby x + \\int^a_{0}f(x)\\dby x = 0\n\\end{aligned}\n\\end{equation*}\\]\n\nThus complete the proof\n\n□\n\nProposition (Mean of Gaussian): The expectation \\(\\mathbb{E}_{x}[x]\\) of nornally distributed Gaussian is \\(\\mu\\)\nProof: Let’s consider the following integral\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{(x-\\mu)^2}{2\\sigma^2}}x\\dby x\n\\end{aligned}\n\\end{equation*}\\]\n\nWe will set \\(z = x - \\mu\\), where we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\frac{1}{\\sqrt{2\\pi\\sigma^2}}&\\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}(z+\\mu)\\dby x \\\\\n    &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\Bigg[ \\underbrace{\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z\\dby z}_{I_1} + \\underbrace{\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\mu\\dby z}_{I_2} \\Bigg]\n\\end{aligned}\n\\end{equation*}\\]\n\nLet’s consider \\(I_1\\), where it is clear that\n\n\\[\\begin{equation*}\n    g(x) = \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z = -\\bracka{-\\exp\\bracka{-\\frac{(-z)^2}{2\\sigma^2}}z} = -g(-x)\n\\end{equation*}\\]\n\nThus the function \\(g(x)\\) is and odd function. Therefore, making the integration \\(I_1\\) vanishes to \\(0\\). Please see the Lemma above for the proof. Now, for the second integration, we can simply recall the normalization result of the Gaussian, where\n\n\\[\\begin{equation*}\n    \\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\mu\\dby z = \\mu\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\dby z = \\mu\\sqrt{2\\pi\\sigma^2}\n\\end{equation*}\\]\n\nFinally, we have\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}(z+\\mu)\\dby x = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\Bigg[ 0 + \\mu\\sqrt{2\\pi\\sigma^2} \\Bigg] = \\mu\n\\end{aligned}\n\\end{equation*}\\]\n\nThus complete the proof.\n\n□\n\nLemma: The variance \\(\\operatorname{var}(x) = \\mathbb{E}[(x - \\mu)^2]\\) is equal to \\(\\mathbb{E}[x^2] - \\mathbb{E}[x]^2\\)\nProof: This is an application of expanding the definition\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\mathbb{E}[(x - \\mu)^2] &= \\mathbb{E}[x^2 -2x\\mu + \\mu^2] \\\\\n    &= \\mathbb{E}[x^2] - 2\\mathbb{E}[x]\\mu + \\mathbb{E}[\\mu^2] \\\\\n    &= \\mathbb{E}[x^2] - 2\\mathbb{E}[x]^2 + \\mathbb{E}[x]^2 \\\\\n    &= \\mathbb{E}[x^2] - \\mathbb{E}[x]^2 \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\n\n□\n\nProposition: The variance of normal distribution is \\(\\sigma^2\\)\nProof: Let’s consider the following equation, where we set \\(z = x-\\mu\\):\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\frac{1}{\\sqrt{2\\pi\\sigma^2}}& \\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{(x-\\mu)^2}{2\\sigma^2}}x^2\\dby x  = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}(z+\\mu)^2\\dby z \\\\\n    &=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\brackb{\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z^2\\dby z + \\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}2\\mu z\\dby z + \\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\mu^2\\dby z } \\\\\n    &=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\brackb{\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z^2\\dby z + 0 + \\mu^2\\sqrt{2\\pi\\sigma^2} \\dby z }\n\\end{aligned}\n\\end{equation*}\\]\n\nNow let’s consider the first integral, please note that\n\n\\[\\begin{equation*}\n    \\frac{d}{dz} \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}} = \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\bracka{-\\frac{z}{\\sigma^2}}\n\\end{equation*}\\]\n\nSo we can perform an integration by-part considering \\(u=z\\) and \\(dv = \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\bracka{-\\frac{z}{\\sigma^2}}\\):\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\int^\\infty_{-\\infty}&\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z^2\\dby z = -\\sigma^2\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\bracka{-\\frac{z}{\\sigma^2}}z\\dby z \\\\\n    &= -\\sigma^2\\brackb{\\left.z\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\right|^\\infty_{-\\infty} - \\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\dby z} \\\\\n    &= -\\sigma^2[0 - \\sqrt{2\\pi\\sigma^2}] = \\sigma^2\\sqrt{2\\pi\\sigma^2}\n\\end{aligned}\n\\end{equation*}\\]\n\nTo show that the evaluation on the left-hand side is zero, we have\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\lim_{z\\rightarrow\\infty} z\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}} &- \\lim_{z\\rightarrow-\\infty} z\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}} \\\\\n    &= 0 - \\lim_{z\\rightarrow-\\infty}1\\cdot\\exp\\bracka{-\\frac{x^2}{2\\sigma^2}}\\bracka{-\\frac{\\sigma^2}{z}} \\\\\n    &= 0-0 = 0\n\\end{aligned}\n\\end{equation*}\\]\n\nThe first equality comes from L’Hospital’s rule. Combinding the results:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\mathbb{E}[x^2] &- \\mathbb{E}[x]^2 = \\sigma^2 + \\mu^2 - \\mu^2 = \\sigma^2\n\\end{aligned}\n\\end{equation*}\\]\n\nThus complete the proof.\n\n□\n\nThis second part is to introduce some of the mathematical basics such as linear algebra. Further results of linear Gaussian models and others will be presented in the next part."
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#useful-backgrounds",
    "href": "posts/1-GiHKAL/index.html#useful-backgrounds",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Useful Backgrounds",
    "text": "Useful Backgrounds\n\nCovariance and Covariance Matrix\nDefinition (Covariance): Given 2 random variables \\(X\\) and \\(Y\\), the covariance is defined as:\n\n\\[\\begin{equation*}\n\\newcommand{\\dby}{\\ \\mathrm{d}}\\newcommand{\\argmax}[1]{\\underset{#1}{\\arg\\max \\ }}\\newcommand{\\argmin}[1]{\\underset{#1}{\\arg\\min \\ }}\\newcommand{\\const}{\\text{const.}}\\newcommand{\\bracka}[1]{\\left( #1 \\right)}\\newcommand{\\brackb}[1]{\\left[ #1 \\right]}\\newcommand{\\brackc}[1]{\\left\\{ #1 \\right\\}}\\newcommand{\\brackd}[1]{\\left\\langle #1 \\right\\rangle}\\newcommand{\\correctquote}[1]{``#1''}\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\operatorname{cov}(X, Y) = \\mathbb{E}\\Big[ (X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y]) \\Big]\n\\end{equation*}\\]\n\nIt is clear that $(X, Y) = (Y, X) $\nDefinition (Covariance Matrix): Now, if we have the collection of random variables in the random vector \\(\\boldsymbol x\\) (of size \\(n\\)), then the collection of covariance between its elements are collected in covariance matrix\n\n\\[\\begin{equation*}\n\\operatorname{cov}(\\boldsymbol x) =\n\\begin{bmatrix}\n    \\operatorname{cov}(x_1, x_1) & \\operatorname{cov}(x_1, x_2) & \\cdots & \\operatorname{cov}(x_1, x_n)  \\\\\n    \\operatorname{cov}(x_2, x_1) & \\operatorname{cov}(x_2, x_2) & \\cdots & \\operatorname{cov}(x_2, x_n)  \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\operatorname{cov}(x_n, x_1) & \\operatorname{cov}(x_n, x_2) & \\cdots & \\operatorname{cov}(x_n, x_n)  \\\\\n\\end{bmatrix} = \\mathbb{E}\\Big[ (\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])(\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])^T \\Big]\n\\end{equation*}\\]\n\nThe equivalent is clear when we perform the matrix multiplication over this.\nRemark (Property of Covariance Matrix): It is clear that the covariance matrix is (from its defintion):\n\nSymmetric, as \\(\\operatorname{cov}(x_a, x_b) = \\operatorname{cov}(x_b, x_a)\\)\nPositive semidefinite, if we consider arbitary constant vector \\(\\boldsymbol a \\in \\mathbb{R}^n\\), then by the linearity of expectation, we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol a^T \\operatorname{cov}(\\boldsymbol x)\\boldsymbol a\n&= \\boldsymbol a^T \\mathbb{E}\\Big[ (\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])(\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])^T \\Big]\\boldsymbol a  \\\\\n&= \\mathbb{E}\\Big[ \\boldsymbol a^T (\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])(\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])^T\\boldsymbol a \\Big]  \\\\\n&= \\mathbb{E}\\Big[ \\big(\\boldsymbol a^T (\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])\\big)^2 \\Big] &gt; 0 \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\n\n\n\nLinear Algebra\nDefinition (Eigenvalues and Eigenvectors): Given the matrix \\(\\boldsymbol X \\in \\mathbb{C}^{n\\times n}\\), then the pair of vector \\(\\boldsymbol v \\in \\mathbb{C}^n\\) and number \\(\\lambda \\in \\mathbb{C}\\) are called eigenvector and eigenvalue iff\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol A \\boldsymbol v = \\lambda\\boldsymbol v\n\\end{aligned}\n\\end{equation*}\\]\n\nProposition (Eigenvalue of Symmetric Matrix): One can show that the eigenvalue of symmetric (real) matrix is always real.\nProof (From here): Let’s consider the pairs of eigenvalues/eigenvectors \\(\\boldsymbol v \\in \\mathbb{C}^n\\) and \\(\\lambda \\in \\mathbb{C}\\) of symmetric (real) matrix \\(\\boldsymbol A\\) i.e \\(\\boldsymbol A\\boldsymbol v = \\lambda\\boldsymbol v\\). Please note that \\((x+yi)(x-yi) = x^2 + y^2 \\ge 0\\), this means that \\(\\bar{\\boldsymbol v}^T\\boldsymbol v \\ge 0\\) (\\(\\bar{\\boldsymbol v}\\) is vector that contains conjugate element of \\(\\boldsymbol v\\)). Now, see that:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\bar{\\boldsymbol v}^T\\boldsymbol A\\boldsymbol v\n&= \\bar{\\boldsymbol v}^T(\\boldsymbol A\\boldsymbol v) = \\lambda\\bar{\\boldsymbol v}^T\\boldsymbol v  \\\\\n&= (\\bar{\\boldsymbol v}^T\\boldsymbol A^T)\\boldsymbol v = \\bar{\\lambda}\\bar{\\boldsymbol v}^T\\boldsymbol v  \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nNote that \\(\\overline{\\boldsymbol A\\boldsymbol v} = \\bar{\\lambda}\\bar{\\boldsymbol v}\\) Since \\(\\bar{\\boldsymbol v}^T\\boldsymbol v &gt; 0\\) we see that \\(\\bar{\\lambda} = \\lambda\\), which implies that \\(\\bar{\\lambda}\\) is real.\n\n□\n\nProposition (Eigenvalue of Positive Definite Matrix): One can show that the eigenvalue of positive definite matrix is non-negative.\nProof: We consider the following equations:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol v^T\\boldsymbol A\\boldsymbol v = \\lambda\\boldsymbol v^T\\boldsymbol v &gt; 0\n\\end{aligned}\n\\end{equation*}\\]\n\nAnd so the eigenvector must be \\(\\lambda &gt; 0\\).\n\n□\n\nDefinition (Linear Transformation): Given the function \\(A: V \\rightarrow W\\), where \\(V\\) and \\(W\\) are vector spaces. Then, for \\(\\boldsymbol v \\in V, \\boldsymbol w \\in W\\) and \\(a, b \\in \\mathbb{R}\\)\n\n\\[\\begin{equation*}\n\\begin{aligned}\nA(a\\boldsymbol v + b\\boldsymbol w) = aA(\\boldsymbol v) + bA(\\boldsymbol w)\n\\end{aligned}\n\\end{equation*}\\]\n\nRemark (Matrix Multipliacation and Linear Transformation): We can represent the linear transformation \\(L : V \\rightarrow W\\) in terms of matrix. Let’s consider the vector \\(\\boldsymbol v \\in V\\) (of dimension \\(n\\)) together with basis vectors \\(\\brackc{\\boldsymbol b_1,\\dots,\\boldsymbol b_n}\\) of \\(V\\) and basis vectors \\(\\brackc{\\boldsymbol c_1, \\dots, \\boldsymbol c_m}\\) of \\(W\\). Then we can represen the vector \\(\\boldsymbol v\\) as:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol v = v_1\\boldsymbol b_1 + \\dots + v_n\\boldsymbol b_n\n\\end{aligned}\n\\end{equation*}\\]\n\nThis means that we can represent the vector \\(\\boldsymbol v\\) as: \\((\\boldsymbol v_1, \\boldsymbol v_2, \\dots, \\boldsymbol v_n)^T\\) Furthermore, we can characterized the transformation of the basis vector:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    &L(\\boldsymbol b_i) = l_{1i} \\boldsymbol c_1 + l_{2i}\\boldsymbol c_2 + \\cdots + l_{mi}\\boldsymbol c_m \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nfor \\(i=1,\\dots,n\\). Then we can see that the definition of linear transformation together with the linear transformation of basis as we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    L(\\boldsymbol v) &= v_1L(\\boldsymbol b_1) + \\dots + v_nL(\\boldsymbol v_n) \\\\\n    &= \\begin{aligned}[t]\n        &v_1\\Big( l_{11} \\boldsymbol c_1 + l_{21}\\boldsymbol c_2 + \\cdots + l_{m1}\\boldsymbol c_m \\Big) \\\\\n        &+v_2\\Big( l_{12} \\boldsymbol c_1 + l_{22}\\boldsymbol c_2 + \\cdots + l_{m2}\\boldsymbol c_m \\Big) \\\\\n        &+v_n\\Big( l_{1n} \\boldsymbol c_1 + l_{2n}\\boldsymbol c_2 + \\cdots + l_{mn}\\boldsymbol c_m \\Big)\\\\\n    \\end{aligned} \\\\\n    &= \\begin{bmatrix}\n        \\sum^n_{i=1}v_il_{1i} \\\\ \\sum^n_{i=1}v_il_{2i} \\\\ \\vdots \\\\ \\sum^n_{i=1}v_il_{ni}\n    \\end{bmatrix} = \\begin{bmatrix}\n        l_{11} & l_{12} & \\cdots & l_{1n} \\\\\n        l_{21} & l_{22} & \\cdots & l_{2n} \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        l_{m1} & l_{m2} & \\cdots & l_{mn} \\\\\n    \\end{bmatrix}\n    \\begin{bmatrix}\n        v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n\n    \\end{bmatrix}\n\\end{aligned}\n\\end{equation*}\\]\n\nand so we have show that the linear transformation can be represented as matrix multiplication (in finite space).\nTheorem (Spectral Theorem) (follows from here): Let \\(n \\le N\\) and \\(W\\) be an n-dimensional subspace of \\(\\mathbb{R}^n\\). Given a linear transformation \\(A:W\\rightarrow W\\) that is symmetric. There are eigenvectors \\(\\boldsymbol v_1,\\dots,\\boldsymbol v_n\\in W\\) of \\(A\\) such that \\(\\brackc{\\boldsymbol v_1,\\dots,\\boldsymbol v_n}\\) is an orthonormal basis for \\(W\\). For normal matrix, we let \\(n=N\\) and \\(W=\\mathbb{R}^n\\).\nRemark (Eigendecomposition): Let’s consider the matrix of eigenvectors \\(\\boldsymbol v_1,\\boldsymbol v_2\\dots,\\boldsymbol v_n \\in \\mathbb{R}^n\\) of symmetric matrix \\(\\boldsymbol A \\in \\mathbb{R}^{n\\times n}\\) together with eigenvalues \\(\\lambda_1,\\lambda_2,\\dots,\\lambda_n\\), then we have :\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\boldsymbol A\n    \\begin{bmatrix}\n        \\kern.6em\\vline & \\kern.2em\\vline\\kern.2em & & \\vline\\kern.6em \\\\\n        \\kern.6em\\boldsymbol v_1 & \\kern.2em\\boldsymbol v_2\\kern.2em & \\kern.2em\\cdots\\kern.2em &  \\boldsymbol v_n\\kern.6em  \\\\\n        \\kern.6em\\vline & \\kern.2em\\vline\\kern.2em & & \\vline\\kern.6em \\\\\n    \\end{bmatrix} &=\n    \\begin{bmatrix}\n        \\kern.6em\\vline & \\kern.2em\\vline\\kern.2em & & \\vline\\kern.6em \\\\\n        \\kern.6em\\boldsymbol v_1 & \\kern.2em\\boldsymbol v_2\\kern.2em & \\kern.2em\\cdots\\kern.2em &  \\boldsymbol v_n\\kern.6em  \\\\\n        \\kern.6em\\vline & \\kern.2em\\vline\\kern.2em & & \\vline\\kern.6em \\\\\n    \\end{bmatrix}\\begin{bmatrix}\n        \\lambda_1 & 0 & \\cdots & 0 \\\\\n        0 & \\lambda_2 & \\cdots & 0 \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & \\lambda_n \\\\\n    \\end{bmatrix} \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nThis is equivalent to \\(\\boldsymbol A\\boldsymbol Q = \\boldsymbol Q\\boldsymbol \\Lambda\\), which mean that if we right multiply by \\(\\boldsymbol Q^T\\) (as we have orthogonal eigenvectors), then we have (or in vectorized format):\n\n\\[\\begin{equation*}\n\\boldsymbol A = \\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^T = \\sum^n_{i=1}\\lambda_i\\boldsymbol v_i\\boldsymbol v_i^T\n\\end{equation*}\\]\n\nPlease note that \\(\\boldsymbol A^{-1}\\) can be represented as:\n\n\\[\\begin{equation*}\n\\boldsymbol A^{-1} = \\boldsymbol Q\\boldsymbol \\Lambda^{-1}\\boldsymbol Q^T = \\sum^n_{i=1}\\frac{1}{\\lambda_i}\\boldsymbol v_i\\boldsymbol v_i^T\n\\end{equation*}\\]\n\nas it is clear that \\(\\boldsymbol A\\boldsymbol A^{-1} = \\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^T\\boldsymbol Q\\boldsymbol \\Lambda^{-1}\\boldsymbol Q^T = \\boldsymbol I\\).\nProposition (Determinant and Eigenvalues): Give matrix \\(\\boldsymbol A\\) together with eigenvalues of \\(\\lambda_1,\\lambda_2,\\dots,\\lambda_n\\), then we can show that\n\n\\[\\begin{equation*}\n    \\abs{\\boldsymbol A} = \\prod^n_{i=1}\\lambda_i\n\\end{equation*}\\]\n\nProof (Diagonalizable Matrix): We consider the eigendecomposition of \\(\\boldsymbol A\\) (if it exists, which is most of the cases here. For more general proof, see linear algebra notes), as we have:\n\n\\[\\begin{equation*}\n    \\abs{\\boldsymbol A} = \\abs{\\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^{-1}} = \\abs{\\boldsymbol Q}\\abs{\\boldsymbol \\Lambda}\\abs{\\boldsymbol Q^{-1}} = \\frac{\\abs{\\boldsymbol Q}}{\\abs{\\boldsymbol Q}}\\abs{\\boldsymbol \\Lambda} =  \\prod^n_{i=1}\\lambda_i\n\\end{equation*}\\]\n\n\n□\n\nProposition (Trace and Eigenvalues): Given a matrix \\(\\boldsymbol A\\) together with eigenvalues of \\(\\lambda_1,\\lambda_2,\\dots,\\lambda_n\\), then we can show that:\n\n\\[\\begin{equation*}\n    \\operatorname{Tr}(\\boldsymbol A) = \\sum^n_{i=1}\\lambda_i\n\\end{equation*}\\]\n\nProof (Diagonalizable Matrix): We consider the eigendecomposition of \\(\\boldsymbol A\\). Consider the trace over it, as we have:\n\n\\[\\begin{equation*}\n    \\operatorname{Tr}(\\boldsymbol A) = \\operatorname{Tr}(\\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^{-1}) = \\operatorname{Tr}(\\boldsymbol \\Lambda\\boldsymbol Q\\boldsymbol Q^{-1}) = \\operatorname{Tr}(\\boldsymbol \\Lambda) = \\sum^n_{i=1}\\lambda_i\n\\end{equation*}\\]\n\n\n\nMiscellaneous\nProposition (Change of Variable): If we consider the transformation of the variables where \\(T : \\mathbb{R}^k \\supset X \\rightarrow \\mathbb{R}^k\\). Then we can show that:\n\n\\[\\begin{equation*}\n\\int_{\\mathbb{R}^k} f(\\boldsymbol y)\\dby \\boldsymbol y = \\int_{\\mathbb{R}^k} f(\\boldsymbol T(\\boldsymbol x))\\abs{\\boldsymbol J_T(\\boldsymbol x)}\\dby \\boldsymbol x\n\\end{equation*}\\]\n\nwhere \\(\\boldsymbol J_T(\\boldsymbol x)\\) is the Jacobian of the transformation \\(T(\\cdot)\\), which is defined to be:\n\n\\[\\begin{equation*}\n\\boldsymbol J_T(\\boldsymbol x) = \\begin{pmatrix}\n    \\cfrac{\\partial T_1(\\cdot)}{\\partial x_1} & \\cfrac{\\partial T_1(\\cdot)}{\\partial x_2}  & \\cdots & \\cfrac{\\partial T_1(\\cdot)}{\\partial x_n} \\\\\n    \\cfrac{\\partial T_2(\\cdot)}{\\partial x_1} & \\cfrac{\\partial T_2(\\cdot)}{\\partial x_2}  & \\cdots & \\cfrac{\\partial T_2(\\cdot)}{\\partial x_n} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\cfrac{\\partial T_n(\\cdot)}{\\partial x_1} & \\cfrac{\\partial T_n(\\cdot)}{\\partial x_2}  & \\cdots & \\cfrac{\\partial T_n(\\cdot)}{\\partial x_n} \\\\\n\\end{pmatrix}\n\\end{equation*}\\]"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#multivariate-guassian-distribution-introduction",
    "href": "posts/1-GiHKAL/index.html#multivariate-guassian-distribution-introduction",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Multivariate Guassian Distribution: Introduction",
    "text": "Multivariate Guassian Distribution: Introduction\nDefinition (Multivariate Gaussian): It is defined as:\n\n\\[\\begin{equation*}\n\\mathcal{N}(\\boldsymbol x | \\boldsymbol \\mu, \\boldsymbol \\Sigma) = \\frac{1}{\\sqrt{\\abs{2\\pi\\boldsymbol \\Sigma}}}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu)\\right\\}\n\\end{equation*}\\]\n\nwhere we call \\(\\boldsymbol \\mu \\in \\mathbb{R}^n\\) a mean and \\(\\boldsymbol \\Sigma \\in \\mathbb{R}^{n\\times n}\\) covariance, which should be symmetric and positive semidefinite (since the covariance is always positive semidefinite and symmetric).\nRemark (2D Independent Gaussian): Now, let’s consider, multivariate Gaussian but in the case that both variables are independent to each other with difference variances, as we define the parameters to be:\n\n\\[\\begin{equation*}\n\\boldsymbol \\mu = \\begin{bmatrix}\n    \\mu_1 \\\\ \\mu_2\n\\end{bmatrix} \\qquad \\boldsymbol \\Sigma =\n\\begin{bmatrix}\n    \\sigma_1^2 & 0 \\\\\n    0 & \\sigma_2^2 \\\\\n\\end{bmatrix}\n\\end{equation*}\\]\n\nNow, let’s expand the multivariate Guassian, please note that \\(\\abs{\\boldsymbol \\Sigma} = \\sigma_1^2\\sigma_2^2\\):\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\mathcal{N}(\\boldsymbol x | \\boldsymbol \\mu, \\boldsymbol \\Sigma) &= \\frac{1}{\\sqrt{\\abs{2\\pi\\boldsymbol \\Sigma}}}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu)\\right\\} \\\\\n&= \\frac{1}{4\\pi^2\\sigma_1^2\\sigma_2^2} \\exp\\brackc{-\\frac{1}{2} \\begin{bmatrix} x_1-\\mu_1 \\\\ x_2-\\mu_2 \\end{bmatrix}^T\n\\begin{bmatrix}\n    \\sigma_1^2 & 0 \\\\\n    0 & \\sigma_2^2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix} x_1-\\mu_1 \\\\ x_2-\\mu_2 \\end{bmatrix}} \\\\\n&= \\frac{1}{4\\pi^2\\sigma_1^2\\sigma_2^2} \\exp\\brackc{-\\frac{1}{2} \\brackb{ \\bracka{\\frac{x_1-\\mu_1}{\\sigma_1}}^2 + \\bracka{\\frac{x_2-\\mu_2}{\\sigma_2}}^2 }}\\\\\n&= \\frac{1}{2\\pi\\sigma_1^2} \\exp\\brackc{-\\frac{1}{2} \\frac{(x_1-\\mu_1)^2}{\\sigma_1^2}} \\frac{1}{2\\pi\\sigma_2^2} \\exp\\brackc{-\\frac{1}{2} \\frac{(x_2-\\mu_2)^2}{\\sigma_2^2}}\\\\\n&= \\mathcal{N}(x_1 | \\mu_1, \\sigma_1^2)\\mathcal{N}(x_2 | \\mu_2, \\sigma_2^2)\n\\end{aligned}\n\\end{equation*}\\]\n\nRemark (Shape of Gaussian): Let’s consider the eigendecomposition of the inverse covariance matrix (which is positive semidefinite and symmetric), as we have\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol \\Sigma^{-1} = \\sum^n_{i=1}\\frac{1}{\\lambda_i}\\boldsymbol u_i\\boldsymbol u_i^T\n\\end{aligned}\n\\end{equation*}\\]\n\nwhere \\(\\lambda_1,\\dots,\\lambda_n\\) and \\(\\boldsymbol u_1,\\dots,\\boldsymbol u_n\\) are the eigenvalues and eigenvectors, repectively of \\(\\boldsymbol \\Sigma\\). Let’s consider the terms inside the exponential to be:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x - \\boldsymbol \\mu)\n= \\sum^n_{i=1}\\frac{(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol u_i\\boldsymbol u_i^T(\\boldsymbol x - \\boldsymbol \\mu)}{\\lambda_i}\n= \\sum^n_{i=1}\\frac{y_i^2}{\\lambda_i}\n\\end{aligned}\n\\end{equation*}\\]\n\nwhere we have \\(y_i = (\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol u_i\\). Consider the vector to be \\(\\boldsymbol y = (y_1,y_2,\\dots,y_n)^T = \\boldsymbol U(\\boldsymbol x - \\boldsymbol \\mu)\\). This gives us the linear transformation over \\(\\boldsymbol x\\), which implies the following shape of Gaussian: - Ellipsoids with the center \\(\\boldsymbol \\mu\\) - Axis is in the direction of eigenvector \\(\\boldsymbol u_i\\) - Scaling of each direction is the eigenvector \\(\\lambda_i\\) associated with \\(\\boldsymbol u_i\\)\nProposition (Normalization of Gaussian): We can show that:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\int \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu)\\right\\} \\dby \\boldsymbol x = \\sqrt{\\abs{2\\pi\\boldsymbol \\Sigma}}\n\\end{aligned}\n\\end{equation*}\\]\n\nProof: Let’s consider the change of variable, in which we will change the variable from \\(x_i\\) to \\(y_i\\) where \\(\\boldsymbol y = \\boldsymbol U(\\boldsymbol x - \\boldsymbol \\mu)\\). To do this we have the find the Jacobian of the transformation, which is:\n\n\\[\\begin{equation*}\n\\begin{aligned}\nJ_{ij} = \\frac{\\partial x_i}{\\partial y_j} = U_{ji}\n\\end{aligned}\n\\end{equation*}\\]\n\nConsider its determinant, as we have: \\(\\abs{\\boldsymbol J}^2 = \\abs{\\boldsymbol U^T}^2 = \\abs{\\boldsymbol U^T}\\abs{\\boldsymbol U} = \\abs{\\boldsymbol U^T\\boldsymbol U} = \\abs{I} = 1\\). Consider the integration as we have (and use the Gaussian integrations):\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\int \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu)\\right\\} \\dby \\boldsymbol x\n&= \\int \\exp\\brackc{\\sum^n_{i=1}-\\frac{y_i^2}{2\\lambda_i}} |\\boldsymbol J| \\dby \\boldsymbol y \\\\\n&= \\int \\prod^n_{i=1}\\exp\\brackc{-\\frac{y_i^2}{2\\lambda_i}} \\dby \\boldsymbol y \\\\\n&= \\prod^n_{i=1}\\int \\exp\\brackc{-\\frac{y_i^2}{2\\lambda_i}} \\dby y_i \\\\\n&= \\prod^n_{i=1}\\sqrt{2\\pi\\lambda_i} = \\sqrt{\\abs{2\\pi\\boldsymbol \\Sigma}}\n\\end{aligned}\n\\end{equation*}\\]\n\nNote that we have shown that the determinant is the product of eigenvalues. Thus the prove is completed.\n\n□"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#useful-backgrounds-1",
    "href": "posts/1-GiHKAL/index.html#useful-backgrounds-1",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Useful Backgrounds",
    "text": "Useful Backgrounds\nProposition (Inverse of Partition Matrix): The block matrix can be inversed as:\n\n\\[\\begin{equation*}\n\\require{color}\n\\newcommand{\\dby}{\\ \\mathrm{d}}\\newcommand{\\argmax}[1]{\\underset{#1}{\\arg\\max \\ }}\\newcommand{\\argmin}[1]{\\underset{#1}{\\arg\\min \\ }}\\newcommand{\\const}{\\text{const.}}\\newcommand{\\bracka}[1]{\\left( #1 \\right)}\\newcommand{\\brackb}[1]{\\left[ #1 \\right]}\\newcommand{\\brackc}[1]{\\left\\{ #1 \\right\\}}\\newcommand{\\brackd}[1]{\\left\\langle #1 \\right\\rangle}\\newcommand{\\correctquote}[1]{``#1''}\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\definecolor{red}{RGB}{244, 67, 54}\n\\definecolor{pink}{RGB}{233, 30, 99}\n\\definecolor{purple}{RGB}{103, 58, 183}\n\\definecolor{yellow}{RGB}{255, 193, 7}\n\\definecolor{grey}{RGB}{96, 125, 139}\n\\definecolor{blue}{RGB}{33, 150, 243}\n\\definecolor{green}{RGB}{0, 150, 136}\n\\begin{bmatrix}\n    \\boldsymbol A & \\boldsymbol B \\\\\n    \\boldsymbol C & \\boldsymbol D \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n    \\boldsymbol M & -\\boldsymbol M\\boldsymbol B\\boldsymbol D^{-1} \\\\\n    -\\boldsymbol D^{-1}\\boldsymbol C\\boldsymbol M & \\boldsymbol D^{-1}+ \\boldsymbol D^{-1}\\boldsymbol C\\boldsymbol M\\boldsymbol B\\boldsymbol D^{-1} \\\\\n\\end{bmatrix}\n\\end{equation*}\\]\n\nwhere we set \\(\\boldsymbol M = (\\boldsymbol A-\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C)^{-1}\\)\nProposition (Inverse Matrix Identity): We can show that\n\n\\[\\begin{equation*}\n    (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1} = \\boldsymbol P\\boldsymbol B^T(\\boldsymbol B\\boldsymbol P\\boldsymbol B^T + \\boldsymbol R)^{-1}\n\\end{equation*}\\]\n\nProof: The can be proven by right multiply the inverse on the right hand-side:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    (\\boldsymbol P^{-1} + &\\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1}(\\boldsymbol B\\boldsymbol P\\boldsymbol B^T + \\boldsymbol R) \\\\\n    &= (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol B\\boldsymbol P\\boldsymbol B^T + (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol R \\\\\n    &= (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol B\\boldsymbol P\\boldsymbol B^T + (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B \\\\\n    &= (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\Big[\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol B\\boldsymbol P + \\boldsymbol P^{-1} \\boldsymbol P\\Big]\\boldsymbol B^T\\\\\n    &= (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\Big[\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol B+ \\boldsymbol P^{-1} \\Big]\\boldsymbol P\\boldsymbol B^T \\\\\n    &= \\boldsymbol P\\boldsymbol B^T \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\n\n□\n\nProposition (Woodbury Identity): We can show that\n\n\\[\\begin{equation*}\n    (\\boldsymbol A + \\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C)^{-1} = \\boldsymbol A^{-1} - \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\n\\end{equation*}\\]\n\nProof: The can be proven by right multiply the inverse on the right hand-side:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\Big[ \\boldsymbol A^{-1} &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1} \\Big](\\boldsymbol A + \\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C) \\\\\n    &= \\boldsymbol A^{-1}(\\boldsymbol A + \\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C) - \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}(\\boldsymbol A + \\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C) \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol A^{-1}\\boldsymbol A + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol A \\\\\n        &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol I + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C \\\\\n        &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol I + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B\\Big[(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol D + (\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B\\Big]\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol I + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B\\Big[(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)\\Big]\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol I + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} = \\boldsymbol I\n\\end{aligned}\n\\end{equation*}\\]\n\n\n□"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#conditional-marginalisation",
    "href": "posts/1-GiHKAL/index.html#conditional-marginalisation",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Conditional & Marginalisation",
    "text": "Conditional & Marginalisation\nRemark (Settings): We consider the setting where we consider the partition of random variables:\n\n\\[\\begin{equation*}\n\\begin{bmatrix}\n    \\boldsymbol x_a \\\\ \\boldsymbol x_b\n\\end{bmatrix}\n\\sim \\mathcal{N}\\bracka{\n\\begin{bmatrix}\n    \\boldsymbol \\mu_a \\\\ \\boldsymbol \\mu_b\n\\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol \\Sigma_{aa} & \\boldsymbol \\Sigma_{ab} \\\\\n    \\boldsymbol \\Sigma_{ba} & \\boldsymbol \\Sigma_{bb} \\\\\n\\end{bmatrix}}\n\\end{equation*}\\]\n\nDue to the symmetric of covariance, we have \\(\\boldsymbol \\Sigma_{ab} = \\boldsymbol \\Sigma_{ba}^T\\). Furthermore, we denote\n\n\\[\\begin{equation*}\n\\boldsymbol \\Sigma^{-1} = \\begin{bmatrix}\n    \\boldsymbol \\Sigma_{aa} & \\boldsymbol \\Sigma_{ab} \\\\\n    \\boldsymbol \\Sigma_{ba} & \\boldsymbol \\Sigma_{bb} \\\\\n\\end{bmatrix}^{-1} =\n\\begin{bmatrix}\n    \\boldsymbol \\Lambda_{aa} & \\boldsymbol \\Lambda_{ab} \\\\\n    \\boldsymbol \\Lambda_{ba} & \\boldsymbol \\Lambda_{bb} \\\\\n\\end{bmatrix} = \\boldsymbol \\Lambda\n\\end{equation*}\\]\n\nwhere \\(\\boldsymbol \\Lambda\\) is called precision matrix. One can consider the inverse of partition matrix to find such a value of \\(\\boldsymbol \\Lambda\\) (will be useful afterward), thus we note that \\(\\boldsymbol \\Sigma_{aa} \\ne \\boldsymbol \\Lambda^{-1}_{aa}\\), and so on.\nRemark (Complete the Square): To find the conditional and marginalision (together with other kinds of Gaussian manipulation), we relies on a method called completing the square. Let’s consider the quadratic form expansion:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    -\\frac{1}{2}&(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu) \\\\\n    &= {\\color{blue}{-\\frac{1}{2}\\boldsymbol x^T\\boldsymbol \\Sigma^{-1}\\boldsymbol x + \\boldsymbol \\mu^T\\boldsymbol \\Sigma^{-1}\\boldsymbol x}} -\\frac{1}{2} \\boldsymbol \\mu^T\\boldsymbol \\Sigma^{-1}\\boldsymbol \\mu \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nNote that the blue term are the term that depends on \\(x\\). This means that to find the Gaussian, we will have to find the first and second order of \\(\\boldsymbol x\\) only. Or, we can consider each individual elements of partitioned random variable:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    -\\frac{1}{2}&(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu) \\\\\n    &= \\begin{aligned}[t]\n        {\\color{green}{-\\frac{1}{2}(\\boldsymbol x_a - \\boldsymbol \\mu_a)^T\\boldsymbol \\Lambda_{aa}(\\boldsymbol x_a - \\boldsymbol \\mu_a)}}{\\color{yellow}{ - \\frac{1}{2}(\\boldsymbol x_a - \\boldsymbol \\mu_a)^T\\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b)}} \\\\\n        {\\color{purple}{-\\frac{1}{2}(\\boldsymbol x_b - \\boldsymbol \\mu_b)^T\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a)}}{\\color{grey}{ - \\frac{1}{2}(\\boldsymbol x_b - \\boldsymbol \\mu_b)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol x_b - \\boldsymbol \\mu_b)}} \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        &{\\color{green} -\\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} -\\frac{1}{2} \\boldsymbol \\mu^T\\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu {\\color{yellow} -\\frac{1}{2}\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b + \\frac{1}{2} \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b} \\\\\n        &{\\color{yellow}+\\frac{1}{2}\\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b} - \\frac{1}{2}\\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b {\\color{purple} -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba} \\boldsymbol \\mu_a}  \\\\\n        &- \\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol \\mu_a {\\color{grey} -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b + \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b} -\\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b\n    \\end{aligned} \\\\\n\\end{aligned}\n\\label{eqn:1}\\tag{1}\n\\end{equation*}\\]\n\nCompleting the square is to match this pattern into our formula in order to get new Gaussian distribution. There are \\(2\\) ways to complete the squre that depends on the scenario: - When we want to find the Gaussian in difference form (but still being Gaussian) i.e conditional - When we want to marginalise some variables out or when we have to find the true for of the distribution without relying on knowing the final form (or when we are not really sure about the final form) i.e marginalisation, posterior\nLet’s just show how it works with examples.\nProposition (Conditional): Consider the following Gaussian\n\n\\[\\begin{equation*}\n\\begin{bmatrix}\n    \\boldsymbol x_a \\\\ \\boldsymbol x_b\n\\end{bmatrix}\n\\sim p(\\boldsymbol x_a, \\boldsymbol x_b) = \\mathcal{N}\\bracka{\n\\begin{bmatrix}\n    \\boldsymbol \\mu_a \\\\ \\boldsymbol \\mu_b\n\\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol \\Sigma_{aa} & \\boldsymbol \\Sigma_{ab} \\\\\n    \\boldsymbol \\Sigma_{ba} & \\boldsymbol \\Sigma_{bb} \\\\\n\\end{bmatrix}}\n\\end{equation*}\\]\n\nWe can show that: \\(p(\\boldsymbol x_a | \\boldsymbol x_b) = \\mathcal{N}(\\boldsymbol x | \\boldsymbol \\mu_{a|b}, \\boldsymbol \\Lambda^{-1}_{aa})\\), where we have - \\(\\boldsymbol \\mu_{a\\lvert b} = \\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{aa}^{-1}\\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b)\\) - Or, we can set \\(\\boldsymbol K = \\boldsymbol \\Sigma_{ab}\\boldsymbol \\Sigma_{bb}^{-1}\\), where we have\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    &\\boldsymbol \\mu_{a|b} = \\boldsymbol \\mu_a + \\boldsymbol K(\\boldsymbol x_b - \\boldsymbol \\mu_b)  \\qquad \\begin{aligned}[t]\n        \\boldsymbol \\Sigma_{a|b} &= \\boldsymbol \\Sigma_{aa} - \\boldsymbol K\\boldsymbol \\Sigma_{bb}\\boldsymbol K^T \\\\\n        &= \\boldsymbol \\Sigma_{aa} - \\boldsymbol \\Sigma_{ab}\\boldsymbol \\Sigma_{bb}^{-1}\\boldsymbol \\Sigma_{ba} \\\\\n    \\end{aligned}\n\\end{aligned}\n\\end{equation*}\\]\n\nPlease note that this follows from block-matrix inverse result (you can try plugging the results in).\nProof: We consider the expansion of the conditional distribution, as we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    -\\frac{1}{2}(\\boldsymbol x_a - \\boldsymbol \\mu_{a|b})^T\\boldsymbol \\Sigma_{a|b}^{-1}(\\boldsymbol x_a - \\boldsymbol \\mu_{a|b}) = {\\color{red}-\\frac{1}{2}\\boldsymbol x_a^T\\boldsymbol \\Sigma_{a|b}^{-1}\\boldsymbol x_a} + {\\color{blue}\\boldsymbol x_a^T\\boldsymbol \\Sigma_{a|b}^{-1}\\boldsymbol \\mu_{a|b}} + \\text{const}\n\\end{aligned}\n\\end{equation*}\\]\n\nLet’s consider the values, which should be equal to equation \\(\\eqref{eqn:1}\\), as we can see that: - The red term: we set \\(\\boldsymbol \\Sigma_{a\\lvert b}^{-1} = \\boldsymbol \\Lambda_{aa}\\) (consider the first green term of the equation \\(\\eqref{eqn:1}\\)) - The blue term., we will have to consider \\((\\dots)^T\\boldsymbol x_a\\). We have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n{\\color{green} \\boldsymbol \\mu_{a}^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} &{\\color{yellow} - \\frac{1}{2}\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b + \\frac{1}{2} \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b} {\\color{purple} - \\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a} \\\\\n&= \\boldsymbol x_a^T\\Big[ \\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{ab}\\boldsymbol x_b + \\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b \\Big] = \\boldsymbol x_a^T\\Big[ \\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b) \\Big] \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nPlease note that \\(\\boldsymbol \\Lambda_{ab}^T = \\boldsymbol \\Lambda_{ba}\\). Now, let’s do “pattern” matching, as we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol x_a^T&\\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_{a|b} = \\boldsymbol x_a^T\\Big[ \\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b) \\Big] \\\\\n\\implies&\\boldsymbol \\mu_{a|b} \\begin{aligned}[t]\n    &= \\boldsymbol \\Lambda_{aa}^{-1}\\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{aa}^{-1}\\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b) \\\\\n    &= \\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{aa}^{-1}\\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b) \\\\\n\\end{aligned}\n\\end{aligned}\n\\end{equation*}\\]\n\nThus the proof is complete.\n\n□\n\nProposition (Marginalisation): Consider the following Gaussian\n\n\\[\\begin{equation*}\n\\begin{bmatrix}\n    \\boldsymbol x_a \\\\ \\boldsymbol x_b\n\\end{bmatrix}\n\\sim p(\\boldsymbol x_a, \\boldsymbol x_b) = \\mathcal{N}\\bracka{\n\\begin{bmatrix}\n    \\boldsymbol \\mu_a \\\\ \\boldsymbol \\mu_b\n\\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol \\Sigma_{aa} & \\boldsymbol \\Sigma_{ab} \\\\\n    \\boldsymbol \\Sigma_{ba} & \\boldsymbol \\Sigma_{bb} \\\\\n\\end{bmatrix}}\n\\end{equation*}\\]\n\nWe can show that: \\(p(\\boldsymbol x_a) = \\mathcal{N}(\\boldsymbol x | \\boldsymbol \\mu_{a}, \\boldsymbol \\Sigma_{aa})\\)\nProof: We collect the terms that contains \\(\\boldsymbol x_b\\) so that we can integrate it out, as we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n{\\color{grey} -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b} &{\\color{grey} +} {\\color{grey}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b}\n{\\color{purple} -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba} \\boldsymbol \\mu_a}\n{\\color{yellow} -\\frac{1}{2}\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b+\\frac{1}{2}\\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b} \\\\\n&=  -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b + \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b -\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba} \\boldsymbol \\mu_a \\\\\n&=  -\\frac{1}{2}\\Big[\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b - 2\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\Lambda_{bb}^{-1}\\underbrace{\\Big(\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b -\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a)\\Big)}_{\\boldsymbol m}\\Big] \\\\\n&=  -\\frac{1}{2}\\Big[\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b - 2\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m + (\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m) \\Big] + \\frac{1}{2}(\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)  \\\\\n&=  -\\frac{1}{2}(\\boldsymbol x_b - \\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol x_b - \\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m) + {\\color{blue}\\frac{1}{2}\\boldsymbol m^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m}  \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nIf we integrate our the quantity, to be:\n\n\\[\\begin{equation*}\n\\int \\exp\\brackc{-\\frac{1}{2}(\\boldsymbol x_b - \\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol x_b - \\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)}\\dby \\boldsymbol x_b\n\\end{equation*}\\]\n\nWe can use the Gaussian integration, like in part 1 and part 2. Now, we consider the other terms that doesn’t depends on \\(\\boldsymbol x_b\\) i.e all terms that depends on \\(\\boldsymbol x_a\\) together with the blue term that been left out.\n\n\\[\\begin{equation*}\n\\begin{aligned}\n{\\color{blue}\\frac{1}{2}\\boldsymbol m^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m} &{\\color{green} -}{\\color{green} \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} {\\color{yellow} + \\frac{1}{2} \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b}{\\color{purple}+ \\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a} \\\\\n&= \\begin{aligned}[t]\n    \\frac{1}{2}\\Big(&\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b -\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a)\\Big)^T\\boldsymbol \\Lambda_{bb}^{-1}\\Big(\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b -\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a)\\Big) \\\\\n    &{\\color{green} - \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} + \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    \\frac{1}{2}\\Big[ \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b &- (\\boldsymbol x_a - \\boldsymbol \\mu_a)^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\mu_b \\\\\n    &- \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a) + (\\boldsymbol x_a - \\boldsymbol \\mu_a)^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a) \\Big] \\\\\n    &{\\color{green} - \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} + \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    \\frac{1}{2}\\Big[ \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b &- \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\mu_b + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\mu_b - \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol \\mu_a \\\\\n    &+ \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a - \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol\\mu_a - \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a \\\\\n    &+ \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol \\mu_a \\Big] {\\color{green} - \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} + \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    \\frac{1}{2}\\Big[-2\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\mu_b &+ \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a - 2\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol\\mu_a\\Big] \\\\\n    &{\\color{green} - \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} + \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\boldsymbol x_a^T\\Big[\\boldsymbol \\Lambda_{aa} - \\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\Big]\\boldsymbol x_a &+ \\boldsymbol x_a^T\\Big[\\boldsymbol \\Lambda_{aa} - \\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\Big]\\boldsymbol\\mu_a + \\text{const}\n\\end{aligned} \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nIf we comparing this form to the normal quadratic expanision of Gaussian, we can set the \\(\\boldsymbol \\mu\\) of marginalised Gaussian is \\(\\boldsymbol \\mu_a\\), while the covariance is \\((\\boldsymbol \\Lambda_{aa} - \\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba})^{-1}\\). If we compare this to the inverse matrix partition, we can see that this is equal to \\(\\boldsymbol \\Sigma_{aa}\\). Thus complete the proof.\n\n□\n\nProposition (Linear Gaussian Model): Consider the distribution to be: \\(p(\\boldsymbol x) = \\mathcal{N}(\\boldsymbol x \\vline {\\color{yellow} \\boldsymbol \\mu} , {\\color{blue} \\boldsymbol \\Lambda^{-1}})\\) and \\(p(\\boldsymbol y \\vline \\boldsymbol x) = \\mathcal{N}(\\boldsymbol y \\vline {\\color{purple}\\boldsymbol A}\\boldsymbol x + {\\color{green} \\boldsymbol b}, {\\color{red} \\boldsymbol L^{-1}})\\). We can show that the following holds:\n\n\\[\\begin{equation*}\np(\\boldsymbol y) = \\mathcal{N}(\\boldsymbol y \\vline {\\color{purple}\\boldsymbol A}{\\color{yellow} \\boldsymbol \\mu} + {\\color{green} \\boldsymbol b}, {\\color{red} \\boldsymbol L^{-1}} + {\\color{purple}\\boldsymbol A}{\\color{blue} \\boldsymbol \\Lambda^{-1}}{\\color{purple}\\boldsymbol A^T}) \\qquad p(\\boldsymbol x \\vline \\boldsymbol y) = \\mathcal{N}\\bracka{ \\boldsymbol x \\vline {\\color{grey} \\boldsymbol \\Sigma}\\brackc{ {\\color{purple}\\boldsymbol A^T} {\\color{red} \\boldsymbol L}(\\boldsymbol y-{\\color{green} \\boldsymbol b}) + {\\color{blue} \\boldsymbol \\Lambda}{\\color{yellow} \\boldsymbol \\mu}}, {\\color{grey} \\boldsymbol \\Sigma} }\n\\end{equation*}\\]\n\nwhere we have \\({\\color{grey} \\boldsymbol \\Sigma} = ({\\color{blue} \\boldsymbol \\Lambda} + {\\color{purple}\\boldsymbol A^T}{\\color{red} \\boldsymbol L}{\\color{purple}\\boldsymbol A})^{-1}\\)\nProof: We will consider the joint random variable \\(\\boldsymbol z = (\\boldsymbol x, \\boldsymbol y)^T\\). Let’s consider the joint distribution and the inside of exponential:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n-\\frac{1}{2}&(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Lambda(\\boldsymbol x - \\boldsymbol \\mu) - \\frac{1}{2}(\\boldsymbol y - \\boldsymbol A\\boldsymbol x - \\boldsymbol b)^T\\boldsymbol L(\\boldsymbol y - \\boldsymbol A\\boldsymbol x - \\boldsymbol b) + \\text{const} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\Big[ \\boldsymbol x^T\\boldsymbol \\Lambda\\boldsymbol x &- 2\\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol x + \\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol \\mu + \\boldsymbol y^T\\boldsymbol L\\boldsymbol y - \\boldsymbol y^T\\boldsymbol L\\boldsymbol A\\boldsymbol x - \\boldsymbol y^T\\boldsymbol L\\boldsymbol b\\\\\n    &-\\boldsymbol x^T\\boldsymbol A^T\\boldsymbol L\\boldsymbol y + \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol A\\boldsymbol x + \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol b - \\boldsymbol b^T\\boldsymbol L\\boldsymbol y +\\boldsymbol b^T\\boldsymbol L\\boldsymbol A\\boldsymbol x + \\boldsymbol b^T\\boldsymbol L\\boldsymbol b \\Big] + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\Big[ \\boldsymbol x^T\\boldsymbol \\Lambda\\boldsymbol x &- 2\\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol x + \\boldsymbol y^T\\boldsymbol L\\boldsymbol y - \\boldsymbol y^T\\boldsymbol L\\boldsymbol A\\boldsymbol x - \\boldsymbol y^T\\boldsymbol L\\boldsymbol b\\\\\n    &-\\boldsymbol x^T\\boldsymbol A^T\\boldsymbol L\\boldsymbol y + \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol A\\boldsymbol x + \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol b - \\boldsymbol b^T\\boldsymbol L\\boldsymbol y +\\boldsymbol b^T\\boldsymbol L\\boldsymbol A\\boldsymbol x  \\Big] + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\Big[ \\boldsymbol x^T\\Big(\\boldsymbol \\Lambda + \\boldsymbol A^T \\boldsymbol L \\boldsymbol A\\Big)\\boldsymbol x &+ \\boldsymbol y^T\\boldsymbol L\\boldsymbol y - \\boldsymbol y^T\\boldsymbol L\\boldsymbol A\\boldsymbol x - \\boldsymbol x^T\\boldsymbol A^T\\boldsymbol L\\boldsymbol y\\\\\n    &+ 2\\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol b  - 2\\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol x - 2\\boldsymbol y^T\\boldsymbol L\\boldsymbol b \\Big] + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\Big[ \\boldsymbol x^T\\Big(\\boldsymbol \\Lambda + \\boldsymbol A^T \\boldsymbol L \\boldsymbol A\\Big)\\boldsymbol x &+ \\boldsymbol y^T\\boldsymbol L\\boldsymbol y - \\boldsymbol y^T\\boldsymbol L\\boldsymbol A\\boldsymbol x - \\boldsymbol x^T\\boldsymbol A^T\\boldsymbol L\\boldsymbol y\\Big]\\\\\n    &- \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol b + \\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol x + \\boldsymbol y^T\\boldsymbol L\\boldsymbol b  + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\n    \\begin{pmatrix}\n        \\boldsymbol x \\\\ \\boldsymbol y\n    \\end{pmatrix}^T\n    \\begin{pmatrix}\n        \\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A & -\\boldsymbol A^T\\boldsymbol L \\\\\n        -\\boldsymbol L\\boldsymbol A & \\boldsymbol L\n    \\end{pmatrix}\n    \\begin{pmatrix}\n        \\boldsymbol x \\\\ \\boldsymbol y\n    \\end{pmatrix}  +\n    \\begin{pmatrix}\n        \\boldsymbol x \\\\ \\boldsymbol y\n    \\end{pmatrix}^T\n    \\begin{pmatrix}\n        \\boldsymbol \\Lambda\\boldsymbol \\mu - \\boldsymbol A^T\\boldsymbol L\\boldsymbol b \\\\\n        \\boldsymbol L\\boldsymbol b\n    \\end{pmatrix} + \\text{const}\n\\end{aligned} \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nWe can use the block-matrix inverse result, to show that:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\begin{pmatrix}\n    \\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A & -\\boldsymbol A^T\\boldsymbol L \\\\\n    -\\boldsymbol L\\boldsymbol A & \\boldsymbol L\n\\end{pmatrix}^{-1} =\n\\begin{pmatrix}\n    \\boldsymbol \\Lambda^{-1} & \\boldsymbol \\Lambda^{-1}\\boldsymbol A^T \\\\\n    \\boldsymbol A\\boldsymbol \\Lambda^{-1} & \\boldsymbol L^{-1} + \\boldsymbol A\\boldsymbol \\Lambda^{-1}\\boldsymbol A^T\n\\end{pmatrix}\n\\end{aligned}\n\\end{equation*}\\]\n\nRecall the Gaussian pattern matching, we can see that the mean is equal to:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\begin{pmatrix}\n    \\boldsymbol \\Lambda^{-1} & \\boldsymbol \\Lambda^{-1}\\boldsymbol A^T \\\\\n    \\boldsymbol A\\boldsymbol \\Lambda^{-1} & \\boldsymbol L^{-1} + \\boldsymbol A\\boldsymbol \\Lambda^{-1}\\boldsymbol A^T\n\\end{pmatrix}\n\\begin{pmatrix}\n    \\boldsymbol \\Lambda\\boldsymbol \\mu - \\boldsymbol A^T\\boldsymbol L\\boldsymbol b \\\\\n    \\boldsymbol L\\boldsymbol b\n\\end{pmatrix} = \\begin{pmatrix}\n    \\boldsymbol \\mu \\\\ \\boldsymbol A\\boldsymbol \\mu + \\boldsymbol b\n\\end{pmatrix}\n\\end{aligned}\n\\end{equation*}\\]\n\nPlease note that with marginalisation result, it is obvious to see how this leads to the final result. Now, for the conditional result, we have the usual result that \\(\\boldsymbol \\Sigma= \\boldsymbol \\Lambda_{xx} = (\\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A)^{-1}\\), for the mean:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\boldsymbol \\mu - &(\\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A)^{-1}(\\boldsymbol A^T\\boldsymbol L)(-\\boldsymbol y + A\\boldsymbol \\mu + \\boldsymbol b) \\\\\n    &= \\boldsymbol \\Sigma\\boldsymbol A^T\\boldsymbol L(\\boldsymbol y - \\boldsymbol b) + \\boldsymbol \\mu - \\boldsymbol \\Sigma\\boldsymbol A^T\\boldsymbol L\\boldsymbol A\\boldsymbol \\mu\n\\end{aligned}\n\\end{equation*}\\]\n\nWe want to show that \\(\\boldsymbol \\mu - \\boldsymbol \\Sigma\\boldsymbol A^T\\boldsymbol L\\boldsymbol A\\boldsymbol \\mu = \\boldsymbol \\Sigma\\boldsymbol \\Lambda\\boldsymbol \\mu\\).\n\n\nLHS: Apply inverse indentity, where we consider the section highlight in pink:\n\\[\n\\begin{aligned}\n    \\boldsymbol \\mu &- \\boldsymbol \\Sigma\\boldsymbol A^T\\boldsymbol L\\boldsymbol A\\boldsymbol \\mu \\\\\n    &= \\boldsymbol \\mu - {\\color{pink}(\\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A)^{-1}\\boldsymbol A^T\\boldsymbol L}\\boldsymbol A\\boldsymbol \\mu \\\\\n    &= \\boldsymbol \\mu - \\boldsymbol \\Lambda^{-1}\\boldsymbol A^T(\\boldsymbol A\\boldsymbol \\Lambda^{-1}\\boldsymbol A^T + \\boldsymbol L^{-1})^{-1}\\boldsymbol A\\boldsymbol \\mu\n\\end{aligned}\n\\]\n\nRHS: We consider the section highlight in blue and use Woodbury Identity:\n\\[\n\\begin{aligned}\n    \\boldsymbol \\Sigma\\boldsymbol \\Lambda\\boldsymbol \\mu &= {\\color{blue}(\\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A)^{-1}}\\boldsymbol \\Lambda\\boldsymbol \\mu \\\\\n    &= \\boldsymbol \\mu - \\boldsymbol \\Lambda^{-1}\\boldsymbol A^T(\\boldsymbol A\\boldsymbol \\Lambda^{-1}\\boldsymbol A^T + \\boldsymbol L^{-1})^{-1}\\boldsymbol A\\boldsymbol \\mu\n\\end{aligned}\n\\]\n\n\nNow we have show that both are equal, thus we conclude the proof.\n\n□"
  },
  {
    "objectID": "posts/2-kernel-test/index.html",
    "href": "posts/2-kernel-test/index.html",
    "title": "Kernel Statistical Test",
    "section": "",
    "text": "In this document, I would like to explain how the MMD and HSIC are derived (roughly). We will start with the brief introduction to RKHS, and then moving on to the statistical testing procedures. This is based on lecture note of the course Reproducing kernel Hilbert spaces in Machine Learning\n\n\nWe recall that the Hilbert space (HS) is a vector space that are equipped with an inner product between vectors and returns a scalar result. Reproducing Kernel HS (RKHS) is the Hilbert spaces that is equipped with the a kernel (that is constructed by the non-unique feature maps). Let’s unpack this, by starting from the definition of kernel\n\n\n\n\n\n\nDefinition\n\n\n\n\nDefinition 1 (Kernel): Given the non-empty set \\(\\mathcal{X}\\), we define a kernel to be \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}\\) such that there is a Hilber space \\(\\mathcal{H}\\) and a function (called feature map) \\(\\phi:\\mathcal{X}\\rightarrow\\mathcal{H}\\) where:\n\\[k(x, y) = \\langle \\phi(x), \\phi(y)\\rangle_\\mathcal{H}\\]\nNoted that vector space \\(\\mathcal{H}\\) doesn’t need to be finite dimension (and so it can have infinite dimension i.e a function like object).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\n\nRemark 1. (Not Unique Feature Map): The good example of feature maps that doesn’t have to be unique is when:\n\\[\n\\phi_1(x) = x \\qquad \\phi_2(x) = \\begin{bmatrix}x/\\sqrt{2} \\\\ x/\\sqrt{2}\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\n\nRemark 2. (Constructing a New Kernel from An Old One): Given the fact that \\(k_1\\) and \\(k_2\\) are kernels, then we can show that, for any \\(x, y\\in\\mathcal{X}\\):\n\n\\(k_1(x, y)+k_2(x, y)\\)\n\\(k_1(x, y)*k_2(x, y)\\)\nFor \\(a\\in\\mathbb{R}\\), such that \\(ak_1(x,y)\\)\nFor any function \\(f:\\mathcal{X}\\rightarrow\\mathcal{X}'\\) (can be neural network or any kind of functions) and kernel \\(k':\\mathcal{X}'\\times\\mathcal{X}'\\rightarrow\\mathbb{R}\\), such that \\(k'(\\phi(x), \\phi(y))\\)\n\nare all kernel. With this would means that \\(k(x, x') = (c + \\langle x, x'\\rangle)^m\\) is also a kernel, or any function that admits Taylor series \\(f\\) (with convergences properties etc.), then \\(f(\\langle x, x'\\rangle)\\) is also a kernel.\n\n\n\nNow, we are ready to define the RKHS, in which it is a special Hilbert space with a special kind of kernel:\n\n\n\n\n\n\nDefinition\n\n\n\n\nDefinition 2 (Reproducing Kernel Hilber Space): Given a Hilbert space \\(\\mathcal{H}\\) of \\(\\mathbb{R}\\) valued functions on non-empty set \\(\\mathcal{X}\\), the kernel \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}\\) is called reproducing and \\(\\mathcal{H}\\) is called RKHS if:\n\nFor all \\(x\\in\\mathcal{X}\\), \\(k(\\cdot, x) \\in\\mathcal{H}\\), then \\(k(\\cdot, x)\\in\\mathcal{H}\\)\nFor all \\(x\\in\\mathcal{X}\\), \\(\\langle{f(\\cdot), k(\\cdot,x)\\rangle}_\\mathcal{H} = f(x)\\)\n\n\n\n\nGiven the defintion, one can see that:\n\\[\n\\langle k(\\cdot, x), k(\\cdot, y)\\rangle_\\mathcal{H} = k(x, y)\n\\]\nwhich means that \\(k(\\cdot,x)\\) for any \\(x\\in\\mathcal{X}\\) can be seen as the feature map (recall that it doesn’t have to be unique), we will call this a canonical feature map. We also have the follows result that illustrate why RKHS is preferable compared to the normal HS of functions.\n\n\n\n\n\n\nRemark\n\n\n\n\nRemark 3. Advanced Topics: Intuitively, we just say that the functions in RKHS acts “smoothly” and “predictably”, in the sense that:\n\nIf the distance between functions \\(\\|f-g\\|_\\mathcal{H}\\) is close to each other then its pointwise evaluation \\(|f(x)-g(x)|\\) for any \\(x\\) would also be close to each other.\nThis can be shown by the fact that the HS \\(\\mathcal{H}\\) has reproducing kernel iff the evaluation operator (defined as \\(\\delta_x : \\mathcal{H}\\rightarrow\\mathbb{R}\\) where \\(\\delta_x(f)=f(x)\\)) is bounded i.e \\(|\\delta_x(f)|\\le \\lambda_x\\|f\\|_\\mathcal{H}\\) for positive constant \\(\\lambda_x\\in\\mathbb{R}\\) (the proof uses the Riesz representation theorem)\n\n\n\n\nFurthermore, if the kernel satisfies the special property, then there is going to be an RHKS that is equipped with the given kernel as:\n\n\n\n\n\n\nTheorem\n\n\n\n\nTheorem 1 (Moore-Aronszajn): A symmetric function \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow \\mathbb{R}\\) is positive definite if: for all \\(a_1,a_2,\\dots,a_n\\in \\mathbb{R}\\) and for all \\(x_1,x_2,\\dots, x_n\\in\\mathcal{X}\\):\n\\[\\sum^n_{i=1}\\sum^n_{j=1}a_ia_jk(x_i, x_j)\\ge0\\]\nIf the kernel is positive definite, then there is a unique RKHS with the reproducing kernel \\(k\\).\n\n\n\n\n\n\nGiven the sample \\((x_i)^m_{i=1}\\sim p\\) and \\((y_i)^m_{i=1}\\sim q\\). Given any feature extraction function \\(\\phi\\), one can find related kernel \\(k(\\cdot,\\cdot)\\) to be: \\(k(a, b)=\\langle \\phi(a), \\phi(b)\\rangle\\). Therefore, the distance between their mean in a feature space of the kernel \\(k(\\cdot,\\cdot)\\) can be computed as:\n\\[\n\\begin{aligned}\n\\Bigg\\| \\frac{1}{m}&\\sum^m_{i=1}\\phi(x_i) - \\frac{1}{n}\\sum^n_{i=1}\\phi(y_i) \\Bigg\\|^2 \\\\\n&= \\frac{1}{m^2}\\sum^m_{i=1}\\sum^m_{j=1}k(x_i,x_j) + \\frac{1}{n^2}\\sum^n_{i=1}\\sum^n_{j=1}k(y_i, y_j) - \\frac{2}{mn}\\sum^m_{i=1}\\sum^n_{j=1}k(x_i, y_i)\n\\end{aligned}\n\\]\nWe can observe 2 things here: 1. If we set the feature extraction function to be \\(\\phi(a)=[a \\ a^2]\\), then we are able to compare both means and variance. 2. One can set the feature extraction function to be arbitrary, as long as one can find the appropriate corresponding kernel (that should be easier to compute than just an inner product of each other). For instance, with RBF, one can have feature extraction function with infinite features! (via Taylor series).\nTherefore, intuitively, we can perform a more power/non-linear relationship between samples. Let’s now move to the actual formulation of the statistical testing.\n\n\n\nIn this section, we are going to given the description of 2 main statistical testing technique that relies on the kernel method: MMD and HSIC (together with its variations). Let’s start with some operators that will be useful for both.\n\n\nGiven the example above in the interlude, we can generalizes the mean of the features map given an element \\(x\\sim P\\), as follows.\n\nDefinition (Mean Embedding): Given positive definite kernel \\(k(x,x')\\) with probability distribution \\(P\\) and \\(Q\\), we define \\(\\mu_P\\) and \\(\\mu_Q\\) such that: \\[\\langle{\\mu_P, \\mu_Q\\rangle} = \\mathbb{E}_{P, Q}[k(x, y)]\\] where \\(x\\sim P\\) and \\(y \\sim Q\\). We can consider the expectation in an RKHS as \\(\\mathbb{E}_P[f(x)] = \\langle{f, \\mu_P}\\rangle_\\mathcal{H}\\) for any function \\(f\\in\\mathcal{H}\\), the function in the corresponding RKHS\n\nWith this, one can see that the empirical mean embedding can be given in the form of:\n\\[\n\\hat{\\mu}_P = \\frac{1}{m}\\sum^m_{i=1}\\phi(x_i) \\qquad \\text{ where } \\qquad x_i\\sim P\n\\]\nIn which, one can show that this element exists.\n\nTheorem: The element \\(\\mu_P\\in\\mathcal{F}\\) defined as \\[\\mathbb{E}_P[f(x)] = \\langle{f, \\mu_P}\\rangle_\\mathcal{H}\\] if the kernel \\(k\\) of RKHS has the property that \\(\\mathbb{E}_P[\\sqrt{k(x, x)}]&lt;\n\\infty\\)\n\nProof Sketch: We can use the Riesz representation theorem by showing that the operator \\(T_Pf=\\mathbb{E}_P[f(x)]\\) is boudned, and thus there is \\(\\mu_P\\) such that \\(T_Pf=\\langle f,\\mu_P\\rangle\\)).\n\n\n\nLet’s formally define the notion of MMD, which tries to answer the question, does the samples \\(\\{x_i\\}^n_{i=1}\\) and \\(\\{y_i\\}^n_{i=1}\\) comes from the same distribution or not ?\n\nDefinition (MMD): Now, we define the quantity of MMD being the distance between \\(2\\) probability distributions \\(P\\) and \\(Q\\) as (together with its, more computable form) \\[ \\begin{aligned} \\operatorname{MMD}^2&(P, Q) = \\|\\mu_P-\\mu_Q\\|^2_\\mathcal{F} \\\\ &= \\mathbb{E}_P[k(x, x')] + \\mathbb{E}_Q[k(y, y')] - 2\\mathbb{E}_{P, Q}[k(x, y)] \\end{aligned} \\] whereby, we have the following unbiased estimate of its quantity: \\[ \\widehat{\\operatorname{MMD}}^2(P, Q) = \\frac{1}{n(n-1)}\\sum_{i\\ne j}k(x_i, x_j) + \\frac{1}{n(n-1)}\\sum_{i\\ne j}k(y_i, y_j) - \\frac{2}{n^2}\\sum_{i,j}k(x_i, y_j) \\] for \\(x_i\\sim P\\) and \\(y_i\\sim Q\\)\n\nYou may wonder, why does MMD is called maximum mean discrepancy ? One can show MMD can be written in an alternative form of:\n\nTheorem: We can show that the MMD can be written in an alternative form of: \\[\\operatorname{MMD}(P, Q) = \\sup_{\\|f\\|\\le1}\\big(\\mathbb{E}_P[f(x)] - \\mathbb{E}_Q[f(x)]\\big)\\]\n\nThis can be interpreted as, given “smooth” function within a ball (therefore not being too extream), we find such a function that maximally distingush the sample of \\(P\\) and \\(Q\\), and the maximum disagreement is the MMD value.\nNow, back to the statistical testing, we can show that the value of MMD will have the following asympototics distribution of:\n\nTheorem: We have the following distribution of the empirical MMD statistics as follows: - When \\(P\\ne Q\\), we have: \\[\\frac{\\widehat{\\operatorname{MMD}}^2 - \\operatorname{MMD}(P, Q)^2}{\\sqrt{V_n(P, Q)}} \\xrightarrow{D} \\mathcal{N}(0, 1)\\]where the variance \\(V_n(P, Q) = \\mathcal{O}(n^{-1})\\) but depends on the chosen kernel. - When \\(P=Q\\), we have: \\[n\\widehat{\\operatorname{MMD}}^2 \\sim \\sum^\\infty_{l=1} \\lambda_l[z^2_l - 2] \\qquad \\text{ where } \\qquad \\lambda_i\\phi_i(x) = \\int_\\mathcal{X}\\widetilde{k}(x,\\widetilde{x})\\phi_i(x)\\text{ d}P(x)\\]where \\(\\widetilde{k}\\) is a centered kernel and \\(z_l\\sim\\mathcal{N}(0, 2)\\)\n\nHowever, to compute such a distribution with null-hypothesis \\(P=Q\\) in closed form is hard, therefore:\n\nWe have to rely on using a boostrap method which is done by permuting the set \\(X\\) and \\(Y\\) before testing (i.e mixing them up)\nThis would gives us the estimate of the MMD statistics when \\(P=Q\\), which can them be used to compute the threshold for statistical test.\n\nNow, to find a best kernel, we have that:\n\nRemark (Finding a best kernek): Given the distribution when \\(P=Q\\), one can see that the power of the test is given to be: \\[\\text{Pr}_1\\left({n\\widehat{\\operatorname{MMD}} &gt; \\hat{c}_\\alpha }\\right) \\rightarrow  \\Phi\\left({\\frac{\\operatorname{MMD}^2(P, Q)}{\\sqrt{V_n(P, Q)}} - \\frac{c_\\alpha}{n\\sqrt{V_n(P, Q)}} }\\right)\\] To find the best kernel, we can find the kernel that maximize the test power. We would like to note the following: \\[\\frac{\\operatorname{MMD}^2(P, Q)}{\\sqrt{V_n(P, Q)}} = \\mathcal{O}(\\sqrt{n}) \\qquad \\frac{c_\\alpha}{n\\sqrt{V_n(P, Q)}} =\\mathcal{O}(n^{-1/2})\\] therefore, we can ignore the second term, and we can maximize the first term only, by setting this to be the objective of the neural network (that perform the feature extraction of the kernel). Note that we can derive the estimator for \\(V_n\\) too.\n\nFor example, one can use the following kernel from\n\\[\nk_\\theta(x, y) = \\big[(1-\\varepsilon)\\kappa(\\Phi_\\theta(x), \\Phi_\\theta(y))+\\varepsilon\\big]q(x, y)\n\\]\nwhere \\(\\Phi_\\theta\\) is a neural network and \\(\\kappa\\) and \\(q\\) are Gaussian kernel, which is able to distinguish between CIFAR-10 vs CIFAR-10 (image dataset).\nNow come the more important question, can we use any kernel to give us the appropriate MMD test ? The answer is obviously no, but what kind of kernel would be approriate ? Starting with a defintion of a good kernel (or we will call it characteristic):\n\nDefinition (Charateristic kernel): A RKHS (with corresponding kernel) is called characteristic if \\(\\operatorname{MMD}(P, Q; \\mathcal{F}) = 0\\) iff \\(P = Q\\)\n\nThat is when \\(P\\) and \\(Q\\) are the same, the value of MMD should be zero. What would be an appropriate kernel ? In this case, we would like to assume that kernel that we are working on is Translation Invariance i.e\n\nDefinition (Translation Invariance): The kernel \\(k\\) is called Translation Invariance if there is a function \\(f\\) such that: \\[k(x,y)=f(x-y)\\] for any \\(x\\) and \\(y\\)\n\nThen, one can have a fourier representation/coefficient of the kernel to be (assume we are within the domain of \\([-\\pi,\\pi]\\)) the multiple within the fourier series expansion:\n\\[\nk(x, y) = \\sum^\\infty_{l=-\\infty} \\hat{k}_l \\exp(il(x-y)) = \\sum^\\infty_{l=-\\infty}\\underbrace{\\left[{\\sqrt{\\hat{k}_l} \\exp(ilx) }\\right]}_{\\phi_l(x)}\\underbrace{\\left[{\\sqrt{\\hat{k}_l}\\exp(-ily)}\\right]}_{\\overline{\\phi_l(y)}}\n\\]\n\\(\\hat{k}_l\\) is called the fourier coefficient of the kernel. For the probability distribution, one can also have a similar way to find the fourier coefficient of them. We have the following result.\n\nTheorem: The value of MMD can be written as: \\[\\operatorname{MMD}^2(P, Q;\\mathcal{F}) = \\sum^\\infty_{l=-\\infty} |\\phi_{P,l} - \\phi_{Q, l}|^2\\hat{k}_l\\] for \\(\\hat{k}_l\\) being the fourier coefficient of the kernel, \\(\\phi_{P,l}\\) and \\(\\phi_{Q,l}\\) are fourier coefficient of the probability distributions \\(P\\) and \\(Q\\), respectively.\n\nTherefore, the kernel is characterisic iff none of the \\(\\hat{k}_l\\) is equal to zero.\nOn the other hand, instead of considering within specific range \\([\\pi,-\\pi]\\), one can also define the RKHS to be universal, which is when:\n\nDefinition (Universal RKHS): Given RKHS, it is universal if when: - \\(k(x, x')\\) is continuous - \\(\\mathcal{X}\\) is compact. - \\(\\mathcal{F}\\) is dense in \\(C(\\mathcal{X})\\) wrt. \\(L_\\infty\\) i.e for \\(\\varepsilon&gt;0\\) and \\(f\\in C(\\mathcal{X})\\), there is \\(g\\in\\mathcal{F}\\) such that: \\[\\|f-g\\|_\\infty\\le\\varepsilon\\]\n\nin which we can show that:\n\nTheorem: If \\(\\mathcal{F}\\) is universal then \\(\\operatorname{MMD}(P, Q;\\mathcal{F}) = 0\\) iff \\(P = Q\\)\n\n\n\n\n\nNow, we are interested in given a pair of variables \\(\\{(x_i, y_i)\\}^n_{i=1}\\sim P_{XY}\\) are they dependent of each other ? - Usually one can use the MMD to find the differences whether this sample is sampled from the \\(P_XP_Y\\) (i.e product of marginal distribution). However, we don’t have an access to this. - Another question is: which kind of kernel would we be use ? is it a product kernel ? or different kind of kernels\n\n\nWe start off by defining the tensor product between elements in the Hilber space.\n\nDefinition (Tensor Product): Given element \\(a,b,c\\in\\mathcal{H}\\) of the Hilbert space, the tensor product between \\(a\\) and \\(b\\) is denoted as \\(a\\otimes b\\) such that: \\[(a\\otimes b)c = \\langle b,c\\rangle_\\mathcal{H}a\\] Note that this is analogous to when \\(a,b\\) and \\(c\\) are vector, then \\((ab^\\top)c=b^\\top ca\\)\n\nNow, we would like to extends the notion of the inner product (and norm) to the linear transformation between Hilbert space. This would gives us Hilbert-Schmidt Operators i.e\n\nDefinition (Hilbert-Schmidt Operators): Given a separable (countable orthonormal basis Hilbert spaces \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\) with orthonormal basis \\((f_i)_{i\\in I}\\) and \\((g_j)_{j\\in I}\\), respectively and 2 linear transformation between them: \\(L:\\mathcal{G}\\rightarrow\\mathcal{F}\\) and \\(M:\\mathcal{G}\\rightarrow\\mathcal{F}\\): \\[\\langle{L, M}\\rangle_{\\operatorname{HS}} = \\sum_{j\\in J}\\langle{Lg_j, Mg_j}\\rangle_\\mathcal{F}\\]\n\nNow, we can define the covariance operator (in similar manners to the mean embedding) as:\n\nDefinition (Covariance Operator): The covariance operators \\(C_{xy} : \\mathcal{G} \\rightarrow \\mathcal{F}\\) is given by: \\[\\langle{f, C_{xy}g}\\rangle_\\mathcal{F} = \\mathbb{E}_{xy}[f(x)g(y)]\\] which we can show to exists if the kernel associated \\(\\mathcal{G}\\) and \\(\\mathcal{F}\\): \\(k_1\\) and \\(k_2\\), respectively, are such that \\(k_1(x,x) &lt; \\infty\\) and \\(k_2(y,y)&lt;\\infty\\)\n\nThe existances can be proven by observe that, for any linear operator \\(A:\\mathcal{G} \\rightarrow \\mathcal{F}\\), we have:\n\\[\n\\langle{C_{xy}, A}\\rangle_{\\operatorname{HS}} = \\mathbb{E}_{xy}\\big[\\langle\\psi(x)\\otimes\\phi(y), A\\rangle_{\\operatorname{HS}}\\big]\n\\]\nand so we can use Riesz representation thoerem to proof the existence. Then, we are ready to define the HSIC\n\n\n\n\nDefinition (Hilbert-Schmidt Indepdent Criterion): The HSIC can be seen as the norm of the centered covariance operator i.e: \\[\\operatorname{HSIC}(P_{XY};\\mathcal{F}, \\mathcal{G}) = \\|{C_{xy} - \\mu_x\\otimes\\mu_y}\\|_{\\operatorname{HS}} = \\|{\\widetilde{C}_{xy}}\\|_{\\operatorname{HS}}\\]\n\nIn relation to MMD, we can show that"
  },
  {
    "objectID": "posts/2-kernel-test/index.html#quick-introduction-to-rkhs",
    "href": "posts/2-kernel-test/index.html#quick-introduction-to-rkhs",
    "title": "Kernel Statistical Test",
    "section": "",
    "text": "We recall that the Hilbert space (HS) is a vector space that are equipped with an inner product between vectors and returns a scalar result. Reproducing Kernel HS (RKHS) is the Hilbert spaces that is equipped with the a kernel (that is constructed by the non-unique feature maps). Let’s unpack this, by starting from the definition of kernel\n\n\n\n\n\n\nDefinition\n\n\n\n\nDefinition 1 (Kernel): Given the non-empty set \\(\\mathcal{X}\\), we define a kernel to be \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}\\) such that there is a Hilber space \\(\\mathcal{H}\\) and a function (called feature map) \\(\\phi:\\mathcal{X}\\rightarrow\\mathcal{H}\\) where:\n\\[k(x, y) = \\langle \\phi(x), \\phi(y)\\rangle_\\mathcal{H}\\]\nNoted that vector space \\(\\mathcal{H}\\) doesn’t need to be finite dimension (and so it can have infinite dimension i.e a function like object).\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\n\nRemark 1. (Not Unique Feature Map): The good example of feature maps that doesn’t have to be unique is when:\n\\[\n\\phi_1(x) = x \\qquad \\phi_2(x) = \\begin{bmatrix}x/\\sqrt{2} \\\\ x/\\sqrt{2}\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nRemark\n\n\n\n\nRemark 2. (Constructing a New Kernel from An Old One): Given the fact that \\(k_1\\) and \\(k_2\\) are kernels, then we can show that, for any \\(x, y\\in\\mathcal{X}\\):\n\n\\(k_1(x, y)+k_2(x, y)\\)\n\\(k_1(x, y)*k_2(x, y)\\)\nFor \\(a\\in\\mathbb{R}\\), such that \\(ak_1(x,y)\\)\nFor any function \\(f:\\mathcal{X}\\rightarrow\\mathcal{X}'\\) (can be neural network or any kind of functions) and kernel \\(k':\\mathcal{X}'\\times\\mathcal{X}'\\rightarrow\\mathbb{R}\\), such that \\(k'(\\phi(x), \\phi(y))\\)\n\nare all kernel. With this would means that \\(k(x, x') = (c + \\langle x, x'\\rangle)^m\\) is also a kernel, or any function that admits Taylor series \\(f\\) (with convergences properties etc.), then \\(f(\\langle x, x'\\rangle)\\) is also a kernel.\n\n\n\nNow, we are ready to define the RKHS, in which it is a special Hilbert space with a special kind of kernel:\n\n\n\n\n\n\nDefinition\n\n\n\n\nDefinition 2 (Reproducing Kernel Hilber Space): Given a Hilbert space \\(\\mathcal{H}\\) of \\(\\mathbb{R}\\) valued functions on non-empty set \\(\\mathcal{X}\\), the kernel \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}\\) is called reproducing and \\(\\mathcal{H}\\) is called RKHS if:\n\nFor all \\(x\\in\\mathcal{X}\\), \\(k(\\cdot, x) \\in\\mathcal{H}\\), then \\(k(\\cdot, x)\\in\\mathcal{H}\\)\nFor all \\(x\\in\\mathcal{X}\\), \\(\\langle{f(\\cdot), k(\\cdot,x)\\rangle}_\\mathcal{H} = f(x)\\)\n\n\n\n\nGiven the defintion, one can see that:\n\\[\n\\langle k(\\cdot, x), k(\\cdot, y)\\rangle_\\mathcal{H} = k(x, y)\n\\]\nwhich means that \\(k(\\cdot,x)\\) for any \\(x\\in\\mathcal{X}\\) can be seen as the feature map (recall that it doesn’t have to be unique), we will call this a canonical feature map. We also have the follows result that illustrate why RKHS is preferable compared to the normal HS of functions.\n\n\n\n\n\n\nRemark\n\n\n\n\nRemark 3. Advanced Topics: Intuitively, we just say that the functions in RKHS acts “smoothly” and “predictably”, in the sense that:\n\nIf the distance between functions \\(\\|f-g\\|_\\mathcal{H}\\) is close to each other then its pointwise evaluation \\(|f(x)-g(x)|\\) for any \\(x\\) would also be close to each other.\nThis can be shown by the fact that the HS \\(\\mathcal{H}\\) has reproducing kernel iff the evaluation operator (defined as \\(\\delta_x : \\mathcal{H}\\rightarrow\\mathbb{R}\\) where \\(\\delta_x(f)=f(x)\\)) is bounded i.e \\(|\\delta_x(f)|\\le \\lambda_x\\|f\\|_\\mathcal{H}\\) for positive constant \\(\\lambda_x\\in\\mathbb{R}\\) (the proof uses the Riesz representation theorem)\n\n\n\n\nFurthermore, if the kernel satisfies the special property, then there is going to be an RHKS that is equipped with the given kernel as:\n\n\n\n\n\n\nTheorem\n\n\n\n\nTheorem 1 (Moore-Aronszajn): A symmetric function \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow \\mathbb{R}\\) is positive definite if: for all \\(a_1,a_2,\\dots,a_n\\in \\mathbb{R}\\) and for all \\(x_1,x_2,\\dots, x_n\\in\\mathcal{X}\\):\n\\[\\sum^n_{i=1}\\sum^n_{j=1}a_ia_jk(x_i, x_j)\\ge0\\]\nIf the kernel is positive definite, then there is a unique RKHS with the reproducing kernel \\(k\\)."
  },
  {
    "objectID": "posts/2-kernel-test/index.html#interlude-why-kernel-in-statistical-testing",
    "href": "posts/2-kernel-test/index.html#interlude-why-kernel-in-statistical-testing",
    "title": "Kernel Statistical Test",
    "section": "",
    "text": "Given the sample \\((x_i)^m_{i=1}\\sim p\\) and \\((y_i)^m_{i=1}\\sim q\\). Given any feature extraction function \\(\\phi\\), one can find related kernel \\(k(\\cdot,\\cdot)\\) to be: \\(k(a, b)=\\langle \\phi(a), \\phi(b)\\rangle\\). Therefore, the distance between their mean in a feature space of the kernel \\(k(\\cdot,\\cdot)\\) can be computed as:\n\\[\n\\begin{aligned}\n\\Bigg\\| \\frac{1}{m}&\\sum^m_{i=1}\\phi(x_i) - \\frac{1}{n}\\sum^n_{i=1}\\phi(y_i) \\Bigg\\|^2 \\\\\n&= \\frac{1}{m^2}\\sum^m_{i=1}\\sum^m_{j=1}k(x_i,x_j) + \\frac{1}{n^2}\\sum^n_{i=1}\\sum^n_{j=1}k(y_i, y_j) - \\frac{2}{mn}\\sum^m_{i=1}\\sum^n_{j=1}k(x_i, y_i)\n\\end{aligned}\n\\]\nWe can observe 2 things here: 1. If we set the feature extraction function to be \\(\\phi(a)=[a \\ a^2]\\), then we are able to compare both means and variance. 2. One can set the feature extraction function to be arbitrary, as long as one can find the appropriate corresponding kernel (that should be easier to compute than just an inner product of each other). For instance, with RBF, one can have feature extraction function with infinite features! (via Taylor series).\nTherefore, intuitively, we can perform a more power/non-linear relationship between samples. Let’s now move to the actual formulation of the statistical testing."
  },
  {
    "objectID": "posts/2-kernel-test/index.html#statistical-testing-with-kernel-method",
    "href": "posts/2-kernel-test/index.html#statistical-testing-with-kernel-method",
    "title": "Kernel Statistical Test",
    "section": "",
    "text": "In this section, we are going to given the description of 2 main statistical testing technique that relies on the kernel method: MMD and HSIC (together with its variations). Let’s start with some operators that will be useful for both.\n\n\nGiven the example above in the interlude, we can generalizes the mean of the features map given an element \\(x\\sim P\\), as follows.\n\nDefinition (Mean Embedding): Given positive definite kernel \\(k(x,x')\\) with probability distribution \\(P\\) and \\(Q\\), we define \\(\\mu_P\\) and \\(\\mu_Q\\) such that: \\[\\langle{\\mu_P, \\mu_Q\\rangle} = \\mathbb{E}_{P, Q}[k(x, y)]\\] where \\(x\\sim P\\) and \\(y \\sim Q\\). We can consider the expectation in an RKHS as \\(\\mathbb{E}_P[f(x)] = \\langle{f, \\mu_P}\\rangle_\\mathcal{H}\\) for any function \\(f\\in\\mathcal{H}\\), the function in the corresponding RKHS\n\nWith this, one can see that the empirical mean embedding can be given in the form of:\n\\[\n\\hat{\\mu}_P = \\frac{1}{m}\\sum^m_{i=1}\\phi(x_i) \\qquad \\text{ where } \\qquad x_i\\sim P\n\\]\nIn which, one can show that this element exists.\n\nTheorem: The element \\(\\mu_P\\in\\mathcal{F}\\) defined as \\[\\mathbb{E}_P[f(x)] = \\langle{f, \\mu_P}\\rangle_\\mathcal{H}\\] if the kernel \\(k\\) of RKHS has the property that \\(\\mathbb{E}_P[\\sqrt{k(x, x)}]&lt;\n\\infty\\)\n\nProof Sketch: We can use the Riesz representation theorem by showing that the operator \\(T_Pf=\\mathbb{E}_P[f(x)]\\) is boudned, and thus there is \\(\\mu_P\\) such that \\(T_Pf=\\langle f,\\mu_P\\rangle\\)).\n\n\n\nLet’s formally define the notion of MMD, which tries to answer the question, does the samples \\(\\{x_i\\}^n_{i=1}\\) and \\(\\{y_i\\}^n_{i=1}\\) comes from the same distribution or not ?\n\nDefinition (MMD): Now, we define the quantity of MMD being the distance between \\(2\\) probability distributions \\(P\\) and \\(Q\\) as (together with its, more computable form) \\[ \\begin{aligned} \\operatorname{MMD}^2&(P, Q) = \\|\\mu_P-\\mu_Q\\|^2_\\mathcal{F} \\\\ &= \\mathbb{E}_P[k(x, x')] + \\mathbb{E}_Q[k(y, y')] - 2\\mathbb{E}_{P, Q}[k(x, y)] \\end{aligned} \\] whereby, we have the following unbiased estimate of its quantity: \\[ \\widehat{\\operatorname{MMD}}^2(P, Q) = \\frac{1}{n(n-1)}\\sum_{i\\ne j}k(x_i, x_j) + \\frac{1}{n(n-1)}\\sum_{i\\ne j}k(y_i, y_j) - \\frac{2}{n^2}\\sum_{i,j}k(x_i, y_j) \\] for \\(x_i\\sim P\\) and \\(y_i\\sim Q\\)\n\nYou may wonder, why does MMD is called maximum mean discrepancy ? One can show MMD can be written in an alternative form of:\n\nTheorem: We can show that the MMD can be written in an alternative form of: \\[\\operatorname{MMD}(P, Q) = \\sup_{\\|f\\|\\le1}\\big(\\mathbb{E}_P[f(x)] - \\mathbb{E}_Q[f(x)]\\big)\\]\n\nThis can be interpreted as, given “smooth” function within a ball (therefore not being too extream), we find such a function that maximally distingush the sample of \\(P\\) and \\(Q\\), and the maximum disagreement is the MMD value.\nNow, back to the statistical testing, we can show that the value of MMD will have the following asympototics distribution of:\n\nTheorem: We have the following distribution of the empirical MMD statistics as follows: - When \\(P\\ne Q\\), we have: \\[\\frac{\\widehat{\\operatorname{MMD}}^2 - \\operatorname{MMD}(P, Q)^2}{\\sqrt{V_n(P, Q)}} \\xrightarrow{D} \\mathcal{N}(0, 1)\\]where the variance \\(V_n(P, Q) = \\mathcal{O}(n^{-1})\\) but depends on the chosen kernel. - When \\(P=Q\\), we have: \\[n\\widehat{\\operatorname{MMD}}^2 \\sim \\sum^\\infty_{l=1} \\lambda_l[z^2_l - 2] \\qquad \\text{ where } \\qquad \\lambda_i\\phi_i(x) = \\int_\\mathcal{X}\\widetilde{k}(x,\\widetilde{x})\\phi_i(x)\\text{ d}P(x)\\]where \\(\\widetilde{k}\\) is a centered kernel and \\(z_l\\sim\\mathcal{N}(0, 2)\\)\n\nHowever, to compute such a distribution with null-hypothesis \\(P=Q\\) in closed form is hard, therefore:\n\nWe have to rely on using a boostrap method which is done by permuting the set \\(X\\) and \\(Y\\) before testing (i.e mixing them up)\nThis would gives us the estimate of the MMD statistics when \\(P=Q\\), which can them be used to compute the threshold for statistical test.\n\nNow, to find a best kernel, we have that:\n\nRemark (Finding a best kernek): Given the distribution when \\(P=Q\\), one can see that the power of the test is given to be: \\[\\text{Pr}_1\\left({n\\widehat{\\operatorname{MMD}} &gt; \\hat{c}_\\alpha }\\right) \\rightarrow  \\Phi\\left({\\frac{\\operatorname{MMD}^2(P, Q)}{\\sqrt{V_n(P, Q)}} - \\frac{c_\\alpha}{n\\sqrt{V_n(P, Q)}} }\\right)\\] To find the best kernel, we can find the kernel that maximize the test power. We would like to note the following: \\[\\frac{\\operatorname{MMD}^2(P, Q)}{\\sqrt{V_n(P, Q)}} = \\mathcal{O}(\\sqrt{n}) \\qquad \\frac{c_\\alpha}{n\\sqrt{V_n(P, Q)}} =\\mathcal{O}(n^{-1/2})\\] therefore, we can ignore the second term, and we can maximize the first term only, by setting this to be the objective of the neural network (that perform the feature extraction of the kernel). Note that we can derive the estimator for \\(V_n\\) too.\n\nFor example, one can use the following kernel from\n\\[\nk_\\theta(x, y) = \\big[(1-\\varepsilon)\\kappa(\\Phi_\\theta(x), \\Phi_\\theta(y))+\\varepsilon\\big]q(x, y)\n\\]\nwhere \\(\\Phi_\\theta\\) is a neural network and \\(\\kappa\\) and \\(q\\) are Gaussian kernel, which is able to distinguish between CIFAR-10 vs CIFAR-10 (image dataset).\nNow come the more important question, can we use any kernel to give us the appropriate MMD test ? The answer is obviously no, but what kind of kernel would be approriate ? Starting with a defintion of a good kernel (or we will call it characteristic):\n\nDefinition (Charateristic kernel): A RKHS (with corresponding kernel) is called characteristic if \\(\\operatorname{MMD}(P, Q; \\mathcal{F}) = 0\\) iff \\(P = Q\\)\n\nThat is when \\(P\\) and \\(Q\\) are the same, the value of MMD should be zero. What would be an appropriate kernel ? In this case, we would like to assume that kernel that we are working on is Translation Invariance i.e\n\nDefinition (Translation Invariance): The kernel \\(k\\) is called Translation Invariance if there is a function \\(f\\) such that: \\[k(x,y)=f(x-y)\\] for any \\(x\\) and \\(y\\)\n\nThen, one can have a fourier representation/coefficient of the kernel to be (assume we are within the domain of \\([-\\pi,\\pi]\\)) the multiple within the fourier series expansion:\n\\[\nk(x, y) = \\sum^\\infty_{l=-\\infty} \\hat{k}_l \\exp(il(x-y)) = \\sum^\\infty_{l=-\\infty}\\underbrace{\\left[{\\sqrt{\\hat{k}_l} \\exp(ilx) }\\right]}_{\\phi_l(x)}\\underbrace{\\left[{\\sqrt{\\hat{k}_l}\\exp(-ily)}\\right]}_{\\overline{\\phi_l(y)}}\n\\]\n\\(\\hat{k}_l\\) is called the fourier coefficient of the kernel. For the probability distribution, one can also have a similar way to find the fourier coefficient of them. We have the following result.\n\nTheorem: The value of MMD can be written as: \\[\\operatorname{MMD}^2(P, Q;\\mathcal{F}) = \\sum^\\infty_{l=-\\infty} |\\phi_{P,l} - \\phi_{Q, l}|^2\\hat{k}_l\\] for \\(\\hat{k}_l\\) being the fourier coefficient of the kernel, \\(\\phi_{P,l}\\) and \\(\\phi_{Q,l}\\) are fourier coefficient of the probability distributions \\(P\\) and \\(Q\\), respectively.\n\nTherefore, the kernel is characterisic iff none of the \\(\\hat{k}_l\\) is equal to zero.\nOn the other hand, instead of considering within specific range \\([\\pi,-\\pi]\\), one can also define the RKHS to be universal, which is when:\n\nDefinition (Universal RKHS): Given RKHS, it is universal if when: - \\(k(x, x')\\) is continuous - \\(\\mathcal{X}\\) is compact. - \\(\\mathcal{F}\\) is dense in \\(C(\\mathcal{X})\\) wrt. \\(L_\\infty\\) i.e for \\(\\varepsilon&gt;0\\) and \\(f\\in C(\\mathcal{X})\\), there is \\(g\\in\\mathcal{F}\\) such that: \\[\\|f-g\\|_\\infty\\le\\varepsilon\\]\n\nin which we can show that:\n\nTheorem: If \\(\\mathcal{F}\\) is universal then \\(\\operatorname{MMD}(P, Q;\\mathcal{F}) = 0\\) iff \\(P = Q\\)"
  },
  {
    "objectID": "posts/2-kernel-test/index.html#hilbert-schmidt-indepdent-criterion",
    "href": "posts/2-kernel-test/index.html#hilbert-schmidt-indepdent-criterion",
    "title": "Kernel Statistical Test",
    "section": "",
    "text": "Now, we are interested in given a pair of variables \\(\\{(x_i, y_i)\\}^n_{i=1}\\sim P_{XY}\\) are they dependent of each other ? - Usually one can use the MMD to find the differences whether this sample is sampled from the \\(P_XP_Y\\) (i.e product of marginal distribution). However, we don’t have an access to this. - Another question is: which kind of kernel would we be use ? is it a product kernel ? or different kind of kernels\n\n\nWe start off by defining the tensor product between elements in the Hilber space.\n\nDefinition (Tensor Product): Given element \\(a,b,c\\in\\mathcal{H}\\) of the Hilbert space, the tensor product between \\(a\\) and \\(b\\) is denoted as \\(a\\otimes b\\) such that: \\[(a\\otimes b)c = \\langle b,c\\rangle_\\mathcal{H}a\\] Note that this is analogous to when \\(a,b\\) and \\(c\\) are vector, then \\((ab^\\top)c=b^\\top ca\\)\n\nNow, we would like to extends the notion of the inner product (and norm) to the linear transformation between Hilbert space. This would gives us Hilbert-Schmidt Operators i.e\n\nDefinition (Hilbert-Schmidt Operators): Given a separable (countable orthonormal basis Hilbert spaces \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\) with orthonormal basis \\((f_i)_{i\\in I}\\) and \\((g_j)_{j\\in I}\\), respectively and 2 linear transformation between them: \\(L:\\mathcal{G}\\rightarrow\\mathcal{F}\\) and \\(M:\\mathcal{G}\\rightarrow\\mathcal{F}\\): \\[\\langle{L, M}\\rangle_{\\operatorname{HS}} = \\sum_{j\\in J}\\langle{Lg_j, Mg_j}\\rangle_\\mathcal{F}\\]\n\nNow, we can define the covariance operator (in similar manners to the mean embedding) as:\n\nDefinition (Covariance Operator): The covariance operators \\(C_{xy} : \\mathcal{G} \\rightarrow \\mathcal{F}\\) is given by: \\[\\langle{f, C_{xy}g}\\rangle_\\mathcal{F} = \\mathbb{E}_{xy}[f(x)g(y)]\\] which we can show to exists if the kernel associated \\(\\mathcal{G}\\) and \\(\\mathcal{F}\\): \\(k_1\\) and \\(k_2\\), respectively, are such that \\(k_1(x,x) &lt; \\infty\\) and \\(k_2(y,y)&lt;\\infty\\)\n\nThe existances can be proven by observe that, for any linear operator \\(A:\\mathcal{G} \\rightarrow \\mathcal{F}\\), we have:\n\\[\n\\langle{C_{xy}, A}\\rangle_{\\operatorname{HS}} = \\mathbb{E}_{xy}\\big[\\langle\\psi(x)\\otimes\\phi(y), A\\rangle_{\\operatorname{HS}}\\big]\n\\]\nand so we can use Riesz representation thoerem to proof the existence. Then, we are ready to define the HSIC\n\n\n\n\nDefinition (Hilbert-Schmidt Indepdent Criterion): The HSIC can be seen as the norm of the centered covariance operator i.e: \\[\\operatorname{HSIC}(P_{XY};\\mathcal{F}, \\mathcal{G}) = \\|{C_{xy} - \\mu_x\\otimes\\mu_y}\\|_{\\operatorname{HS}} = \\|{\\widetilde{C}_{xy}}\\|_{\\operatorname{HS}}\\]\n\nIn relation to MMD, we can show that"
  },
  {
    "objectID": "works.html",
    "href": "works.html",
    "title": "Works",
    "section": "",
    "text": "Apart from blogs, I also work on larger scale projects, here are the list of them.\n\n\n\n  \n\nThai Translation of Category theory for Programmers (ทฤษฎีcategoryสำหรับโปรแกรมเมอร์)\n\n\n\n\n\n  \n\n Bayesian Machine Learning and statistical learning theory for Courses at UCL\n\n\n\n\n\n  \n\n Notes on Category Theory and Type Theory (Work in Progress)"
  },
  {
    "objectID": "miscellaneous/1-plato/index.html",
    "href": "miscellaneous/1-plato/index.html",
    "title": "Introduction to Plato",
    "section": "",
    "text": "This post is a distillation of “Introduction to the study of Plato”, which is the first chapter of The Cambridge Companion to Plato. We will try to add other contents as well, for example: Stanford Encyclopedia of Philosophy (SEP) article on Plato 1. The main contents is to provides an overview/introduction of Plato’s works and development of his thought. Finally, the traslation will comes from  Perseus Digital Library."
  },
  {
    "objectID": "miscellaneous/1-plato/index.html#footnotes",
    "href": "miscellaneous/1-plato/index.html#footnotes",
    "title": "Introduction to Plato",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe author of most of the articles is Richard Kraut, where he is the prominant scholar on greek philosophy.↩︎\nThis comes from Eutyphro 2c and 3b. Plato has writting a series of dialogues about the trial and death of Socrates, which are (in chronological order): Eutyphro, Apology, Crito and Phaedo. All of them will be closely examined in later blog posts.↩︎\nThis is based on 19th century works and Aristotle.↩︎\nThis is a quotation from the Orcacle (see Apology 23a) given as the explanation to the riddle that Socrates tried to understand, stated as: Delphi (the Orcale) declared, when Chaerephon asked who is the wisest, that no one was wiser [than Socrates] (Apology 20e-21a). This puzzled Socrates since he is conscious that I am not wise either much or little (Apology 21b). We will explore this in more detail in later posts.↩︎\nFor More details, in Apology Socrates sought to under the Oracle’s statement (see the note above) by examine people who claim to posess some wisdom (Apology 21b) in order to invalidate the Oracle.↩︎\nThis relies on the fact that the soul is immortal, which is yet to be proven. The immortality will be main topic in Phaedo.↩︎\nPlato used mathematics as a tool for explaination. His love for mathematics isn’t presented in the eariler works, while appearing regulary in this middle and late dialogues.↩︎\nPlato also combined the theory of recollection and theory of form, as when we observe 2 equal sticks, as in our mind, we think about Equality. Since both 2 objects are not the same, the act of thinking should be an act of recollection. (Phaedo 74d)↩︎\nHowever, Plato doesn’t encorage truth-seeker to commit suicide in order to be free from the body. He gives the reason that we belong to Gods and thus it is best for us to be free when our owner said so (Phaedo 62d)↩︎\nThat is why he suggested abolishing the private wealth in ruling part as a partial solution, and of course to preven the lower class to be exploited. Furthermore, in Plato’s ideal city should have the sense of community, even if, they don’t share an equal understanding of human good.↩︎\nThe topic discussed in Philebus is concerning about the place of pleasure in the best human life, so it is logical to bring Socrates back as the main interlocutors.↩︎\nThe openning of the of Timaeus alludes to the conversation about the best city (talked to The Republic), but this might holds any weight.↩︎"
  }
]