<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2023-11-28">
<meta name="description" content="A cheat overview of the use of kernel method to perform high dimensional statistical test.">

<title>Kernel Statistical Test (WIP) – Phu’s Website</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-c37a40b267d3139d93432c283e34c257.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Phu’s Website</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../writings.html"> 
<span class="menu-text">Writings</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../works.html"> 
<span class="menu-text">Works</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../miscellaneous.html"> 
<span class="menu-text">Miscellaneous</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Kernel Statistical Test (WIP)</h1>
                  <div>
        <div class="description">
          A cheat overview of the use of kernel method to perform high dimensional statistical test.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 28, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#kernel-testing" id="toc-kernel-testing" class="nav-link active" data-scroll-target="#kernel-testing">Kernel Testing</a>
  <ul class="collapse">
  <li><a href="#statistical-testing" id="toc-statistical-testing" class="nav-link" data-scroll-target="#statistical-testing">Statistical Testing</a>
  <ul class="collapse">
  <li><a href="#u-and-v-statistics" id="toc-u-and-v-statistics" class="nav-link" data-scroll-target="#u-and-v-statistics">U And V Statistics</a></li>
  <li><a href="#quality-of-statistical-test" id="toc-quality-of-statistical-test" class="nav-link" data-scroll-target="#quality-of-statistical-test">Quality of Statistical Test</a></li>
  </ul></li>
  <li><a href="#quick-introduction-to-rkhs" id="toc-quick-introduction-to-rkhs" class="nav-link" data-scroll-target="#quick-introduction-to-rkhs">Quick Introduction to RKHS</a></li>
  <li><a href="#interlude-why-kernel-in-statistical-testing" id="toc-interlude-why-kernel-in-statistical-testing" class="nav-link" data-scroll-target="#interlude-why-kernel-in-statistical-testing">Interlude (Why Kernel in Statistical Testing?)</a></li>
  <li><a href="#maximum-mean-discrepancy-mmd" id="toc-maximum-mean-discrepancy-mmd" class="nav-link" data-scroll-target="#maximum-mean-discrepancy-mmd">Maximum Mean Discrepancy (MMD)</a>
  <ul class="collapse">
  <li><a href="#mean-embedding" id="toc-mean-embedding" class="nav-link" data-scroll-target="#mean-embedding">Mean Embedding</a></li>
  <li><a href="#estimators-and-statistics" id="toc-estimators-and-statistics" class="nav-link" data-scroll-target="#estimators-and-statistics">Estimators and Statistics</a></li>
  <li><a href="#in-search-for-the-appropriate-kernel" id="toc-in-search-for-the-appropriate-kernel" class="nav-link" data-scroll-target="#in-search-for-the-appropriate-kernel">In search for the appropriate kernel</a></li>
  </ul></li>
  <li><a href="#hilbert-schmidt-indepdent-criterion" id="toc-hilbert-schmidt-indepdent-criterion" class="nav-link" data-scroll-target="#hilbert-schmidt-indepdent-criterion">Hilbert-Schmidt Indepdent Criterion</a>
  <ul class="collapse">
  <li><a href="#preliminary-defintions-covariance-operators" id="toc-preliminary-defintions-covariance-operators" class="nav-link" data-scroll-target="#preliminary-defintions-covariance-operators">Preliminary Defintions + Covariance Operators</a></li>
  <li><a href="#definitionpropertiesestimation" id="toc-definitionpropertiesestimation" class="nav-link" data-scroll-target="#definitionpropertiesestimation">Definition/Properties/Estimation</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="kernel-testing" class="level1">
<h1>Kernel Testing</h1>
<p>In this document, we explore how the MMD and HSIC are derived. We will start with the brief introduction to RKHS, and then moving on to the statistical testing procedures. This is based on <a href="https://github.com/Phutoast/UCL-CSML-Notes/blob/main/adv-topic-in-ml/adv-topic.pdf">lecture note</a> of the course <a href="http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/rkhscourse.html">Reproducing kernel Hilbert spaces in Machine Learning</a></p>
<div>
<p><span class="math display">\[\begin{equation*}
\newcommand{\dby}{\ \mathrm{d}}\newcommand{\bracka}[1]{\left( #1 \right)}\newcommand{\brackb}[1]{\left[ #1 \right]}\newcommand{\brackc}[1]{\left\{ #1 \right\}}\newcommand{\abs}[1]{\left|#1\right|}\newcommand{\brackd}[1]{\left\langle #1\right\rangle}
\newcommand{\norm}[1]{\left\| #1\right\|}
\end{equation*}\]</span></p>
</div>
<section id="statistical-testing" class="level2">
<h2 class="anchored" data-anchor-id="statistical-testing">Statistical Testing</h2>
<p>Before we start with any tools related to kernel, let’s review/introduce some tools for statistical testing.</p>
<section id="u-and-v-statistics" class="level3">
<h3 class="anchored" data-anchor-id="u-and-v-statistics">U And V Statistics</h3>
<p>This section is based on <span class="citation" data-cites="serfling2009approximation">Serfling (<a href="#ref-serfling2009approximation" role="doc-biblioref">2009</a>)</span> (which is cited in <span class="citation" data-cites="muandet2016kernel">Muandet et al. (<a href="#ref-muandet2016kernel" role="doc-biblioref">2016</a>)</span>). In a nutshell, U and V statistics are two ways to estimate an expectation of a function over number of sampled points. Formally:</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<div id="rem-setting-u-v-stat" class="proof remark">
<p><span class="proof-title"><em>Remark 1</em>. </span>Given <span class="math inline">\(\boldsymbol x_1, \boldsymbol x_2,\dots,\boldsymbol x_m\)</span> be independent observation on a distribution <span class="math inline">\(P\)</span>. Consider the functional <span class="math inline">\(\theta = \theta(P)\)</span> defined as:</p>
<p><span class="math display">\[
\begin{aligned}
    \theta(P) &amp;= \mathbb{E}_{P}[h(\boldsymbol x_1, \boldsymbol x_2, \dots, \boldsymbol x_m)] \\
    &amp;= \int\cdots\int h(\boldsymbol x_1,\boldsymbol x_2,\dots, \boldsymbol x_m)\dby P(\boldsymbol x_1)\cdots P(\boldsymbol x_m)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(h(\boldsymbol x_1,\boldsymbol x_2,\dots,\boldsymbol x_m)\)</span> is called kernel (not related to reproducing kernel below) and is a real-valued measurable function. Usually, we assume <span class="math inline">\(h(\boldsymbol x_1,\boldsymbol x_2,\dots,\boldsymbol x_m)\)</span> to be symmetric (permuttion of the inputs doesn’t affect the output) and:</p>
<p><span class="math display">\[
\mathbb{E}_{P}[h^2(\boldsymbol x_1,\boldsymbol x_2,\dots,\boldsymbol x_c)] &lt; \infty
\]</span></p>
</div>
</div>
</div>
<p>We can have unbiased estimate of this (hence the name <span class="math inline">\(U\)</span>), defined to be:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-u-stat" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1</strong></span> <strong>(U Statistics):</strong> Given the sample <span class="math inline">\(\boldsymbol x_1, \boldsymbol x_2,\dots, \boldsymbol x_n\)</span> where <span class="math inline">\(n\ge m\)</span>, then U-statistics of order <span class="math inline">\(m\)</span> is defined as:</p>
<p><span class="math display">\[
U_n = U(\boldsymbol x_1,\boldsymbol x_2,\dots,\boldsymbol x_n) = \cfrac{1}{\begin{pmatrix}n \\m \end{pmatrix}} \sum^{(1,2,\dots,n)}_{(i_1,i_2,\dots,i_m)} h(\boldsymbol x_{i_1},\boldsymbol x_{i_2},\dots,\boldsymbol x_{i_m})
\]</span></p>
<p>where <span class="math inline">\(\sum^{(1,2,\dots,n)}_{(i_1,i_2,\dots,i_m)}\)</span> is taken over the combination of <span class="math inline">\(m\)</span> distinct element <span class="math inline">\(\brackc{i_1,i_2,\dots,i_m}\)</span> from <span class="math inline">\(\brackc{1,2,\dots,n}\)</span>.</p>
</div>
</div>
</div>
<p>A good example of this is unbiased variance estimate:</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<div id="rem-u-stat-var-example" class="proof remark">
<p><span class="proof-title"><em>Remark 2</em>. </span>Let’s turn the variance into an expectation of some kernels:</p>
<p><span class="math display">\[
\begin{aligned}
    \operatorname{Var}_P(x) &amp;= \mathbb{E}[x^2] - \mathbb{E}[x]^2 \\
    &amp;= \frac{1}{2}\Big[\mathbb{E}_{x_1}[x_1^2] - \mathbb{E}_{x_1}[x_1]^2 + \mathbb{E}_{x_2}[x_2^2] - \mathbb{E}_{x_2}[x_{2}]^2\Big] \\
    &amp;= \frac{1}{2}\mathbb{E}_{x_1x_2}\Big[x_1^2 - 2x_1x_2 +x_2^2\Big] = \frac{1}{2}\mathbb{E}_{x_1x_2}\Big[(x_1 - x_2)^2\Big] \\
\end{aligned}
\]</span></p>
<p>This implies that we have the kernel <span class="math inline">\(h(x_1, x_2) = 1/2 (x_1 - x_2)^2\)</span>, thus the U-statistics is given by:</p>
<p><span class="math display">\[
\begin{aligned}
    U_n &amp;= \frac{2}{n(n-1)}\sum_{1\le i &lt; j \le n} \frac{1}{2} (x_i - x_j)^2 \\
    &amp;= \frac{1}{n(n-1)} \bracka{\sum^n_{i=1}x_i^2 - \brackb{\sum^n_{j=1} x_j}^2}
\end{aligned}
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-degen-u-stat" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2</strong></span> <strong>(Degenerate U-Statistics):</strong> We can further define the function (From <span class="citation" data-cites="serfling2009approximation">Serfling (<a href="#ref-serfling2009approximation" role="doc-biblioref">2009</a>)</span>, section 5.2.1)</p>
<p><span class="math display">\[
\begin{aligned}
h_c(\boldsymbol x_1, &amp;\boldsymbol x_2,\dots, \boldsymbol x_c) \\
&amp;= \mathbb{E}_{P}[h(\boldsymbol x_1, \dots, \boldsymbol x_c, \boldsymbol X_{c+1},\dots,\boldsymbol X_m)]
\end{aligned}
\]</span></p>
<p>And, with <span class="math inline">\(\sigma^2_c = \operatorname{var}_{P}(h_c)\)</span>, we can show that:</p>
<p><span class="math display">\[
0 = \sigma_0^2 \le\sigma^2_2\le\cdots\le\sigma^2_m = \operatorname{Var}_P(h)
\]</span></p>
<p>U-statistics is called degenerate of order <span class="math inline">\(k\)</span> if <span class="math inline">\(\sigma_1^2 = \sigma^2_2 = \cdots = \sigma^2_k = 0\)</span>.</p>
</div>
</div>
</div>
<p>Then, we have the following asymptotics results for both non-degenerate and degenerate U statistics:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<div id="thm-asym-no-degen-u" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1</strong></span> <strong>(<span class="citation" data-cites="serfling2009approximation">Serfling (<a href="#ref-serfling2009approximation" role="doc-biblioref">2009</a>)</span> Section 5.5.1, Theorem A):</strong> If <span class="math inline">\(\mathbb{E}_{P}[h^2] &lt; \infty\)</span> and <span class="math inline">\(\sigma^2_1 &gt; 0\)</span> then, the asymptotics behavior of non-degenerate U-statistics is characterized by: <span class="math display">\[
\sqrt{n} (U_n - \theta) \xrightarrow[]{d} \mathcal{N}(0, m^2(\sigma^2_1)^2)
\]</span> As <span class="math inline">\(\xrightarrow[]{d}\)</span> means converges in distribution.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<div id="thm-asym-1-degen-u" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2</strong></span> <strong>(<span class="citation" data-cites="serfling2009approximation">Serfling (<a href="#ref-serfling2009approximation" role="doc-biblioref">2009</a>)</span> Section 5.5.2):</strong> Asume that <span class="math inline">\(\sigma_1^2=0&lt;\sigma_2\)</span> and <span class="math inline">\(\mathbb{E}_{P}[h^2] &lt; \infty\)</span>, then:</p>
<p><span class="math display">\[
n(U_n-\theta) \xrightarrow{d} \frac{m(m-1)}{2}\sum^\infty_{j=1}\lambda_j(\chi^2_{1j}-1)
\]</span></p>
<p>where <span class="math inline">\(\chi^2_{11}, \chi^2_{12},\dots\)</span> are independent <span class="math inline">\(\chi^2\)</span>-distribution of order one random variables. And <span class="math inline">\(\lambda_j\)</span> is the eigenvalue of operatorn <span class="math inline">\(A\)</span>, for <span class="math inline">\(g\in L_2\)</span> and <span class="math inline">\(x\in\mathbb{R}\)</span>:</p>
<p><span class="math display">\[
Ag(x) = \int^\infty_{-\infty} \widetilde{h}_2(x_1, x_2)g(y)\text{ d}P(y)
\]</span></p>
<p>where <span class="math inline">\(\widetilde{h}_2 = h_2-\theta\)</span> (centered kernel).</p>
</div>
</div>
</div>
<p>In the case where the estimator of U-statistics may be hard to compute, we can use V-statistics instead (although the )</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-v-stat" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3</strong></span> <strong>(V-Statistics):</strong> The related statistics of U-Statistics is called V-statistics, which is also easier to compute:</p>
<p><span class="math display">\[
V_n = \frac{1}{n^m}\sum^n_{i_1=1}\cdots\sum^n_{i_m=1} h(\boldsymbol x_{i_1},\boldsymbol x_{i_2},\dots,\boldsymbol x_{i_m})
\]</span></p>
<p>This is equivalent <span class="math inline">\(\theta(P_n)\)</span> where <span class="math inline">\(P_n\)</span> is the discrete uniform distribution over the set <span class="math inline">\(\brackc{\boldsymbol x_{1},\boldsymbol x_{2},\dots,\boldsymbol x_{n}}\)</span></p>
</div>
</div>
</div>
</section>
<section id="quality-of-statistical-test" class="level3">
<h3 class="anchored" data-anchor-id="quality-of-statistical-test">Quality of Statistical Test</h3>
<p>To judge/pick the statistical test, we would have to define the concept of errors, in which there are two types. Suppose, we are given a test <span class="math inline">\(\phi(\brackc{X_n}^N_{n=1})\)</span> that takes <span class="math inline">\(N\)</span> data points, where it returns <span class="math inline">\(0\)</span> if <span class="math inline">\(\mathcal{H}_0\)</span> is true and <span class="math inline">\(1\)</span> otherwise. Then:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-type-1-error" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4</strong></span> <strong>(Type-I Error/Correct Error Rate):</strong> This happens when the null hypothesis is true but the test reject anyways. Furthermore, the test has <em>correct type 1 error rate</em> if for <span class="math inline">\(P \in \mathcal{H}_0\)</span> (under null hypothesis):</p>
<p><span class="math display">\[P
\bracka{\phi\bracka{\brackc{X_n}^N_{n=1}} = 1} \le \alpha
\]</span></p>
<p>with significance level <span class="math inline">\(\alpha\)</span>.</p>
</div>
</div>
</div>
<p>On the other hand, we can also have error when we are under <span class="math inline">\(\mathcal{H}_1\)</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-type-2-error" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5</strong></span> <em>(Type-II Error/Power/Consistence):</em> This happens when the null hypothesis is wrong but the test accepts the null hypothesis.</p>
<p>In which, a <em>power</em> is the probability that the test correctly rejects the null hypothesis under alternative hypothesis setting i.e under <span class="math inline">\(P\in \mathcal{H}_1\)</span>:</p>
<p><span class="math display">\[
\operatorname{Power} = P\bracka{\phi\bracka{\brackc{X_n}^N_{n=1}} = 1}
\]</span></p>
<p>Furthermore, the test <span class="math inline">\(\phi\)</span> is <em>consistence</em> if <span class="math inline">\(P \in \mathcal{H}_1\)</span> then: <span class="math display">\[\lim_{N\rightarrow\infty} P\bracka{\phi\bracka{\brackc{X_n}^N_{n=1}} = 1} = 1\]</span></p>
</div>
</div>
</div>
</section>
</section>
<section id="quick-introduction-to-rkhs" class="level2">
<h2 class="anchored" data-anchor-id="quick-introduction-to-rkhs">Quick Introduction to RKHS</h2>
<p>We recall that the Hilbert space (HS) is a vector space that are equipped with an inner product between vectors and returns a scalar result. Reproducing Kernel HS (RKHS) is the Hilbert spaces that is equipped with the a kernel (that is constructed by the non-unique feature maps). Let’s unpack this, by starting from the definition of kernel</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-kernel" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6</strong></span> <strong>(Kernel):</strong> Given the non-empty set <span class="math inline">\(\mathcal{X}\)</span>, we define a <em>kernel</em> to be <span class="math inline">\(k:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}\)</span> such that there is a Hilbert space <span class="math inline">\(\mathcal{H}\)</span> and a function (called <em>feature map</em>) <span class="math inline">\(\phi:\mathcal{X}\rightarrow\mathcal{H}\)</span> where:</p>
<p><span class="math display">\[k(x, y) = \langle \phi(x), \phi(y)\rangle_\mathcal{H}\]</span></p>
<p>Noted that vector space <span class="math inline">\(\mathcal{H}\)</span> doesn’t need to be finite dimension (and so it can have infinite dimension i.e a function like object).</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<div id="rem-non-unique-feat-map" class="proof remark">
<p><span class="proof-title"><em>Remark 3</em>. </span><em>(Not Unique Feature Map):</em> The good example of feature maps that doesn’t have to be unique is when:</p>
<p><span class="math display">\[
\phi_1(x) = x \qquad \phi_2(x) = \begin{bmatrix}x/\sqrt{2} \\ x/\sqrt{2}\end{bmatrix}
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<div id="rem-constructing-kernel" class="proof remark">
<p><span class="proof-title"><em>Remark 4</em>. </span><em>(Constructing a New Kernel from An Old One):</em> Given the fact that <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> are kernels, then we can show that, for any <span class="math inline">\(x, y\in\mathcal{X}\)</span>:</p>
<ul>
<li><span class="math inline">\(k_1(x, y)+k_2(x, y)\)</span></li>
<li><span class="math inline">\(k_1(x, y)*k_2(x, y)\)</span></li>
<li>For <span class="math inline">\(a\in\mathbb{R}\)</span>, such that <span class="math inline">\(ak_1(x,y)\)</span></li>
<li>For any function <span class="math inline">\(f:\mathcal{X}\rightarrow\mathcal{X}'\)</span> (can be neural network or any kind of functions) and kernel <span class="math inline">\(k':\mathcal{X}'\times\mathcal{X}'\rightarrow\mathbb{R}\)</span>, such that <span class="math inline">\(k'(\phi(x), \phi(y))\)</span></li>
</ul>
<p>are all kernel. With this would means that <span class="math inline">\(k(x, x') = (c + \langle x, x'\rangle)^m\)</span> is also a kernel, or any function that admits Taylor series <span class="math inline">\(f\)</span> (with convergences properties etc.), then <span class="math inline">\(f(\langle x, x'\rangle)\)</span> is also a kernel.</p>
</div>
</div>
</div>
<p>Now, we are ready to define the RKHS, in which it is a special Hilbert space with a special kind of kernel:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-rkhs" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7</strong></span> <strong>(Reproducing Kernel Hilbert Space):</strong> Given a Hilbert space <span class="math inline">\(\mathcal{H}\)</span> of <span class="math inline">\(\mathbb{R}\)</span> valued functions on non-empty set <span class="math inline">\(\mathcal{X}\)</span>, the kernel <span class="math inline">\(k:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}\)</span> is called <em>reproducing</em> and <span class="math inline">\(\mathcal{H}\)</span> is called RKHS if:</p>
<ul>
<li>For all <span class="math inline">\(x\in\mathcal{X}\)</span>, <span class="math inline">\(k(\cdot, x) \in\mathcal{H}\)</span>, then <span class="math inline">\(k(\cdot, x)\in\mathcal{H}\)</span></li>
<li>For all <span class="math inline">\(x\in\mathcal{X}\)</span>, <span class="math inline">\(\langle{f(\cdot), k(\cdot,x)\rangle}_\mathcal{H} = f(x)\)</span></li>
</ul>
</div>
</div>
</div>
<p>Given the defintion, one can see that:</p>
<p><span class="math display">\[
\langle k(\cdot, x), k(\cdot, y)\rangle_\mathcal{H} = k(x, y)
\]</span></p>
<p>which means that <span class="math inline">\(k(\cdot,x)\)</span> for any <span class="math inline">\(x\in\mathcal{X}\)</span> can be seen as the feature map (recall that it doesn’t have to be unique), we will call this a <em>canonical feature map</em>. The following result on the Hilbert space will be useful</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<div id="thm-riesz-rep" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3</strong></span> <strong>(Riesz Representation):</strong> In Hilbert space <span class="math inline">\(\mathcal{H}\)</span>, all bounded linear function <span class="math inline">\(f\)</span> is of form <span class="math inline">\(\brackd{\cdot,g}_\mathcal{H}\)</span> for some <span class="math inline">\(g\in\mathcal{H}\)</span>.</p>
</div>
</div>
</div>
<p>We also have the follows result that illustrate why RKHS is preferable compared to the normal Hilbert space of functions.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<div id="rem-further-obv-rkhs" class="proof remark">
<p><span class="proof-title"><em>Remark 5</em>. </span><em>(Advanced Topics):</em> Intuitively, we just say that the functions in RKHS acts “smoothly” and “predictably”:</p>
<p>If the distance between functions <span class="math inline">\(\|f-g\|_\mathcal{H}\)</span> is close to each other then its pointwise evaluation <span class="math inline">\(|f(x)-g(x)|\)</span> for any <span class="math inline">\(x\)</span> would also be close to each other.</p>
</div>
</div>
</div>
<p>And, the evaluation operation makes sense i.e it is bounded.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Propositions
</div>
</div>
<div class="callout-body-container callout-body">
<div id="prp-alt-def-rkhs" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1</strong></span> A Hilbert space <span class="math inline">\(\mathcal{H}\)</span> is an RKHS iff the operator <span class="math inline">\(\delta_x\)</span> such that for all <span class="math inline">\(f\in\mathcal{H}, x\in\mathcal{X}\)</span>, we have <span class="math inline">\(\delta_x f = f(x)\)</span> is bounded and linear.</p>
</div>
</div>
</div>
<details>
<p><span class="math inline">\(\boldsymbol{(\implies):}\)</span> Since <span class="math inline">\(\mathcal{H}\)</span> is an RKHS, then there is a kernel <span class="math inline">\(k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\)</span>.</p>
<p>Assuming that the evaluating operator is unique, we can define an evaluating operator to be <span class="math inline">\(\delta_x(f)=\brackd{f, k(\cdot, x)}_\mathcal{H}\)</span>, we can consider Cauchy-Schwarz inequality, for all <span class="math inline">\(f\in\mathcal{H}\)</span>, we have</p>
<p><span class="math display">\[
\begin{aligned}
\abs{\delta_x(f)} &amp;= \abs{\brackd{f, k(\cdot, x)}_\mathcal{H}} \le \norm{f}_\mathcal{H}\norm{k(\cdot, x)}_\mathcal{H} \\
&amp;= \norm{f}_\mathcal{H}\sqrt{\brackd{k(\cdot, x), k(\cdot, x)}} = \norm{f}_\mathcal{H}\sqrt{k(x, x)} &lt; \infty
\end{aligned}
\]</span></p>
<p>The linear property is straightforward.</p>
<p><span class="math inline">\(\boldsymbol{(\impliedby):}\)</span> We can define the reproducing kernel <span class="math inline">\(k(\cdot, x)\)</span> for each point <span class="math inline">\(x\)</span> to be the element such that <span class="math inline">\(\delta_x=\brackd{\cdot, k(\cdot, x)}\)</span> per Riesz representation theorem (<a href="#thm-riesz-rep" class="quarto-xref">Theorem&nbsp;3</a>) as <span class="math inline">\(\delta_x\)</span> is bounded and linear.</p>
<div style="text-align: right">
□
</div>
<summary>
Proof
</summary>
</details>
<p>Furthermore, if the kernel satisfies the special property, then there is going to be an RHKS that is equipped with the given kernel as:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<div id="thm-moore-aronszajn" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4</strong></span> <strong>(Moore-Aronszajn):</strong> A symmetric function <span class="math inline">\(k:\mathcal{X}\times\mathcal{X}\rightarrow \mathbb{R}\)</span> is positive definite if: for all <span class="math inline">\(a_1,a_2,\dots,a_n\in \mathbb{R}\)</span> and for all <span class="math inline">\(x_1,x_2,\dots, x_n\in\mathcal{X}\)</span>:</p>
<p><span class="math display">\[\sum^n_{i=1}\sum^n_{j=1}a_ia_jk(x_i, x_j)\ge0\]</span></p>
<p>If the kernel is <em>positive definite</em>, then there is a unique RKHS with the reproducing kernel <span class="math inline">\(k\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="interlude-why-kernel-in-statistical-testing" class="level2">
<h2 class="anchored" data-anchor-id="interlude-why-kernel-in-statistical-testing">Interlude (Why Kernel in Statistical Testing?)</h2>
<p>Given the sample <span class="math inline">\((x_i)^m_{i=1}\sim p\)</span> and <span class="math inline">\((y_i)^m_{i=1}\sim q\)</span>. Given any feature extraction function <span class="math inline">\(\phi\)</span>, one can find related kernel <span class="math inline">\(k(\cdot,\cdot)\)</span> to be: <span class="math inline">\(k(a, b)=\langle \phi(a), \phi(b)\rangle\)</span>. Therefore, the distance between their mean in a feature space of the kernel <span class="math inline">\(k(\cdot,\cdot)\)</span> can be computed as:</p>
<p><span class="math display">\[
\begin{aligned}
\Bigg\| \frac{1}{m}&amp;\sum^m_{i=1}\phi(x_i) - \frac{1}{n}\sum^n_{i=1}\phi(y_i) \Bigg\|^2 \\
&amp;= \frac{1}{m^2}\sum^m_{i=1}\sum^m_{j=1}k(x_i,x_j) + \frac{1}{n^2}\sum^n_{i=1}\sum^n_{j=1}k(y_i, y_j) - \frac{2}{mn}\sum^m_{i=1}\sum^n_{j=1}k(x_i, y_i)
\end{aligned}
\]</span></p>
<p>We can observe 2 things here:</p>
<ol type="1">
<li>If we set the feature extraction function to be <span class="math inline">\(\phi(a)=[a \ a^2]\)</span>, then we are able to compare both means and variance.</li>
<li>One can set the feature extraction function to be arbitrary, as long as one can find the appropriate corresponding kernel (that should be easier to compute than just an inner product of each other). For instance, with RBF, one can have feature extraction function with infinite features! (via Taylor series).</li>
</ol>
<p>Therefore, intuitively, we can perform a more power/non-linear relationship between samples. Let’s now move to the actual formulation of the statistical testing.</p>
</section>
<section id="maximum-mean-discrepancy-mmd" class="level2">
<h2 class="anchored" data-anchor-id="maximum-mean-discrepancy-mmd">Maximum Mean Discrepancy (MMD)</h2>
<p>In this section, we are going to given the description of 2 main statistical testing technique that relies on the kernel method: MMD and HSIC (together with its variations). Let’s start with some operators that will be useful for both.</p>
<section id="mean-embedding" class="level3">
<h3 class="anchored" data-anchor-id="mean-embedding">Mean Embedding</h3>
<p>Given the example above in the interlude, we can generalizes the <em>mean</em> of the features map given an element <span class="math inline">\(x\sim P\)</span>, as follows.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-mean-emb" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8</strong></span> <strong>(Mean Embedding):</strong> Given positive definite kernel <span class="math inline">\(k(x,x')\)</span> with probability distribution <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>, we define <span class="math inline">\(\mu_P\)</span> and <span class="math inline">\(\mu_Q\)</span> such that: <span class="math display">\[\langle{\mu_P, \mu_Q\rangle} = \mathbb{E}_{P, Q}[k(x, y)]\]</span></p>
<p>where <span class="math inline">\(x\sim P\)</span> and <span class="math inline">\(y \sim Q\)</span>. We can consider the expectation in an RKHS as <span class="math inline">\(\mathbb{E}_P[f(x)] = \langle{f, \mu_P}\rangle_\mathcal{H}\)</span> for any function <span class="math inline">\(f\in\mathcal{H}\)</span>, the function in the corresponding RKHS</p>
</div>
</div>
</div>
<p>With this, one can see that the empirical mean embedding can be given in the form of:</p>
<p><span class="math display">\[
\hat{\mu}_P = \frac{1}{m}\sum^m_{i=1}\phi(x_i) \qquad \text{ where } \qquad x_i\sim P
\]</span></p>
<p>In which, one can show that <span class="math inline">\(\mu_P\)</span> actually exists.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<div id="thm-mean-emb-exists" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5</strong></span> <strong>:</strong> The element <span class="math inline">\(\mu_P\in\mathcal{F}\)</span> defined as <span class="math display">\[\mathbb{E}_P[f(x)] = \langle{f, \mu_P}\rangle_\mathcal{H}\]</span></p>
<p>exists, if the kernel <span class="math inline">\(k\)</span> of RKHS has the property that <span class="math inline">\(\mathbb{E}_P[\sqrt{k(x, x)}]&lt;
\infty\)</span></p>
</div>
</div>
</div>
<details>
<p>Using Riesz representation theorem (<a href="#thm-riesz-rep" class="quarto-xref">Theorem&nbsp;3</a>), we see that the operator <span class="math inline">\(E(f) = \mathbb{E}_P[f(x)]\)</span> is linear as the expectation is linear. And, it is bounded because:</p>
<p><span class="math display">\[
\begin{aligned}
\big|\mathbb{E}_P[f(x)]\big| &amp;\le \mathbb{E}_{P} \Big[ \big|f(x)\big| \Big] =  \mathbb{E}_{P} \Big[ \big|\brackd{f, k(\cdot, x)}_\mathcal{H}\big| \Big] \\
&amp;\le \mathbb{E}_P\big[ \norm{f}_\mathcal{H}\cdot\norm{k(\cdot, x)}_\mathcal{H} \big] \\
&amp;= \norm{f}_\mathcal{H} \mathbb{E}_P\brackb{ \sqrt{k(x, x)} } &lt;\infty
\end{aligned}
\]</span></p>
<p>So <span class="math inline">\(\mu_P\)</span> exists.</p>
<div style="text-align: right">
□
</div>
<summary>
Proof
</summary>
</details>
</section>
<section id="estimators-and-statistics" class="level3">
<h3 class="anchored" data-anchor-id="estimators-and-statistics">Estimators and Statistics</h3>
<p>Let’s formally define the notion of MMD, which tries to answer the question, does the samples <span class="math inline">\(\{x_i\}^n_{i=1}\)</span> and <span class="math inline">\(\{y_i\}^n_{i=1}\)</span> comes from the same distribution or not ?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-mmd" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9</strong></span> <strong>(MMD):</strong> MMD is the distance between <span class="math inline">\(2\)</span> probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> as (together with its, more computable form)</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{MMD}^2&amp;(P, Q) = \|\mu_P-\mu_Q\|^2_\mathcal{F} \\
&amp;= \mathbb{E}_P[k(x, x')] + \mathbb{E}_Q[k(y, y')] - 2\mathbb{E}_{P, Q}[k(x, y)]
\end{aligned}
\]</span></p>
<p>whereby, we have the following unbiased estimate of its quantity:</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\operatorname{MMD}}^2(P, Q) = \frac{1}{n(n-1)}\sum_{i\ne j}k(x_i, x_j) &amp;+ \frac{1}{n(n-1)}\sum_{i\ne j}k(y_i, y_j) \\
&amp;- \frac{2}{n^2}\sum_{i,j}k(x_i, y_j)
\end{aligned}
\]</span></p>
<p>for <span class="math inline">\(x_i\sim P\)</span> and <span class="math inline">\(y_i\sim Q\)</span></p>
</div>
</div>
</div>
<p>You may wonder, why does MMD is called <em>maximum mean</em> discrepancy ? One can show MMD can be written in an alternative form of:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-mmd-alt-form" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10</strong></span> <strong>:</strong> Alternatively MMD can be written as:</p>
<p><span class="math display">\[\operatorname{MMD}(P, Q) = \sup_{\|f\|\le1}\big(\mathbb{E}_P[f(x)] - \mathbb{E}_Q[f(x)]\big)\]</span></p>
<p>That is, given “smooth” function within a ball (therefore not being too extream), we find such a function that maximally distingush the sample of <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>, and this maximum disagreement is the MMD value.</p>
</div>
</div>
</div>
<details>
<p>We have that:</p>
<p><span class="math display">\[
\begin{aligned}
\sup_{\|f\|\le1}\big(\mathbb{E}_P[f(x)] - \mathbb{E}_Q&amp;[f(x)]\big) = \sup_{\|f\|\le1}\big(\brackd{f, \mu_P} - \brackd{f, \mu_Q}\big) \\
&amp;= \sup_{\|f\|\le1}\brackd{f, \mu_P-\mu_Q} \\
&amp;= \brackd{\frac{\mu_p-\mu_Q}{\norm{\mu_P-\mu_Q}_\mathcal{F}}, \mu_P-\mu_Q} \\
&amp;= \frac{\norm{\mu_P-\mu_Q}_\mathcal{F}^2}{\norm{\mu_P-\mu_Q}_\mathcal{F}} = \norm{\mu_P-\mu_Q}_\mathcal{F} \\
\end{aligned}
\]</span></p>
<p>The third equation follows from the fact that the inner product is maximized when <span class="math inline">\(\mu_P-\mu_Q\)</span> are in the same direction at <span class="math inline">\(f\)</span>.</p>
<div style="text-align: right">
□
</div>
<summary>
Proof
</summary>
</details>
<p>Now, back to the statistical testing, we can show that the value of MMD will have the following asymptotics distribution of:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<div id="thm-asym-dist-mmd" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6</strong></span> <strong>:</strong> We have the following distribution of the empirical MMD statistics:</p>
<ul>
<li><p>When <span class="math inline">\(P\ne Q\)</span>, we have: <span class="math display">\[\frac{\widehat{\operatorname{MMD}}^2 - \operatorname{MMD}(P, Q)^2}{\sqrt{V_n(P, Q)}} \xrightarrow{D} \mathcal{N}(0, 1)\]</span>where the variance <span class="math inline">\(V_n(P, Q) = \mathcal{O}(n^{-1})\)</span> depending on the kernel.</p></li>
<li><p>When <span class="math inline">\(P=Q\)</span>, we have: <span class="math display">\[n\widehat{\operatorname{MMD}}^2 \sim \sum^\infty_{l=1} \lambda_l[z^2_l - 2] \qquad \text{ where } \qquad \lambda_i\phi_i(x) = \int_\mathcal{X}\widetilde{k}(x,\widetilde{x})\phi_i(x)\text{ d}P(x)\]</span>where <span class="math inline">\(\widetilde{k}\)</span> is a centered kernel and <span class="math inline">\(z_l\sim\mathcal{N}(0, 2)\)</span></p></li>
</ul>
</div>
</div>
</div>
<details>
<p>For the first case, we see that the unbiased estimate in is a non-degenerate U-statistics (see <span class="citation" data-cites="gretton2012kernel">Gretton et al. (<a href="#ref-gretton2012kernel" role="doc-biblioref">2012</a>)</span>, lemma 6), so <a href="#thm-asym-no-degen-u" class="quarto-xref">Theorem&nbsp;1</a> is applied.</p>
<p>For the second case, it can be shown that if <span class="math inline">\(P=Q\)</span>, then the U-statistics is degenerate (see <span class="citation" data-cites="gretton2012kernel">Gretton et al. (<a href="#ref-gretton2012kernel" role="doc-biblioref">2012</a>)</span>, appendix B.1) so we can use <a href="#thm-asym-1-degen-u" class="quarto-xref">Theorem&nbsp;2</a>.</p>
<div style="text-align: right">
□
</div>
<summary>
Proof (Sketch)
</summary>
</details>
<p>However, to compute such a distribution with null-hypothesis <span class="math inline">\(P=Q\)</span> in closed form is hard, therefore:</p>
<ul>
<li>We have to rely on using a boostrap method which is done by <em>permuting</em> the set <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> before testing (i.e mixing them up)</li>
<li>This would gives us the estimate of the MMD statistics when <span class="math inline">\(P=Q\)</span>, which can them be used to compute the threshold for statistical test.</li>
</ul>
<p>Now, to find a best kernel, we can try to maximize its power:</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remark
</div>
</div>
<div class="callout-body-container callout-body">
<div id="rem-set-intial-terminal" class="proof remark">
<p><span class="proof-title"><em>Remark 6</em>. </span><em>(Finding a best kernel):</em> Given the distribution when <span class="math inline">\(P=Q\)</span>, using <a href="#def-type-2-error" class="quarto-xref">Definition&nbsp;5</a> of power, we denote <span class="math inline">\(\text{Pr}_1\)</span> as the probability under this condition, then:</p>
<p><span class="math display">\[
\text{Pr}_1\left({n\widehat{\operatorname{MMD}} &gt; \hat{c}_\alpha }\right) \rightarrow  \Phi\left({\frac{\operatorname{MMD}^2(P, Q)}{\sqrt{V_n(P, Q)}} - \frac{c_\alpha}{n\sqrt{V_n(P, Q)}} }\right)
\]</span></p>
<p>So, we can find the kernel that maximize the test power. Furthermore, it canbe shown that:</p>
<p><span class="math display">\[
\frac{\operatorname{MMD}^2(P, Q)}{\sqrt{V_n(P, Q)}} = \mathcal{O}(\sqrt{n}) \qquad \frac{c_\alpha}{n\sqrt{V_n(P, Q)}} =\mathcal{O}(n^{-1/2})
\]</span></p>
<p>Therefore, we can ignore the second term, and maximize the first term only.</p>
</div>
</div>
</div>
<p>We can even use neural network as feature extractor in kernel, and do the backpropagation of them, as in <span class="citation" data-cites="liu2020learning">Liu et al. (<a href="#ref-liu2020learning" role="doc-biblioref">2020</a>)</span></p>
<p><span class="math display">\[
k_\theta(x, y) = \big[(1-\varepsilon)\kappa(\Phi_\theta(x), \Phi_\theta(y))+\varepsilon\big]q(x, y)
\]</span></p>
<p>where <span class="math inline">\(\Phi_\theta\)</span> is a neural network and <span class="math inline">\(\kappa\)</span> and <span class="math inline">\(q\)</span> are Gaussian kernel, which is able to distinguish between CIFAR-10 vs CIFAR-10.1.</p>
</section>
<section id="in-search-for-the-appropriate-kernel" class="level3">
<h3 class="anchored" data-anchor-id="in-search-for-the-appropriate-kernel">In search for the appropriate kernel</h3>
<p>Starting with a defintion of a good kernel (or we will call it characteristic):</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-char-kernel" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 11</strong></span> <strong>(Characterisic):</strong> A RKHS (with corresponding kernel) is called <em>characteristic</em> if <span class="math inline">\(\operatorname{MMD}(P, Q; \mathcal{F}) = 0\)</span> iff <span class="math inline">\(P = Q\)</span></p>
</div>
</div>
</div>
<p>We would like to assume that kernel that we are working on is <em>Translation Invariance</em> i.e</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-trans-invar" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 12</strong></span> <strong>(Translation Invariance):</strong> The kernel <span class="math inline">\(k\)</span> is called <em>Translation Invariance</em> if there is a function <span class="math inline">\(f\)</span> such that:</p>
<p><span class="math display">\[k(x,y)=f(x-y)\]</span></p>
<p>for any <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span></p>
</div>
</div>
</div>
<p>Consider Fourier representation/coefficient of the kernel (assume we are within the domain of <span class="math inline">\([-\pi,\pi]\)</span>):</p>
<p><span class="math display">\[
\begin{aligned}
k(x, y) &amp;= \sum^\infty_{l=-\infty} \hat{k}_l \exp(il(x-y)) \\
&amp;= \sum^\infty_{l=-\infty}\underbrace{\left[{\sqrt{\hat{k}_l} \exp(ilx) }\right]}_{\phi_l(x)}\underbrace{\left[{\sqrt{\hat{k}_l}\exp(-ily)}\right]}_{\overline{\phi_l(y)}}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\hat{k}_l\)</span> is called the fourier coefficient of the kernel. We can also do find the Fourier representation of the probability distribution. In which one can show that:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<div id="thm-mmd-fourier" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7</strong></span> <strong>:</strong> The value of MMD can be written as:</p>
<p><span class="math display">\[
\operatorname{MMD}^2(P, Q;\mathcal{F}) = \sum^\infty_{l=-\infty} |\phi_{P,l} - \phi_{Q, l}|^2\hat{k}_l
\]</span></p>
<p>where <span class="math inline">\(\hat{k}_l\)</span>, <span class="math inline">\(\phi_{P,l}\)</span> and <span class="math inline">\(\phi_{Q,l}\)</span> are Fourier coefficient of the kernel, probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>, respectively.</p>
</div>
</div>
</div>
<p>Therefore, the kernel is characterisic iff none of the <span class="math inline">\(\hat{k}_l\)</span> is equal to zero.</p>
<p>On the other hand, instead of considering within specific range <span class="math inline">\([\pi,-\pi]\)</span>, one can also define the RKHS to be universal i.e</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<div id="def-univer-kernel" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 13</strong></span> <strong>(Universal RKHS):</strong> Given RKHS, it is <em>universal</em> if when:</p>
<ul>
<li><span class="math inline">\(k(x, x')\)</span> is continuous</li>
<li><span class="math inline">\(\mathcal{X}\)</span> is compact.</li>
<li><span class="math inline">\(\mathcal{F}\)</span> is dense in <span class="math inline">\(C(\mathcal{X})\)</span> wrt. <span class="math inline">\(L_\infty\)</span> i.e for <span class="math inline">\(\varepsilon&gt;0\)</span> and <span class="math inline">\(f\in C(\mathcal{X})\)</span>, there is <span class="math inline">\(g\in\mathcal{F}\)</span> such that: <span class="math display">\[\|f-g\|_\infty\le\varepsilon\]</span></li>
</ul>
</div>
</div>
</div>
<p>Then, we can show that:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<div id="thm-mmd-universal" class="theorem">
<p><span class="theorem-title"><strong>Theorem 8</strong></span> If <span class="math inline">\(\mathcal{F}\)</span> is universal then <span class="math inline">\(\operatorname{MMD}(P, Q;\mathcal{F}) = 0\)</span> iff <span class="math inline">\(P = Q\)</span></p>
</div>
</div>
</div>
</section>
</section>
<section id="hilbert-schmidt-indepdent-criterion" class="level2">
<h2 class="anchored" data-anchor-id="hilbert-schmidt-indepdent-criterion">Hilbert-Schmidt Indepdent Criterion</h2>
<p>Now, we are interested in given a pair of variables <span class="math inline">\(\{(x_i, y_i)\}^n_{i=1}\sim P_{XY}\)</span> are they dependent of each other ? - Usually one can use the MMD to find the differences whether this sample is sampled from the <span class="math inline">\(P_XP_Y\)</span> (i.e product of marginal distribution). However, we don’t have an access to this. - Another question is: which kind of kernel would we be use ? is it a product kernel ? or different kind of kernels</p>
<section id="preliminary-defintions-covariance-operators" class="level3">
<h3 class="anchored" data-anchor-id="preliminary-defintions-covariance-operators">Preliminary Defintions + Covariance Operators</h3>
<p>We start off by defining the tensor product between elements in the Hilbert space.</p>
<blockquote class="blockquote">
<p><strong>Definition (Tensor Product):</strong> Given element <span class="math inline">\(a,b,c\in\mathcal{H}\)</span> of the Hilbert space, the <em>tensor product</em> between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is denoted as <span class="math inline">\(a\otimes b\)</span> such that: <span class="math display">\[(a\otimes b)c = \langle b,c\rangle_\mathcal{H}a\]</span> Note that this is analogous to when <span class="math inline">\(a,b\)</span> and <span class="math inline">\(c\)</span> are vector, then <span class="math inline">\((ab^\top)c=b^\top ca\)</span></p>
</blockquote>
<p>Now, we would like to extends the notion of the inner product (and norm) to the linear transformation between Hilbert space. This would gives us Hilbert-Schmidt Operators i.e</p>
<blockquote class="blockquote">
<p><strong>Definition (Hilbert-Schmidt Operators):</strong> Given a separable (countable orthonormal basis Hilbert spaces <span class="math inline">\(\mathcal{F}\)</span> and <span class="math inline">\(\mathcal{G}\)</span> with orthonormal basis <span class="math inline">\((f_i)_{i\in I}\)</span> and <span class="math inline">\((g_j)_{j\in I}\)</span>, respectively and 2 linear transformation between them: <span class="math inline">\(L:\mathcal{G}\rightarrow\mathcal{F}\)</span> and <span class="math inline">\(M:\mathcal{G}\rightarrow\mathcal{F}\)</span>: <span class="math display">\[\langle{L, M}\rangle_{\operatorname{HS}} = \sum_{j\in J}\langle{Lg_j, Mg_j}\rangle_\mathcal{F}\]</span></p>
</blockquote>
<p>Now, we can define the covariance operator (in similar manners to the mean embedding) as:</p>
<blockquote class="blockquote">
<p><strong>Definition (Covariance Operator):</strong> The <em>covariance operators</em> <span class="math inline">\(C_{xy} : \mathcal{G} \rightarrow \mathcal{F}\)</span> is given by: <span class="math display">\[\langle{f, C_{xy}g}\rangle_\mathcal{F} = \mathbb{E}_{xy}[f(x)g(y)]\]</span> which we can show to exists if the kernel associated <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{F}\)</span>: <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>, respectively, are such that <span class="math inline">\(k_1(x,x) &lt; \infty\)</span> and <span class="math inline">\(k_2(y,y)&lt;\infty\)</span></p>
</blockquote>
<p>The existances can be proven by observe that, for any linear operator <span class="math inline">\(A:\mathcal{G} \rightarrow \mathcal{F}\)</span>, we have:</p>
<p><span class="math display">\[
\langle{C_{xy}, A}\rangle_{\operatorname{HS}} = \mathbb{E}_{xy}\big[\langle\psi(x)\otimes\phi(y), A\rangle_{\operatorname{HS}}\big]
\]</span></p>
<p>and so we can use Riesz representation thoerem to proof the existence. Then, we are ready to define the HSIC</p>
</section>
<section id="definitionpropertiesestimation" class="level3">
<h3 class="anchored" data-anchor-id="definitionpropertiesestimation">Definition/Properties/Estimation</h3>
<blockquote class="blockquote">
<p><strong>Definition (Hilbert-Schmidt Indepdent Criterion):</strong> The HSIC can be seen as the norm of the centered covariance operator i.e: <span class="math display">\[\operatorname{HSIC}(P_{XY};\mathcal{F}, \mathcal{G}) = \|{C_{xy} - \mu_x\otimes\mu_y}\|_{\operatorname{HS}} = \|{\widetilde{C}_{xy}}\|_{\operatorname{HS}}\]</span></p>
</blockquote>
<p>In relation to MMD, we can show that</p>



</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-gretton2012kernel" class="csl-entry" role="listitem">
Gretton, Arthur, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. 2012. <span>“A Kernel Two-Sample Test.”</span> <em>The Journal of Machine Learning Research</em> 13 (1): 723–73.
</div>
<div id="ref-liu2020learning" class="csl-entry" role="listitem">
Liu, Feng, Wenkai Xu, Jie Lu, Guangquan Zhang, Arthur Gretton, and Danica J Sutherland. 2020. <span>“Learning Deep Kernels for Non-Parametric Two-Sample Tests.”</span> In <em>International Conference on Machine Learning</em>, 6316–26. PMLR.
</div>
<div id="ref-muandet2016kernel" class="csl-entry" role="listitem">
Muandet, Krikamol, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard Schölkopf. 2016. <span>“Kernel Mean Embedding of Distributions: A Review and Beyond.”</span> <em>arXiv Preprint arXiv:1605.09522</em>.
</div>
<div id="ref-serfling2009approximation" class="csl-entry" role="listitem">
Serfling, Robert J. 2009. <em>Approximation Theorems of Mathematical Statistics</em>. Vol. 162. John Wiley &amp; Sons.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/Phutoast\.github\.io\/phutoast\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>