[
  {
    "objectID": "works.html",
    "href": "works.html",
    "title": "Works",
    "section": "",
    "text": "Apart from blogs, I also works on larger scale projects, here are the list of them.\n\n\n  \n\nThai Translation of Category theory for Programmers (ทฤษฎีcategoryสำหรับโปรแกรมเมอร์)"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html",
    "href": "posts/1-GiHKAL/index.html",
    "title": "Gaussian That I Have Known and Loved",
    "section": "",
    "text": "This write is based from the book: Pattern Recognition and Machine Learning, partly from CS229 notes on Guassian, and The Principles of Deep Learning Theory: An Effective Theory Approach to Understanding Neural Networks (especially on the chapter on Wick’s theorem, in which in this written we have provided the exposition on the proof too), where we aim to give and expands most proofs and interesting results regarding Gaussian distributions, together with some results that makes the complete picture."
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#gaussian-definition-and-fundamental-integral",
    "href": "posts/1-GiHKAL/index.html#gaussian-definition-and-fundamental-integral",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Gaussian Definition and Fundamental Integral",
    "text": "Gaussian Definition and Fundamental Integral\nDefinition (Single Variable Gaussian Distribution): The Gaussian distribution is defined as\n\n\\[\\begin{equation*}\n\\newcommand{\\dby}{\\ \\mathrm{d}}\\newcommand{\\argmax}[1]{\\underset{#1}{\\arg\\max \\ }}\\newcommand{\\argmin}[1]{\\underset{#1}{\\arg\\min \\ }}\\newcommand{\\const}{\\text{const.}}\\newcommand{\\bracka}[1]{\\left( #1 \\right)}\\newcommand{\\brackb}[1]{\\left[ #1 \\right]}\\newcommand{\\brackc}[1]{\\left\\{ #1 \\right\\}}\\newcommand{\\brackd}[1]{\\left\\langle #1 \\right\\rangle}\\newcommand{\\correctquote}[1]{``#1''}\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\\newcommand{\\abs}[1]{\\left|#1\\right|}\n    p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{ -\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right\\}\n\\end{equation*}\\]\n\nwhere \\(\\mu\\) is a parameter called mean, while \\(\\sigma\\) is a parameter called standard derivation. However, we also define \\(\\sigma^2\\) as variance. Finally, the standard normal distribution is Gaussian distribution with mean \\(0\\) and variance \\(1\\).\nProposition (Gaussian integral): The integration of \\(\\exp(-x^2)\\) is equal to\n\n\\[\\begin{equation*}\n    \\int^\\infty_{-\\infty}\\exp(-x^2)\\dby x = \\sqrt{\\pi}\n\\end{equation*}\\]\n\nWe can use this identity to find the normalizing factor of Gaussian distribution.\nProof: We will transform the problem into \\(2\\)D polar coordinate system, which make the integration easier.\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\bracka{\\int^\\infty_{-\\infty}\\exp(-x^2)\\dby x}^2 &= \\bracka{\\int^\\infty_{-\\infty}\\exp(-x^2)\\dby x}\\bracka{\\int^\\infty_{-\\infty}\\exp(-y^2)\\dby y} \\\\\n&= \\int^\\infty_{-\\infty}\\int^\\infty_{-\\infty}\\exp\\bracka{-(x^2+y^2)}\\dby x\\dby y \\\\\n&= \\int^{2\\pi}_{0}\\int^\\infty_{0}\\exp\\bracka{-r^2}r\\dby r\\dby\\theta \\\\\n&= 2\\pi\\int^\\infty_{0}\\exp\\bracka{-r^2}r\\dby r = \\pi\\int^{0}_{-\\infty} \\exp\\bracka{u}\\dby u = \\pi\n\\end{aligned}\n\\end{equation*}\\]\n\nPlease note that we use \\(u\\)-substution with \\(-r^2\\), in the last step.\n\n□\n\nCorollary (Normalization of Gaussian Distribution): We consider the integration\n\n\\[\\begin{equation*}\n    \\int^\\infty_{-\\infty}\\exp\\left\\{ -\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\} \\dby x = \\sqrt{2\\pi\\sigma^2}\n\\end{equation*}\\]\n\nProof: Starting by setting \\(z=x-\\mu\\), which we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\int^\\infty_{-\\infty}\\exp\\left\\{ -\\frac{z^2}{2\\sigma^2}\\right\\} \\dby z &= \\sigma\\sqrt{2} \\int^\\infty_{-\\infty}\\frac{1}{\\sqrt{2}\\sigma}\\exp\\left\\{ -\\frac{z^2}{2\\sigma^2}\\right\\} \\dby z \\\\\n&= \\sigma\\sqrt{2} \\int^\\infty_{-\\infty} \\exp(-y^2)\\dby = \\sigma\\sqrt{2\\pi}\n\\end{aligned}\n\\end{equation*}\\]\n\nwhere we have \\(y = z/(\\sqrt{2}\\sigma)\\), and we finishes the proof.\n\n□"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#statistics-of-single-variable-gaussian-distribution",
    "href": "posts/1-GiHKAL/index.html#statistics-of-single-variable-gaussian-distribution",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Statistics of Single Variable Gaussian Distribution",
    "text": "Statistics of Single Variable Gaussian Distribution\nDefinition (Odd Function): The function \\(f(x)\\) is an odd function iff \\(f(-x) = -x\\)\nLemma (Odd Function Integration): The integral over \\([-a, a]\\), where \\(a\\in\\mathbb{R}^+\\) of an odd function \\(f(x)\\) is \\(0\\) i.e\n\n\\[\\begin{equation*}\\int^a_{-a}f(x)\\dby x = 0\\end{equation*}\\]\n\nProof: We can see that:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\int^a_{-a}f(x)\\dby x &= \\int^0_{-a}f(x)\\dby x + \\int^a_{0}f(x)\\dby x \\\\\n    &= \\int^a_{0}f(-x)\\dby x + \\int^a_{0}f(x)\\dby x \\\\\n    &= -\\int^a_{0}f(x)\\dby x + \\int^a_{0}f(x)\\dby x = 0\n\\end{aligned}\n\\end{equation*}\\]\n\nThus complete the proof\n\n□\n\nProposition (Mean of Gaussian): The expectation \\(\\mathbb{E}_{x}[x]\\) of nornally distributed Gaussian is \\(\\mu\\)\nProof: Let’s consider the following integral\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{(x-\\mu)^2}{2\\sigma^2}}x\\dby x\n\\end{aligned}\n\\end{equation*}\\]\n\nWe will set \\(z = x - \\mu\\), where we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\frac{1}{\\sqrt{2\\pi\\sigma^2}}&\\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}(z+\\mu)\\dby x \\\\\n    &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\Bigg[ \\underbrace{\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z\\dby z}_{I_1} + \\underbrace{\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\mu\\dby z}_{I_2} \\Bigg]\n\\end{aligned}\n\\end{equation*}\\]\n\nLet’s consider \\(I_1\\), where it is clear that\n\n\\[\\begin{equation*}\n    g(x) = \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z = -\\bracka{-\\exp\\bracka{-\\frac{(-z)^2}{2\\sigma^2}}z} = -g(-x)\n\\end{equation*}\\]\n\nThus the function \\(g(x)\\) is and odd function. Therefore, making the integration \\(I_1\\) vanishes to \\(0\\). Please see the Lemma above for the proof. Now, for the second integration, we can simply recall the normalization result of the Gaussian, where\n\n\\[\\begin{equation*}\n    \\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\mu\\dby z = \\mu\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\dby z = \\mu\\sqrt{2\\pi\\sigma^2}\n\\end{equation*}\\]\n\nFinally, we have\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}(z+\\mu)\\dby x = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\Bigg[ 0 + \\mu\\sqrt{2\\pi\\sigma^2} \\Bigg] = \\mu\n\\end{aligned}\n\\end{equation*}\\]\n\nThus complete the proof.\n\n□\n\nLemma: The variance \\(\\operatorname{var}(x) = \\mathbb{E}[(x - \\mu)^2]\\) is equal to \\(\\mathbb{E}[x^2] - \\mathbb{E}[x]^2\\)\nProof: This is an application of expanding the definition\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\mathbb{E}[(x - \\mu)^2] &= \\mathbb{E}[x^2 -2x\\mu + \\mu^2] \\\\\n    &= \\mathbb{E}[x^2] - 2\\mathbb{E}[x]\\mu + \\mathbb{E}[\\mu^2] \\\\\n    &= \\mathbb{E}[x^2] - 2\\mathbb{E}[x]^2 + \\mathbb{E}[x]^2 \\\\\n    &= \\mathbb{E}[x^2] - \\mathbb{E}[x]^2 \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\n\n□\n\nProposition: The variance of normal distribution is \\(\\sigma^2\\)\nProof: Let’s consider the following equation, where we set \\(z = x-\\mu\\):\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\frac{1}{\\sqrt{2\\pi\\sigma^2}}& \\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{(x-\\mu)^2}{2\\sigma^2}}x^2\\dby x  = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int^\\infty_{-\\infty} \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}(z+\\mu)^2\\dby z \\\\\n    &=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\brackb{\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z^2\\dby z + \\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}2\\mu z\\dby z + \\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\mu^2\\dby z } \\\\\n    &=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\brackb{\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z^2\\dby z + 0 + \\mu^2\\sqrt{2\\pi\\sigma^2} \\dby z }\n\\end{aligned}\n\\end{equation*}\\]\n\nNow let’s consider the first integral, please note that\n\n\\[\\begin{equation*}\n    \\frac{d}{dz} \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}} = \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\bracka{-\\frac{z}{\\sigma^2}}\n\\end{equation*}\\]\n\nSo we can perform an integration by-part considering \\(u=z\\) and \\(dv = \\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\bracka{-\\frac{z}{\\sigma^2}}\\):\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\int^\\infty_{-\\infty}&\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}z^2\\dby z = -\\sigma^2\\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\bracka{-\\frac{z}{\\sigma^2}}z\\dby z \\\\\n    &= -\\sigma^2\\brackb{\\left.z\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\right|^\\infty_{-\\infty} - \\int^\\infty_{-\\infty}\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}}\\dby z} \\\\\n    &= -\\sigma^2[0 - \\sqrt{2\\pi\\sigma^2}] = \\sigma^2\\sqrt{2\\pi\\sigma^2}\n\\end{aligned}\n\\end{equation*}\\]\n\nTo show that the evaluation on the left-hand side is zero, we have\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\lim_{z\\rightarrow\\infty} z\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}} &- \\lim_{z\\rightarrow-\\infty} z\\exp\\bracka{-\\frac{z^2}{2\\sigma^2}} \\\\\n    &= 0 - \\lim_{z\\rightarrow-\\infty}1\\cdot\\exp\\bracka{-\\frac{x^2}{2\\sigma^2}}\\bracka{-\\frac{\\sigma^2}{z}} \\\\\n    &= 0-0 = 0\n\\end{aligned}\n\\end{equation*}\\]\n\nThe first equality comes from L’Hospital’s rule. Combinding the results:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\mathbb{E}[x^2] &- \\mathbb{E}[x]^2 = \\sigma^2 + \\mu^2 - \\mu^2 = \\sigma^2\n\\end{aligned}\n\\end{equation*}\\]\n\nThus complete the proof.\n\n□\n\nThis second part is to introduce some of the mathematical basics such as linear algebra. Further results of linear Gaussian models and others will be presented in the next part."
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#useful-backgrounds",
    "href": "posts/1-GiHKAL/index.html#useful-backgrounds",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Useful Backgrounds",
    "text": "Useful Backgrounds\n\nCovariance and Covariance Matrix\nDefinition (Covariance): Given 2 random variables \\(X\\) and \\(Y\\), the covariance is defined as:\n\n\\[\\begin{equation*}\n\\newcommand{\\dby}{\\ \\mathrm{d}}\\newcommand{\\argmax}[1]{\\underset{#1}{\\arg\\max \\ }}\\newcommand{\\argmin}[1]{\\underset{#1}{\\arg\\min \\ }}\\newcommand{\\const}{\\text{const.}}\\newcommand{\\bracka}[1]{\\left( #1 \\right)}\\newcommand{\\brackb}[1]{\\left[ #1 \\right]}\\newcommand{\\brackc}[1]{\\left\\{ #1 \\right\\}}\\newcommand{\\brackd}[1]{\\left\\langle #1 \\right\\rangle}\\newcommand{\\correctquote}[1]{``#1''}\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\operatorname{cov}(X, Y) = \\mathbb{E}\\Big[ (X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y]) \\Big]\n\\end{equation*}\\]\n\nIt is clear that $(X, Y) = (Y, X) $\nDefinition (Covariance Matrix): Now, if we have the collection of random variables in the random vector \\(\\boldsymbol x\\) (of size \\(n\\)), then the collection of covariance between its elements are collected in covariance matrix\n\n\\[\\begin{equation*}\n\\operatorname{cov}(\\boldsymbol x) =\n\\begin{bmatrix}\n    \\operatorname{cov}(x_1, x_1) & \\operatorname{cov}(x_1, x_2) & \\cdots & \\operatorname{cov}(x_1, x_n)  \\\\\n    \\operatorname{cov}(x_2, x_1) & \\operatorname{cov}(x_2, x_2) & \\cdots & \\operatorname{cov}(x_2, x_n)  \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\operatorname{cov}(x_n, x_1) & \\operatorname{cov}(x_n, x_2) & \\cdots & \\operatorname{cov}(x_n, x_n)  \\\\\n\\end{bmatrix} = \\mathbb{E}\\Big[ (\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])(\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])^T \\Big]\n\\end{equation*}\\]\n\nThe equivalent is clear when we perform the matrix multiplication over this.\nRemark (Property of Covariance Matrix): It is clear that the covariance matrix is (from its defintion):\n\nSymmetric, as \\(\\operatorname{cov}(x_a, x_b) = \\operatorname{cov}(x_b, x_a)\\)\nPositive semidefinite, if we consider arbitary constant vector \\(\\boldsymbol a \\in \\mathbb{R}^n\\), then by the linearity of expectation, we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol a^T \\operatorname{cov}(\\boldsymbol x)\\boldsymbol a\n&= \\boldsymbol a^T \\mathbb{E}\\Big[ (\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])(\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])^T \\Big]\\boldsymbol a  \\\\\n&= \\mathbb{E}\\Big[ \\boldsymbol a^T (\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])(\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])^T\\boldsymbol a \\Big]  \\\\\n&= \\mathbb{E}\\Big[ \\big(\\boldsymbol a^T (\\boldsymbol x - \\mathbb{E}[\\boldsymbol x])\\big)^2 \\Big] &gt; 0 \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\n\n\n\nLinear Algebra\nDefinition (Eigenvalues and Eigenvectors): Given the matrix \\(\\boldsymbol X \\in \\mathbb{C}^{n\\times n}\\), then the pair of vector \\(\\boldsymbol v \\in \\mathbb{C}^n\\) and number \\(\\lambda \\in \\mathbb{C}\\) are called eigenvector and eigenvalue iff\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol A \\boldsymbol v = \\lambda\\boldsymbol v\n\\end{aligned}\n\\end{equation*}\\]\n\nProposition (Eigenvalue of Symmetric Matrix): One can show that the eigenvalue of symmetric (real) matrix is always real.\nProof (From here): Let’s consider the pairs of eigenvalues/eigenvectors \\(\\boldsymbol v \\in \\mathbb{C}^n\\) and \\(\\lambda \\in \\mathbb{C}\\) of symmetric (real) matrix \\(\\boldsymbol A\\) i.e \\(\\boldsymbol A\\boldsymbol v = \\lambda\\boldsymbol v\\). Please note that \\((x+yi)(x-yi) = x^2 + y^2 \\ge 0\\), this means that \\(\\bar{\\boldsymbol v}^T\\boldsymbol v \\ge 0\\) (\\(\\bar{\\boldsymbol v}\\) is vector that contains conjugate element of \\(\\boldsymbol v\\)). Now, see that:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\bar{\\boldsymbol v}^T\\boldsymbol A\\boldsymbol v\n&= \\bar{\\boldsymbol v}^T(\\boldsymbol A\\boldsymbol v) = \\lambda\\bar{\\boldsymbol v}^T\\boldsymbol v  \\\\\n&= (\\bar{\\boldsymbol v}^T\\boldsymbol A^T)\\boldsymbol v = \\bar{\\lambda}\\bar{\\boldsymbol v}^T\\boldsymbol v  \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nNote that \\(\\overline{\\boldsymbol A\\boldsymbol v} = \\bar{\\lambda}\\bar{\\boldsymbol v}\\) Since \\(\\bar{\\boldsymbol v}^T\\boldsymbol v &gt; 0\\) we see that \\(\\bar{\\lambda} = \\lambda\\), which implies that \\(\\bar{\\lambda}\\) is real.\n\n□\n\nProposition (Eigenvalue of Positive Definite Matrix): One can show that the eigenvalue of positive definite matrix is non-negative.\nProof: We consider the following equations:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol v^T\\boldsymbol A\\boldsymbol v = \\lambda\\boldsymbol v^T\\boldsymbol v &gt; 0\n\\end{aligned}\n\\end{equation*}\\]\n\nAnd so the eigenvector must be \\(\\lambda &gt; 0\\).\n\n□\n\nDefinition (Linear Transformation): Given the function \\(A: V \\rightarrow W\\), where \\(V\\) and \\(W\\) are vector spaces. Then, for \\(\\boldsymbol v \\in V, \\boldsymbol w \\in W\\) and \\(a, b \\in \\mathbb{R}\\)\n\n\\[\\begin{equation*}\n\\begin{aligned}\nA(a\\boldsymbol v + b\\boldsymbol w) = aA(\\boldsymbol v) + bA(\\boldsymbol w)\n\\end{aligned}\n\\end{equation*}\\]\n\nRemark (Matrix Multipliacation and Linear Transformation): We can represent the linear transformation \\(L : V \\rightarrow W\\) in terms of matrix. Let’s consider the vector \\(\\boldsymbol v \\in V\\) (of dimension \\(n\\)) together with basis vectors \\(\\brackc{\\boldsymbol b_1,\\dots,\\boldsymbol b_n}\\) of \\(V\\) and basis vectors \\(\\brackc{\\boldsymbol c_1, \\dots, \\boldsymbol c_m}\\) of \\(W\\). Then we can represen the vector \\(\\boldsymbol v\\) as:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol v = v_1\\boldsymbol b_1 + \\dots + v_n\\boldsymbol b_n\n\\end{aligned}\n\\end{equation*}\\]\n\nThis means that we can represent the vector \\(\\boldsymbol v\\) as: \\((\\boldsymbol v_1, \\boldsymbol v_2, \\dots, \\boldsymbol v_n)^T\\) Furthermore, we can characterized the transformation of the basis vector:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    &L(\\boldsymbol b_i) = l_{1i} \\boldsymbol c_1 + l_{2i}\\boldsymbol c_2 + \\cdots + l_{mi}\\boldsymbol c_m \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nfor \\(i=1,\\dots,n\\). Then we can see that the definition of linear transformation together with the linear transformation of basis as we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    L(\\boldsymbol v) &= v_1L(\\boldsymbol b_1) + \\dots + v_nL(\\boldsymbol v_n) \\\\\n    &= \\begin{aligned}[t]\n        &v_1\\Big( l_{11} \\boldsymbol c_1 + l_{21}\\boldsymbol c_2 + \\cdots + l_{m1}\\boldsymbol c_m \\Big) \\\\\n        &+v_2\\Big( l_{12} \\boldsymbol c_1 + l_{22}\\boldsymbol c_2 + \\cdots + l_{m2}\\boldsymbol c_m \\Big) \\\\\n        &+v_n\\Big( l_{1n} \\boldsymbol c_1 + l_{2n}\\boldsymbol c_2 + \\cdots + l_{mn}\\boldsymbol c_m \\Big)\\\\\n    \\end{aligned} \\\\\n    &= \\begin{bmatrix}\n        \\sum^n_{i=1}v_il_{1i} \\\\ \\sum^n_{i=1}v_il_{2i} \\\\ \\vdots \\\\ \\sum^n_{i=1}v_il_{ni}\n    \\end{bmatrix} = \\begin{bmatrix}\n        l_{11} & l_{12} & \\cdots & l_{1n} \\\\\n        l_{21} & l_{22} & \\cdots & l_{2n} \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        l_{m1} & l_{m2} & \\cdots & l_{mn} \\\\\n    \\end{bmatrix}\n    \\begin{bmatrix}\n        v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n\n    \\end{bmatrix}\n\\end{aligned}\n\\end{equation*}\\]\n\nand so we have show that the linear transformation can be represented as matrix multiplication (in finite space).\nTheorem (Spectral Theorem) (follows from here): Let \\(n \\le N\\) and \\(W\\) be an n-dimensional subspace of \\(\\mathbb{R}^n\\). Given a linear transformation \\(A:W\\rightarrow W\\) that is symmetric. There are eigenvectors \\(\\boldsymbol v_1,\\dots,\\boldsymbol v_n\\in W\\) of \\(A\\) such that \\(\\brackc{\\boldsymbol v_1,\\dots,\\boldsymbol v_n}\\) is an orthonormal basis for \\(W\\). For normal matrix, we let \\(n=N\\) and \\(W=\\mathbb{R}^n\\).\nRemark (Eigendecomposition): Let’s consider the matrix of eigenvectors \\(\\boldsymbol v_1,\\boldsymbol v_2\\dots,\\boldsymbol v_n \\in \\mathbb{R}^n\\) of symmetric matrix \\(\\boldsymbol A \\in \\mathbb{R}^{n\\times n}\\) together with eigenvalues \\(\\lambda_1,\\lambda_2,\\dots,\\lambda_n\\), then we have :\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\boldsymbol A\n    \\begin{bmatrix}\n        \\kern.6em\\vline & \\kern.2em\\vline\\kern.2em & & \\vline\\kern.6em \\\\\n        \\kern.6em\\boldsymbol v_1 & \\kern.2em\\boldsymbol v_2\\kern.2em & \\kern.2em\\cdots\\kern.2em &  \\boldsymbol v_n\\kern.6em  \\\\\n        \\kern.6em\\vline & \\kern.2em\\vline\\kern.2em & & \\vline\\kern.6em \\\\\n    \\end{bmatrix} &=\n    \\begin{bmatrix}\n        \\kern.6em\\vline & \\kern.2em\\vline\\kern.2em & & \\vline\\kern.6em \\\\\n        \\kern.6em\\boldsymbol v_1 & \\kern.2em\\boldsymbol v_2\\kern.2em & \\kern.2em\\cdots\\kern.2em &  \\boldsymbol v_n\\kern.6em  \\\\\n        \\kern.6em\\vline & \\kern.2em\\vline\\kern.2em & & \\vline\\kern.6em \\\\\n    \\end{bmatrix}\\begin{bmatrix}\n        \\lambda_1 & 0 & \\cdots & 0 \\\\\n        0 & \\lambda_2 & \\cdots & 0 \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        0 & 0 & \\cdots & \\lambda_n \\\\\n    \\end{bmatrix} \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nThis is equivalent to \\(\\boldsymbol A\\boldsymbol Q = \\boldsymbol Q\\boldsymbol \\Lambda\\), which mean that if we right multiply by \\(\\boldsymbol Q^T\\) (as we have orthogonal eigenvectors), then we have (or in vectorized format):\n\n\\[\\begin{equation*}\n\\boldsymbol A = \\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^T = \\sum^n_{i=1}\\lambda_i\\boldsymbol v_i\\boldsymbol v_i^T\n\\end{equation*}\\]\n\nPlease note that \\(\\boldsymbol A^{-1}\\) can be represented as:\n\n\\[\\begin{equation*}\n\\boldsymbol A^{-1} = \\boldsymbol Q\\boldsymbol \\Lambda^{-1}\\boldsymbol Q^T = \\sum^n_{i=1}\\frac{1}{\\lambda_i}\\boldsymbol v_i\\boldsymbol v_i^T\n\\end{equation*}\\]\n\nas it is clear that \\(\\boldsymbol A\\boldsymbol A^{-1} = \\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^T\\boldsymbol Q\\boldsymbol \\Lambda^{-1}\\boldsymbol Q^T = \\boldsymbol I\\).\nProposition (Determinant and Eigenvalues): Give matrix \\(\\boldsymbol A\\) together with eigenvalues of \\(\\lambda_1,\\lambda_2,\\dots,\\lambda_n\\), then we can show that\n\n\\[\\begin{equation*}\n    \\abs{\\boldsymbol A} = \\prod^n_{i=1}\\lambda_i\n\\end{equation*}\\]\n\nProof (Diagonalizable Matrix): We consider the eigendecomposition of \\(\\boldsymbol A\\) (if it exists, which is most of the cases here. For more general proof, see linear algebra notes), as we have:\n\n\\[\\begin{equation*}\n    \\abs{\\boldsymbol A} = \\abs{\\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^{-1}} = \\abs{\\boldsymbol Q}\\abs{\\boldsymbol \\Lambda}\\abs{\\boldsymbol Q^{-1}} = \\frac{\\abs{\\boldsymbol Q}}{\\abs{\\boldsymbol Q}}\\abs{\\boldsymbol \\Lambda} =  \\prod^n_{i=1}\\lambda_i\n\\end{equation*}\\]\n\n\n□\n\nProposition (Trace and Eigenvalues): Given a matrix \\(\\boldsymbol A\\) together with eigenvalues of \\(\\lambda_1,\\lambda_2,\\dots,\\lambda_n\\), then we can show that:\n\n\\[\\begin{equation*}\n    \\operatorname{Tr}(\\boldsymbol A) = \\sum^n_{i=1}\\lambda_i\n\\end{equation*}\\]\n\nProof (Diagonalizable Matrix): We consider the eigendecomposition of \\(\\boldsymbol A\\). Consider the trace over it, as we have:\n\n\\[\\begin{equation*}\n    \\operatorname{Tr}(\\boldsymbol A) = \\operatorname{Tr}(\\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^{-1}) = \\operatorname{Tr}(\\boldsymbol \\Lambda\\boldsymbol Q\\boldsymbol Q^{-1}) = \\operatorname{Tr}(\\boldsymbol \\Lambda) = \\sum^n_{i=1}\\lambda_i\n\\end{equation*}\\]\n\n\n\nMiscellaneous\nProposition (Change of Variable): If we consider the transformation of the variables where \\(T : \\mathbb{R}^k \\supset X \\rightarrow \\mathbb{R}^k\\). Then we can show that:\n\n\\[\\begin{equation*}\n\\int_{\\mathbb{R}^k} f(\\boldsymbol y)\\dby \\boldsymbol y = \\int_{\\mathbb{R}^k} f(\\boldsymbol T(\\boldsymbol x))\\abs{\\boldsymbol J_T(\\boldsymbol x)}\\dby \\boldsymbol x\n\\end{equation*}\\]\n\nwhere \\(\\boldsymbol J_T(\\boldsymbol x)\\) is the Jacobian of the transformation \\(T(\\cdot)\\), which is defined to be:\n\n\\[\\begin{equation*}\n\\boldsymbol J_T(\\boldsymbol x) = \\begin{pmatrix}\n    \\cfrac{\\partial T_1(\\cdot)}{\\partial x_1} & \\cfrac{\\partial T_1(\\cdot)}{\\partial x_2}  & \\cdots & \\cfrac{\\partial T_1(\\cdot)}{\\partial x_n} \\\\\n    \\cfrac{\\partial T_2(\\cdot)}{\\partial x_1} & \\cfrac{\\partial T_2(\\cdot)}{\\partial x_2}  & \\cdots & \\cfrac{\\partial T_2(\\cdot)}{\\partial x_n} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\cfrac{\\partial T_n(\\cdot)}{\\partial x_1} & \\cfrac{\\partial T_n(\\cdot)}{\\partial x_2}  & \\cdots & \\cfrac{\\partial T_n(\\cdot)}{\\partial x_n} \\\\\n\\end{pmatrix}\n\\end{equation*}\\]"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#multivariate-guassian-distribution-introduction",
    "href": "posts/1-GiHKAL/index.html#multivariate-guassian-distribution-introduction",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Multivariate Guassian Distribution: Introduction",
    "text": "Multivariate Guassian Distribution: Introduction\nDefinition (Multivariate Gaussian): It is defined as:\n\n\\[\\begin{equation*}\n\\mathcal{N}(\\boldsymbol x | \\boldsymbol \\mu, \\boldsymbol \\Sigma) = \\frac{1}{\\sqrt{\\abs{2\\pi\\boldsymbol \\Sigma}}}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu)\\right\\}\n\\end{equation*}\\]\n\nwhere we call \\(\\boldsymbol \\mu \\in \\mathbb{R}^n\\) a mean and \\(\\boldsymbol \\Sigma \\in \\mathbb{R}^{n\\times n}\\) covariance, which should be symmetric and positive semidefinite (since the covariance is always positive semidefinite and symmetric).\nRemark (2D Independent Gaussian): Now, let’s consider, multivariate Gaussian but in the case that both variables are independent to each other with difference variances, as we define the parameters to be:\n\n\\[\\begin{equation*}\n\\boldsymbol \\mu = \\begin{bmatrix}\n    \\mu_1 \\\\ \\mu_2\n\\end{bmatrix} \\qquad \\boldsymbol \\Sigma =\n\\begin{bmatrix}\n    \\sigma_1^2 & 0 \\\\\n    0 & \\sigma_2^2 \\\\\n\\end{bmatrix}\n\\end{equation*}\\]\n\nNow, let’s expand the multivariate Guassian, please note that \\(\\abs{\\boldsymbol \\Sigma} = \\sigma_1^2\\sigma_2^2\\):\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\mathcal{N}(\\boldsymbol x | \\boldsymbol \\mu, \\boldsymbol \\Sigma) &= \\frac{1}{\\sqrt{\\abs{2\\pi\\boldsymbol \\Sigma}}}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu)\\right\\} \\\\\n&= \\frac{1}{4\\pi^2\\sigma_1^2\\sigma_2^2} \\exp\\brackc{-\\frac{1}{2} \\begin{bmatrix} x_1-\\mu_1 \\\\ x_2-\\mu_2 \\end{bmatrix}^T\n\\begin{bmatrix}\n    \\sigma_1^2 & 0 \\\\\n    0 & \\sigma_2^2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix} x_1-\\mu_1 \\\\ x_2-\\mu_2 \\end{bmatrix}} \\\\\n&= \\frac{1}{4\\pi^2\\sigma_1^2\\sigma_2^2} \\exp\\brackc{-\\frac{1}{2} \\brackb{ \\bracka{\\frac{x_1-\\mu_1}{\\sigma_1}}^2 + \\bracka{\\frac{x_2-\\mu_2}{\\sigma_2}}^2 }}\\\\\n&= \\frac{1}{2\\pi\\sigma_1^2} \\exp\\brackc{-\\frac{1}{2} \\frac{(x_1-\\mu_1)^2}{\\sigma_1^2}} \\frac{1}{2\\pi\\sigma_2^2} \\exp\\brackc{-\\frac{1}{2} \\frac{(x_2-\\mu_2)^2}{\\sigma_2^2}}\\\\\n&= \\mathcal{N}(x_1 | \\mu_1, \\sigma_1^2)\\mathcal{N}(x_2 | \\mu_2, \\sigma_2^2)\n\\end{aligned}\n\\end{equation*}\\]\n\nRemark (Shape of Gaussian): Let’s consider the eigendecomposition of the inverse covariance matrix (which is positive semidefinite and symmetric), as we have\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol \\Sigma^{-1} = \\sum^n_{i=1}\\frac{1}{\\lambda_i}\\boldsymbol u_i\\boldsymbol u_i^T\n\\end{aligned}\n\\end{equation*}\\]\n\nwhere \\(\\lambda_1,\\dots,\\lambda_n\\) and \\(\\boldsymbol u_1,\\dots,\\boldsymbol u_n\\) are the eigenvalues and eigenvectors, repectively of \\(\\boldsymbol \\Sigma\\). Let’s consider the terms inside the exponential to be:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x - \\boldsymbol \\mu)\n= \\sum^n_{i=1}\\frac{(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol u_i\\boldsymbol u_i^T(\\boldsymbol x - \\boldsymbol \\mu)}{\\lambda_i}\n= \\sum^n_{i=1}\\frac{y_i^2}{\\lambda_i}\n\\end{aligned}\n\\end{equation*}\\]\n\nwhere we have \\(y_i = (\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol u_i\\). Consider the vector to be \\(\\boldsymbol y = (y_1,y_2,\\dots,y_n)^T = \\boldsymbol U(\\boldsymbol x - \\boldsymbol \\mu)\\). This gives us the linear transformation over \\(\\boldsymbol x\\), which implies the following shape of Gaussian: - Ellipsoids with the center \\(\\boldsymbol \\mu\\) - Axis is in the direction of eigenvector \\(\\boldsymbol u_i\\) - Scaling of each direction is the eigenvector \\(\\lambda_i\\) associated with \\(\\boldsymbol u_i\\)\nProposition (Normalization of Gaussian): We can show that:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\int \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu)\\right\\} \\dby \\boldsymbol x = \\sqrt{\\abs{2\\pi\\boldsymbol \\Sigma}}\n\\end{aligned}\n\\end{equation*}\\]\n\nProof: Let’s consider the change of variable, in which we will change the variable from \\(x_i\\) to \\(y_i\\) where \\(\\boldsymbol y = \\boldsymbol U(\\boldsymbol x - \\boldsymbol \\mu)\\). To do this we have the find the Jacobian of the transformation, which is:\n\n\\[\\begin{equation*}\n\\begin{aligned}\nJ_{ij} = \\frac{\\partial x_i}{\\partial y_j} = U_{ji}\n\\end{aligned}\n\\end{equation*}\\]\n\nConsider its determinant, as we have: \\(\\abs{\\boldsymbol J}^2 = \\abs{\\boldsymbol U^T}^2 = \\abs{\\boldsymbol U^T}\\abs{\\boldsymbol U} = \\abs{\\boldsymbol U^T\\boldsymbol U} = \\abs{I} = 1\\). Consider the integration as we have (and use the Gaussian integrations):\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\int \\exp\\left\\{-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu)\\right\\} \\dby \\boldsymbol x\n&= \\int \\exp\\brackc{\\sum^n_{i=1}-\\frac{y_i^2}{2\\lambda_i}} |\\boldsymbol J| \\dby \\boldsymbol y \\\\\n&= \\int \\prod^n_{i=1}\\exp\\brackc{-\\frac{y_i^2}{2\\lambda_i}} \\dby \\boldsymbol y \\\\\n&= \\prod^n_{i=1}\\int \\exp\\brackc{-\\frac{y_i^2}{2\\lambda_i}} \\dby y_i \\\\\n&= \\prod^n_{i=1}\\sqrt{2\\pi\\lambda_i} = \\sqrt{\\abs{2\\pi\\boldsymbol \\Sigma}}\n\\end{aligned}\n\\end{equation*}\\]\n\nNote that we have shown that the determinant is the product of eigenvalues. Thus the prove is completed.\n\n□"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#useful-backgrounds-1",
    "href": "posts/1-GiHKAL/index.html#useful-backgrounds-1",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Useful Backgrounds",
    "text": "Useful Backgrounds\nProposition (Inverse of Partition Matrix): The block matrix can be inversed as:\n\n\\[\\begin{equation*}\n\\require{color}\n\\newcommand{\\dby}{\\ \\mathrm{d}}\\newcommand{\\argmax}[1]{\\underset{#1}{\\arg\\max \\ }}\\newcommand{\\argmin}[1]{\\underset{#1}{\\arg\\min \\ }}\\newcommand{\\const}{\\text{const.}}\\newcommand{\\bracka}[1]{\\left( #1 \\right)}\\newcommand{\\brackb}[1]{\\left[ #1 \\right]}\\newcommand{\\brackc}[1]{\\left\\{ #1 \\right\\}}\\newcommand{\\brackd}[1]{\\left\\langle #1 \\right\\rangle}\\newcommand{\\correctquote}[1]{``#1''}\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\\newcommand{\\abs}[1]{\\left|#1\\right|}\n\\definecolor{red}{RGB}{244, 67, 54}\n\\definecolor{pink}{RGB}{233, 30, 99}\n\\definecolor{purple}{RGB}{103, 58, 183}\n\\definecolor{yellow}{RGB}{255, 193, 7}\n\\definecolor{grey}{RGB}{96, 125, 139}\n\\definecolor{blue}{RGB}{33, 150, 243}\n\\definecolor{green}{RGB}{0, 150, 136}\n\\begin{bmatrix}\n    \\boldsymbol A & \\boldsymbol B \\\\\n    \\boldsymbol C & \\boldsymbol D \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n    \\boldsymbol M & -\\boldsymbol M\\boldsymbol B\\boldsymbol D^{-1} \\\\\n    -\\boldsymbol D^{-1}\\boldsymbol C\\boldsymbol M & \\boldsymbol D^{-1}+ \\boldsymbol D^{-1}\\boldsymbol C\\boldsymbol M\\boldsymbol B\\boldsymbol D^{-1} \\\\\n\\end{bmatrix}\n\\end{equation*}\\]\n\nwhere we set \\(\\boldsymbol M = (\\boldsymbol A-\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C)^{-1}\\)\nProposition (Inverse Matrix Identity): We can show that\n\n\\[\\begin{equation*}\n    (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1} = \\boldsymbol P\\boldsymbol B^T(\\boldsymbol B\\boldsymbol P\\boldsymbol B^T + \\boldsymbol R)^{-1}\n\\end{equation*}\\]\n\nProof: The can be proven by right multiply the inverse on the right hand-side:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    (\\boldsymbol P^{-1} + &\\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1}(\\boldsymbol B\\boldsymbol P\\boldsymbol B^T + \\boldsymbol R) \\\\\n    &= (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol B\\boldsymbol P\\boldsymbol B^T + (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol R \\\\\n    &= (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol B\\boldsymbol P\\boldsymbol B^T + (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\boldsymbol B \\\\\n    &= (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\Big[\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol B\\boldsymbol P + \\boldsymbol P^{-1} \\boldsymbol P\\Big]\\boldsymbol B^T\\\\\n    &= (\\boldsymbol P^{-1} + \\boldsymbol B^T\\boldsymbol R^{-1}\\boldsymbol B)^{-1}\\Big[\\boldsymbol B\\boldsymbol R^{-1}\\boldsymbol B+ \\boldsymbol P^{-1} \\Big]\\boldsymbol P\\boldsymbol B^T \\\\\n    &= \\boldsymbol P\\boldsymbol B^T \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\n\n□\n\nProposition (Woodbury Identity): We can show that\n\n\\[\\begin{equation*}\n    (\\boldsymbol A + \\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C)^{-1} = \\boldsymbol A^{-1} - \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\n\\end{equation*}\\]\n\nProof: The can be proven by right multiply the inverse on the right hand-side:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\Big[ \\boldsymbol A^{-1} &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1} \\Big](\\boldsymbol A + \\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C) \\\\\n    &= \\boldsymbol A^{-1}(\\boldsymbol A + \\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C) - \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}(\\boldsymbol A + \\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C) \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol A^{-1}\\boldsymbol A + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol A \\\\\n        &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol I + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C \\\\\n        &- \\boldsymbol A^{-1}\\boldsymbol B(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol I + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B\\Big[(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol D + (\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}\\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B\\Big]\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol I + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B\\Big[(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)^{-1}(\\boldsymbol D + \\boldsymbol C\\boldsymbol A^{-1}\\boldsymbol B)\\Big]\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        \\boldsymbol I + \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C &- \\boldsymbol A^{-1}\\boldsymbol B\\boldsymbol D^{-1}\\boldsymbol C \\\\\n    \\end{aligned} = \\boldsymbol I\n\\end{aligned}\n\\end{equation*}\\]\n\n\n□"
  },
  {
    "objectID": "posts/1-GiHKAL/index.html#conditional-marginalisation",
    "href": "posts/1-GiHKAL/index.html#conditional-marginalisation",
    "title": "Gaussian That I Have Known and Loved",
    "section": "Conditional & Marginalisation",
    "text": "Conditional & Marginalisation\nRemark (Settings): We consider the setting where we consider the partition of random variables:\n\n\\[\\begin{equation*}\n\\begin{bmatrix}\n    \\boldsymbol x_a \\\\ \\boldsymbol x_b\n\\end{bmatrix}\n\\sim \\mathcal{N}\\bracka{\n\\begin{bmatrix}\n    \\boldsymbol \\mu_a \\\\ \\boldsymbol \\mu_b\n\\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol \\Sigma_{aa} & \\boldsymbol \\Sigma_{ab} \\\\\n    \\boldsymbol \\Sigma_{ba} & \\boldsymbol \\Sigma_{bb} \\\\\n\\end{bmatrix}}\n\\end{equation*}\\]\n\nDue to the symmetric of covariance, we have \\(\\boldsymbol \\Sigma_{ab} = \\boldsymbol \\Sigma_{ba}^T\\). Furthermore, we denote\n\n\\[\\begin{equation*}\n\\boldsymbol \\Sigma^{-1} = \\begin{bmatrix}\n    \\boldsymbol \\Sigma_{aa} & \\boldsymbol \\Sigma_{ab} \\\\\n    \\boldsymbol \\Sigma_{ba} & \\boldsymbol \\Sigma_{bb} \\\\\n\\end{bmatrix}^{-1} =\n\\begin{bmatrix}\n    \\boldsymbol \\Lambda_{aa} & \\boldsymbol \\Lambda_{ab} \\\\\n    \\boldsymbol \\Lambda_{ba} & \\boldsymbol \\Lambda_{bb} \\\\\n\\end{bmatrix} = \\boldsymbol \\Lambda\n\\end{equation*}\\]\n\nwhere \\(\\boldsymbol \\Lambda\\) is called precision matrix. One can consider the inverse of partition matrix to find such a value of \\(\\boldsymbol \\Lambda\\) (will be useful afterward), thus we note that \\(\\boldsymbol \\Sigma_{aa} \\ne \\boldsymbol \\Lambda^{-1}_{aa}\\), and so on.\nRemark (Complete the Square): To find the conditional and marginalision (together with other kinds of Gaussian manipulation), we relies on a method called completing the square. Let’s consider the quadratic form expansion:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    -\\frac{1}{2}&(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu) \\\\\n    &= {\\color{blue}{-\\frac{1}{2}\\boldsymbol x^T\\boldsymbol \\Sigma^{-1}\\boldsymbol x + \\boldsymbol \\mu^T\\boldsymbol \\Sigma^{-1}\\boldsymbol x}} -\\frac{1}{2} \\boldsymbol \\mu^T\\boldsymbol \\Sigma^{-1}\\boldsymbol \\mu \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nNote that the blue term are the term that depends on \\(x\\). This means that to find the Gaussian, we will have to find the first and second order of \\(\\boldsymbol x\\) only. Or, we can consider each individual elements of partitioned random variable:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    -\\frac{1}{2}&(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol x-\\boldsymbol \\mu) \\\\\n    &= \\begin{aligned}[t]\n        {\\color{green}{-\\frac{1}{2}(\\boldsymbol x_a - \\boldsymbol \\mu_a)^T\\boldsymbol \\Lambda_{aa}(\\boldsymbol x_a - \\boldsymbol \\mu_a)}}{\\color{yellow}{ - \\frac{1}{2}(\\boldsymbol x_a - \\boldsymbol \\mu_a)^T\\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b)}} \\\\\n        {\\color{purple}{-\\frac{1}{2}(\\boldsymbol x_b - \\boldsymbol \\mu_b)^T\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a)}}{\\color{grey}{ - \\frac{1}{2}(\\boldsymbol x_b - \\boldsymbol \\mu_b)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol x_b - \\boldsymbol \\mu_b)}} \\\\\n    \\end{aligned} \\\\\n    &= \\begin{aligned}[t]\n        &{\\color{green} -\\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} -\\frac{1}{2} \\boldsymbol \\mu^T\\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu {\\color{yellow} -\\frac{1}{2}\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b + \\frac{1}{2} \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b} \\\\\n        &{\\color{yellow}+\\frac{1}{2}\\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b} - \\frac{1}{2}\\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b {\\color{purple} -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba} \\boldsymbol \\mu_a}  \\\\\n        &- \\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol \\mu_a {\\color{grey} -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b + \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b} -\\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b\n    \\end{aligned} \\\\\n\\end{aligned}\n\\label{eqn:1}\\tag{1}\n\\end{equation*}\\]\n\nCompleting the square is to match this pattern into our formula in order to get new Gaussian distribution. There are \\(2\\) ways to complete the squre that depends on the scenario: - When we want to find the Gaussian in difference form (but still being Gaussian) i.e conditional - When we want to marginalise some variables out or when we have to find the true for of the distribution without relying on knowing the final form (or when we are not really sure about the final form) i.e marginalisation, posterior\nLet’s just show how it works with examples.\nProposition (Conditional): Consider the following Gaussian\n\n\\[\\begin{equation*}\n\\begin{bmatrix}\n    \\boldsymbol x_a \\\\ \\boldsymbol x_b\n\\end{bmatrix}\n\\sim p(\\boldsymbol x_a, \\boldsymbol x_b) = \\mathcal{N}\\bracka{\n\\begin{bmatrix}\n    \\boldsymbol \\mu_a \\\\ \\boldsymbol \\mu_b\n\\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol \\Sigma_{aa} & \\boldsymbol \\Sigma_{ab} \\\\\n    \\boldsymbol \\Sigma_{ba} & \\boldsymbol \\Sigma_{bb} \\\\\n\\end{bmatrix}}\n\\end{equation*}\\]\n\nWe can show that: \\(p(\\boldsymbol x_a | \\boldsymbol x_b) = \\mathcal{N}(\\boldsymbol x | \\boldsymbol \\mu_{a|b}, \\boldsymbol \\Lambda^{-1}_{aa})\\), where we have - \\(\\boldsymbol \\mu_{a\\lvert b} = \\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{aa}^{-1}\\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b)\\) - Or, we can set \\(\\boldsymbol K = \\boldsymbol \\Sigma_{ab}\\boldsymbol \\Sigma_{bb}^{-1}\\), where we have\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    &\\boldsymbol \\mu_{a|b} = \\boldsymbol \\mu_a + \\boldsymbol K(\\boldsymbol x_b - \\boldsymbol \\mu_b)  \\qquad \\begin{aligned}[t]\n        \\boldsymbol \\Sigma_{a|b} &= \\boldsymbol \\Sigma_{aa} - \\boldsymbol K\\boldsymbol \\Sigma_{bb}\\boldsymbol K^T \\\\\n        &= \\boldsymbol \\Sigma_{aa} - \\boldsymbol \\Sigma_{ab}\\boldsymbol \\Sigma_{bb}^{-1}\\boldsymbol \\Sigma_{ba} \\\\\n    \\end{aligned}\n\\end{aligned}\n\\end{equation*}\\]\n\nPlease note that this follows from block-matrix inverse result (you can try plugging the results in).\nProof: We consider the expansion of the conditional distribution, as we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    -\\frac{1}{2}(\\boldsymbol x_a - \\boldsymbol \\mu_{a|b})^T\\boldsymbol \\Sigma_{a|b}^{-1}(\\boldsymbol x_a - \\boldsymbol \\mu_{a|b}) = {\\color{red}-\\frac{1}{2}\\boldsymbol x_a^T\\boldsymbol \\Sigma_{a|b}^{-1}\\boldsymbol x_a} + {\\color{blue}\\boldsymbol x_a^T\\boldsymbol \\Sigma_{a|b}^{-1}\\boldsymbol \\mu_{a|b}} + \\text{const}\n\\end{aligned}\n\\end{equation*}\\]\n\nLet’s consider the values, which should be equal to equation \\(\\eqref{eqn:1}\\), as we can see that: - The red term: we set \\(\\boldsymbol \\Sigma_{a\\lvert b}^{-1} = \\boldsymbol \\Lambda_{aa}\\) (consider the first green term of the equation \\(\\eqref{eqn:1}\\)) - The blue term., we will have to consider \\((\\dots)^T\\boldsymbol x_a\\). We have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n{\\color{green} \\boldsymbol \\mu_{a}^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} &{\\color{yellow} - \\frac{1}{2}\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b + \\frac{1}{2} \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b} {\\color{purple} - \\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a} \\\\\n&= \\boldsymbol x_a^T\\Big[ \\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{ab}\\boldsymbol x_b + \\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b \\Big] = \\boldsymbol x_a^T\\Big[ \\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b) \\Big] \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nPlease note that \\(\\boldsymbol \\Lambda_{ab}^T = \\boldsymbol \\Lambda_{ba}\\). Now, let’s do “pattern” matching, as we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol x_a^T&\\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_{a|b} = \\boldsymbol x_a^T\\Big[ \\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b) \\Big] \\\\\n\\implies&\\boldsymbol \\mu_{a|b} \\begin{aligned}[t]\n    &= \\boldsymbol \\Lambda_{aa}^{-1}\\boldsymbol \\Lambda_{aa}\\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{aa}^{-1}\\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b) \\\\\n    &= \\boldsymbol \\mu_a - \\boldsymbol \\Lambda_{aa}^{-1}\\boldsymbol \\Lambda_{ab}(\\boldsymbol x_b - \\boldsymbol \\mu_b) \\\\\n\\end{aligned}\n\\end{aligned}\n\\end{equation*}\\]\n\nThus the proof is complete.\n\n□\n\nProposition (Marginalisation): Consider the following Gaussian\n\n\\[\\begin{equation*}\n\\begin{bmatrix}\n    \\boldsymbol x_a \\\\ \\boldsymbol x_b\n\\end{bmatrix}\n\\sim p(\\boldsymbol x_a, \\boldsymbol x_b) = \\mathcal{N}\\bracka{\n\\begin{bmatrix}\n    \\boldsymbol \\mu_a \\\\ \\boldsymbol \\mu_b\n\\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol \\Sigma_{aa} & \\boldsymbol \\Sigma_{ab} \\\\\n    \\boldsymbol \\Sigma_{ba} & \\boldsymbol \\Sigma_{bb} \\\\\n\\end{bmatrix}}\n\\end{equation*}\\]\n\nWe can show that: \\(p(\\boldsymbol x_a) = \\mathcal{N}(\\boldsymbol x | \\boldsymbol \\mu_{a}, \\boldsymbol \\Sigma_{aa})\\)\nProof: We collect the terms that contains \\(\\boldsymbol x_b\\) so that we can integrate it out, as we have:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n{\\color{grey} -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b} &{\\color{grey} +} {\\color{grey}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b}\n{\\color{purple} -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba} \\boldsymbol \\mu_a}\n{\\color{yellow} -\\frac{1}{2}\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b+\\frac{1}{2}\\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol x_b} \\\\\n&=  -\\frac{1}{2}\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b + \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b -\\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\boldsymbol x_b^T\\boldsymbol \\Lambda_{ba} \\boldsymbol \\mu_a \\\\\n&=  -\\frac{1}{2}\\Big[\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b - 2\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\Lambda_{bb}^{-1}\\underbrace{\\Big(\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b -\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a)\\Big)}_{\\boldsymbol m}\\Big] \\\\\n&=  -\\frac{1}{2}\\Big[\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol x_b - 2\\boldsymbol x_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m + (\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m) \\Big] + \\frac{1}{2}(\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)  \\\\\n&=  -\\frac{1}{2}(\\boldsymbol x_b - \\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol x_b - \\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m) + {\\color{blue}\\frac{1}{2}\\boldsymbol m^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m}  \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nIf we integrate our the quantity, to be:\n\n\\[\\begin{equation*}\n\\int \\exp\\brackc{-\\frac{1}{2}(\\boldsymbol x_b - \\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)^T\\boldsymbol \\Lambda_{bb}(\\boldsymbol x_b - \\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m)}\\dby \\boldsymbol x_b\n\\end{equation*}\\]\n\nWe can use the Gaussian integration, like in part 1 and part 2. Now, we consider the other terms that doesn’t depends on \\(\\boldsymbol x_b\\) i.e all terms that depends on \\(\\boldsymbol x_a\\) together with the blue term that been left out.\n\n\\[\\begin{equation*}\n\\begin{aligned}\n{\\color{blue}\\frac{1}{2}\\boldsymbol m^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol m} &{\\color{green} -}{\\color{green} \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} {\\color{yellow} + \\frac{1}{2} \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b}{\\color{purple}+ \\frac{1}{2}\\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a} \\\\\n&= \\begin{aligned}[t]\n    \\frac{1}{2}\\Big(&\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b -\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a)\\Big)^T\\boldsymbol \\Lambda_{bb}^{-1}\\Big(\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b -\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a)\\Big) \\\\\n    &{\\color{green} - \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} + \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    \\frac{1}{2}\\Big[ \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b &- (\\boldsymbol x_a - \\boldsymbol \\mu_a)^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\mu_b \\\\\n    &- \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a) + (\\boldsymbol x_a - \\boldsymbol \\mu_a)^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}(\\boldsymbol x_a - \\boldsymbol \\mu_a) \\Big] \\\\\n    &{\\color{green} - \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} + \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    \\frac{1}{2}\\Big[ \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{bb}\\boldsymbol \\mu_b &- \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\mu_b + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\mu_b - \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a + \\boldsymbol \\mu_b^T\\boldsymbol \\Lambda_{ba}\\boldsymbol \\mu_a \\\\\n    &+ \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a - \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol\\mu_a - \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a \\\\\n    &+ \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol \\mu_a \\Big] {\\color{green} - \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} + \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    \\frac{1}{2}\\Big[-2\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\mu_b &+ \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol x_a - 2\\boldsymbol x_a^T\\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\boldsymbol\\mu_a\\Big] \\\\\n    &{\\color{green} - \\frac{1}{2}\\boldsymbol x^T_a\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a + \\boldsymbol \\mu_a^T\\boldsymbol \\Lambda_{aa}\\boldsymbol x_a} + \\boldsymbol x_a^T\\boldsymbol \\Lambda_{ab}\\boldsymbol \\mu_b + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\boldsymbol x_a^T\\Big[\\boldsymbol \\Lambda_{aa} - \\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\Big]\\boldsymbol x_a &+ \\boldsymbol x_a^T\\Big[\\boldsymbol \\Lambda_{aa} - \\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba}\\Big]\\boldsymbol\\mu_a + \\text{const}\n\\end{aligned} \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nIf we comparing this form to the normal quadratic expanision of Gaussian, we can set the \\(\\boldsymbol \\mu\\) of marginalised Gaussian is \\(\\boldsymbol \\mu_a\\), while the covariance is \\((\\boldsymbol \\Lambda_{aa} - \\boldsymbol \\Lambda_{ba}^T\\boldsymbol \\Lambda_{bb}^{-1}\\boldsymbol \\Lambda_{ba})^{-1}\\). If we compare this to the inverse matrix partition, we can see that this is equal to \\(\\boldsymbol \\Sigma_{aa}\\). Thus complete the proof.\n\n□\n\nProposition (Linear Gaussian Model): Consider the distribution to be: \\(p(\\boldsymbol x) = \\mathcal{N}(\\boldsymbol x \\vline {\\color{yellow} \\boldsymbol \\mu} , {\\color{blue} \\boldsymbol \\Lambda^{-1}})\\) and \\(p(\\boldsymbol y \\vline \\boldsymbol x) = \\mathcal{N}(\\boldsymbol y \\vline {\\color{purple}\\boldsymbol A}\\boldsymbol x + {\\color{green} \\boldsymbol b}, {\\color{red} \\boldsymbol L^{-1}})\\). We can show that the following holds:\n\n\\[\\begin{equation*}\np(\\boldsymbol y) = \\mathcal{N}(\\boldsymbol y \\vline {\\color{purple}\\boldsymbol A}{\\color{yellow} \\boldsymbol \\mu} + {\\color{green} \\boldsymbol b}, {\\color{red} \\boldsymbol L^{-1}} + {\\color{purple}\\boldsymbol A}{\\color{blue} \\boldsymbol \\Lambda^{-1}}{\\color{purple}\\boldsymbol A^T}) \\qquad p(\\boldsymbol x \\vline \\boldsymbol y) = \\mathcal{N}\\bracka{ \\boldsymbol x \\vline {\\color{grey} \\boldsymbol \\Sigma}\\brackc{ {\\color{purple}\\boldsymbol A^T} {\\color{red} \\boldsymbol L}(\\boldsymbol y-{\\color{green} \\boldsymbol b}) + {\\color{blue} \\boldsymbol \\Lambda}{\\color{yellow} \\boldsymbol \\mu}}, {\\color{grey} \\boldsymbol \\Sigma} }\n\\end{equation*}\\]\n\nwhere we have \\({\\color{grey} \\boldsymbol \\Sigma} = ({\\color{blue} \\boldsymbol \\Lambda} + {\\color{purple}\\boldsymbol A^T}{\\color{red} \\boldsymbol L}{\\color{purple}\\boldsymbol A})^{-1}\\)\nProof: We will consider the joint random variable \\(\\boldsymbol z = (\\boldsymbol x, \\boldsymbol y)^T\\). Let’s consider the joint distribution and the inside of exponential:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n-\\frac{1}{2}&(\\boldsymbol x - \\boldsymbol \\mu)^T\\boldsymbol \\Lambda(\\boldsymbol x - \\boldsymbol \\mu) - \\frac{1}{2}(\\boldsymbol y - \\boldsymbol A\\boldsymbol x - \\boldsymbol b)^T\\boldsymbol L(\\boldsymbol y - \\boldsymbol A\\boldsymbol x - \\boldsymbol b) + \\text{const} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\Big[ \\boldsymbol x^T\\boldsymbol \\Lambda\\boldsymbol x &- 2\\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol x + \\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol \\mu + \\boldsymbol y^T\\boldsymbol L\\boldsymbol y - \\boldsymbol y^T\\boldsymbol L\\boldsymbol A\\boldsymbol x - \\boldsymbol y^T\\boldsymbol L\\boldsymbol b\\\\\n    &-\\boldsymbol x^T\\boldsymbol A^T\\boldsymbol L\\boldsymbol y + \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol A\\boldsymbol x + \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol b - \\boldsymbol b^T\\boldsymbol L\\boldsymbol y +\\boldsymbol b^T\\boldsymbol L\\boldsymbol A\\boldsymbol x + \\boldsymbol b^T\\boldsymbol L\\boldsymbol b \\Big] + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\Big[ \\boldsymbol x^T\\boldsymbol \\Lambda\\boldsymbol x &- 2\\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol x + \\boldsymbol y^T\\boldsymbol L\\boldsymbol y - \\boldsymbol y^T\\boldsymbol L\\boldsymbol A\\boldsymbol x - \\boldsymbol y^T\\boldsymbol L\\boldsymbol b\\\\\n    &-\\boldsymbol x^T\\boldsymbol A^T\\boldsymbol L\\boldsymbol y + \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol A\\boldsymbol x + \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol b - \\boldsymbol b^T\\boldsymbol L\\boldsymbol y +\\boldsymbol b^T\\boldsymbol L\\boldsymbol A\\boldsymbol x  \\Big] + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\Big[ \\boldsymbol x^T\\Big(\\boldsymbol \\Lambda + \\boldsymbol A^T \\boldsymbol L \\boldsymbol A\\Big)\\boldsymbol x &+ \\boldsymbol y^T\\boldsymbol L\\boldsymbol y - \\boldsymbol y^T\\boldsymbol L\\boldsymbol A\\boldsymbol x - \\boldsymbol x^T\\boldsymbol A^T\\boldsymbol L\\boldsymbol y\\\\\n    &+ 2\\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol b  - 2\\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol x - 2\\boldsymbol y^T\\boldsymbol L\\boldsymbol b \\Big] + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\\Big[ \\boldsymbol x^T\\Big(\\boldsymbol \\Lambda + \\boldsymbol A^T \\boldsymbol L \\boldsymbol A\\Big)\\boldsymbol x &+ \\boldsymbol y^T\\boldsymbol L\\boldsymbol y - \\boldsymbol y^T\\boldsymbol L\\boldsymbol A\\boldsymbol x - \\boldsymbol x^T\\boldsymbol A^T\\boldsymbol L\\boldsymbol y\\Big]\\\\\n    &- \\boldsymbol x^T\\boldsymbol A^T \\boldsymbol L \\boldsymbol b + \\boldsymbol \\mu^T\\boldsymbol \\Lambda\\boldsymbol x + \\boldsymbol y^T\\boldsymbol L\\boldsymbol b  + \\text{const}\n\\end{aligned} \\\\\n&= \\begin{aligned}[t]\n    -\\frac{1}{2}\n    \\begin{pmatrix}\n        \\boldsymbol x \\\\ \\boldsymbol y\n    \\end{pmatrix}^T\n    \\begin{pmatrix}\n        \\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A & -\\boldsymbol A^T\\boldsymbol L \\\\\n        -\\boldsymbol L\\boldsymbol A & \\boldsymbol L\n    \\end{pmatrix}\n    \\begin{pmatrix}\n        \\boldsymbol x \\\\ \\boldsymbol y\n    \\end{pmatrix}  +\n    \\begin{pmatrix}\n        \\boldsymbol x \\\\ \\boldsymbol y\n    \\end{pmatrix}^T\n    \\begin{pmatrix}\n        \\boldsymbol \\Lambda\\boldsymbol \\mu - \\boldsymbol A^T\\boldsymbol L\\boldsymbol b \\\\\n        \\boldsymbol L\\boldsymbol b\n    \\end{pmatrix} + \\text{const}\n\\end{aligned} \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\nWe can use the block-matrix inverse result, to show that:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\begin{pmatrix}\n    \\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A & -\\boldsymbol A^T\\boldsymbol L \\\\\n    -\\boldsymbol L\\boldsymbol A & \\boldsymbol L\n\\end{pmatrix}^{-1} =\n\\begin{pmatrix}\n    \\boldsymbol \\Lambda^{-1} & \\boldsymbol \\Lambda^{-1}\\boldsymbol A^T \\\\\n    \\boldsymbol A\\boldsymbol \\Lambda^{-1} & \\boldsymbol L^{-1} + \\boldsymbol A\\boldsymbol \\Lambda^{-1}\\boldsymbol A^T\n\\end{pmatrix}\n\\end{aligned}\n\\end{equation*}\\]\n\nRecall the Gaussian pattern matching, we can see that the mean is equal to:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n\\begin{pmatrix}\n    \\boldsymbol \\Lambda^{-1} & \\boldsymbol \\Lambda^{-1}\\boldsymbol A^T \\\\\n    \\boldsymbol A\\boldsymbol \\Lambda^{-1} & \\boldsymbol L^{-1} + \\boldsymbol A\\boldsymbol \\Lambda^{-1}\\boldsymbol A^T\n\\end{pmatrix}\n\\begin{pmatrix}\n    \\boldsymbol \\Lambda\\boldsymbol \\mu - \\boldsymbol A^T\\boldsymbol L\\boldsymbol b \\\\\n    \\boldsymbol L\\boldsymbol b\n\\end{pmatrix} = \\begin{pmatrix}\n    \\boldsymbol \\mu \\\\ \\boldsymbol A\\boldsymbol \\mu + \\boldsymbol b\n\\end{pmatrix}\n\\end{aligned}\n\\end{equation*}\\]\n\nPlease note that with marginalisation result, it is obvious to see how this leads to the final result. Now, for the conditional result, we have the usual result that \\(\\boldsymbol \\Sigma= \\boldsymbol \\Lambda_{xx} = (\\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A)^{-1}\\), for the mean:\n\n\\[\\begin{equation*}\n\\begin{aligned}\n    \\boldsymbol \\mu - &(\\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A)^{-1}(\\boldsymbol A^T\\boldsymbol L)(-\\boldsymbol y + A\\boldsymbol \\mu + \\boldsymbol b) \\\\\n    &= \\boldsymbol \\Sigma\\boldsymbol A^T\\boldsymbol L(\\boldsymbol y - \\boldsymbol b) + \\boldsymbol \\mu - \\boldsymbol \\Sigma\\boldsymbol A^T\\boldsymbol L\\boldsymbol A\\boldsymbol \\mu\n\\end{aligned}\n\\end{equation*}\\]\n\nWe want to show that \\(\\boldsymbol \\mu - \\boldsymbol \\Sigma\\boldsymbol A^T\\boldsymbol L\\boldsymbol A\\boldsymbol \\mu = \\boldsymbol \\Sigma\\boldsymbol \\Lambda\\boldsymbol \\mu\\).\n\n\nLHS: Apply inverse indentity, where we consider the section highlight in pink:\n\\[\n\\begin{aligned}\n    \\boldsymbol \\mu &- \\boldsymbol \\Sigma\\boldsymbol A^T\\boldsymbol L\\boldsymbol A\\boldsymbol \\mu \\\\\n    &= \\boldsymbol \\mu - {\\color{pink}(\\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A)^{-1}\\boldsymbol A^T\\boldsymbol L}\\boldsymbol A\\boldsymbol \\mu \\\\\n    &= \\boldsymbol \\mu - \\boldsymbol \\Lambda^{-1}\\boldsymbol A^T(\\boldsymbol A\\boldsymbol \\Lambda^{-1}\\boldsymbol A^T + \\boldsymbol L^{-1})^{-1}\\boldsymbol A\\boldsymbol \\mu\n\\end{aligned}\n\\]\n\nRHS: We consider the section highlight in blue and use Woodbury Identity:\n\\[\n\\begin{aligned}\n    \\boldsymbol \\Sigma\\boldsymbol \\Lambda\\boldsymbol \\mu &= {\\color{blue}(\\boldsymbol \\Lambda + \\boldsymbol A^T\\boldsymbol L\\boldsymbol A)^{-1}}\\boldsymbol \\Lambda\\boldsymbol \\mu \\\\\n    &= \\boldsymbol \\mu - \\boldsymbol \\Lambda^{-1}\\boldsymbol A^T(\\boldsymbol A\\boldsymbol \\Lambda^{-1}\\boldsymbol A^T + \\boldsymbol L^{-1})^{-1}\\boldsymbol A\\boldsymbol \\mu\n\\end{aligned}\n\\]\n\n\nNow we have show that both are equal, thus we conclude the proof.\n\n□"
  },
  {
    "objectID": "miscellaneous/1-plato/index.html",
    "href": "miscellaneous/1-plato/index.html",
    "title": "Introduction to Plato",
    "section": "",
    "text": "This post is a distillation of “Introduction to the study of Plato”, which is the first chapter of The Cambridge Companion to Plato. We will try to add other contents as well, for example: Stanford Encyclopedia of Philosophy (SEP) article on Plato 1. The main contents is to provides an overview/introduction of Plato’s works and development of his thought. Finally, the traslation will comes from  Perseus Digital Library."
  },
  {
    "objectID": "miscellaneous/1-plato/index.html#footnotes",
    "href": "miscellaneous/1-plato/index.html#footnotes",
    "title": "Introduction to Plato",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe author of most of the articles is Richard Kraut, where he is the prominant scholar on greek philosophy.↩︎\nThis comes from Eutyphro 2c and 3b. Plato has writting a series of dialogues about the trial and death of Socrates, which are (in chronological order): Eutyphro, Apology, Crito and Phaedo. All of them will be closely examined in later blog posts.↩︎\nThis is based on 19th century works and Aristotle.↩︎\nThis is a quotation from the Orcacle (see Apology 23a) given as the explanation to the riddle that Socrates tried to understand, stated as: Delphi (the Orcale) declared, when Chaerephon asked who is the wisest, that no one was wiser [than Socrates] (Apology 20e-21a). This puzzled Socrates since he is conscious that I am not wise either much or little (Apology 21b). We will explore this in more detail in later posts.↩︎\nFor More details, in Apology Socrates sought to under the Oracle’s statement (see the note above) by examine people who claim to posess some wisdom (Apology 21b) in order to invalidate the Oracle.↩︎\nThis relies on the fact that the soul is immortal, which is yet to be proven. The immortality will be main topic in Phaedo.↩︎\nPlato used mathematics as a tool for explaination. His love for mathematics isn’t presented in the eariler works, while appearing regulary in this middle and late dialogues.↩︎\nPlato also combined the theory of recollection and theory of form, as when we observe 2 equal sticks, as in our mind, we think about Equality. Since both 2 objects are not the same, the act of thinking should be an act of recollection. (Phaedo 74d)↩︎\nHowever, Plato doesn’t encorage truth-seeker to commit suicide in order to be free from the body. He gives the reason that we belong to Gods and thus it is best for us to be free when our owner said so (Phaedo 62d)↩︎\nThat is why he suggested abolishing the private wealth in ruling part as a partial solution, and of course to preven the lower class to be exploited. Furthermore, in Plato’s ideal city should have the sense of community, even if, they don’t share an equal understanding of human good.↩︎\nThe topic discussed in Philebus is concerning about the place of pleasure in the best human life, so it is logical to bring Socrates back as the main interlocutors.↩︎\nThe openning of the of Timaeus alludes to the conversation about the best city (talked to The Republic), but this might holds any weight.↩︎"
  },
  {
    "objectID": "writings.html",
    "href": "writings.html",
    "title": "Phu's Website",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nMay 16, 2024\n\n\nYoneda Lemma: An Exploration (WIP)\n\n\nCategory Theory\n\n\n\n\nNov 28, 2023\n\n\nKernel Statistical Test\n\n\nMachine Learning\n\n\n\n\nNov 27, 2022\n\n\nGaussian That I Have Known and Loved\n\n\nMachine Learning\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "miscellaneous.html",
    "href": "miscellaneous.html",
    "title": "Phu's Website",
    "section": "",
    "text": "Here is the collection of writing/exposition/summary of philosophy (and other humanities subjects) notes/talks/books/chapters, please proceed with causion…\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nAug 2, 2022\n\n\nIntroduction to Plato\n\n\nphilosophers\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hey!",
    "section": "",
    "text": "When we first begin to believe anything, what we believe is not a single proposition, it is a whole system of propositions. (Light dawns gradually over the whole.)\n—Ludwig Wittgenstein’s On Certainty §141\n\n\nNice to meet you. I’m Phu Sakulwongtana (ภูร์ สกุลวงศ์ธนา) from Thailand. I’m generally interested in machine learning and mathematics (especially category theory and its application).\n\n\n\n\nThis website contains what I have learnt over the year. I decided to split my contents to be in 3 types:\n\nTechnical Writing: mainly about math (especially category theory) and machine learning\nCollection of my larger works: including my WIP thai translation of the book Category theory for Programmers by Bartosz Milewski, and other longer form writing.\nNotes on subjects that I am quite new into, for example: philosophy and other related fields (might be abit of chemistry and neuroscience mixed in). These are mostly expository/summary notes from the book that I have read.\n\n\n\nIf you find any mistakes and/or wanted to contact me, please send an email to:\n\n(first name)(last name)@gmail.com\n\nwithout any separation between first and last name"
  },
  {
    "objectID": "posts/2-kernel-test/index.html",
    "href": "posts/2-kernel-test/index.html",
    "title": "Kernel Statistical Test",
    "section": "",
    "text": "In this document, I would like to explain how the MMD and HSIC are derived (roughly). We will start with the brief introduction to RKHS, and then moving on to the statistical testing procedures. This is based on lecture note of the course Reproducing kernel Hilbert spaces in Machine Learning\n\n\nWe recall that the Hilbert space (HS) is a vector space that are equipped with an inner product between vectors and returns a scalar result. Reproducing Kernel HS (RKHS) is the Hilbert spaces that is equipped with the a kernel (that is constructed by the non-unique feature maps). Let’s unpack this, by starting from the definition of kernel\n\nDefinition (Kernel): Given the non-empty set \\(\\mathcal{X}\\), we define a kernel to be \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}\\) such that there is a Hilber space \\(\\mathcal{H}\\) and a function (called feature map) \\(\\phi:\\mathcal{X}\\rightarrow\\mathcal{H}\\) where: \\[k(x, y) = \\langle \\phi(x), \\phi(y)\\rangle_\\mathcal{H}\\] Noted that vector space \\(\\mathcal{H}\\) doesn’t need to be finite dimension (and so it can have infinite dimension i.e a function like object).\n\nThe good example of feature maps that doesn’t have to be unique is when:\n\\[\n\\phi_1(x) = x \\qquad \\phi_2(x) = \\begin{bmatrix}x/\\sqrt{2} \\\\ x/\\sqrt{2}\\end{bmatrix}\n\\]\nWe have the following way to construct a new kernel from the old one, given the fact that \\(k_1\\) and \\(k_2\\) are kernels, then we can show that, for any \\(x, y\\in\\mathcal{X}\\):\n\n\\(k_1(x, y)+k_2(x, y)\\)\n\\(k_1(x, y)*k_2(x, y)\\)\nFor \\(a\\in\\mathbb{R}\\), such that \\(ak_1(x,y)\\)\nFor any function \\(f:\\mathcal{X}\\rightarrow\\mathcal{X}'\\) (can be neural network or any kind of functions) and kernel \\(k':\\mathcal{X}'\\times\\mathcal{X}'\\rightarrow\\mathbb{R}\\), such that \\(k'(\\phi(x), \\phi(y))\\)\n\nare all kernel. With this would means that the following function \\(k(x, x') = (c + \\langle x, x'\\rangle)^m\\) is also a kernel, or if we have a function that admits Taylor series \\(f\\) (with convergences properties etc.), then \\(f(\\langle x, x'\\rangle)\\) is also a kernel.\nNow, we are ready to define the RKHS, in which it is a special Hilbert space with a special kind of kernel that satisfies additional\n\nDefinition (Reproducing Kernel Hilber Space): Given a Hilbert space \\(\\mathcal{H}\\) of \\(\\mathbb{R}\\) valued functions on non-empty set \\(\\mathcal{X}\\), the kernel \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}\\) is called reproducing and \\(\\mathcal{H}\\) is called RKHS if:\n\nFor all \\(x\\in\\mathcal{X}\\), \\(k(\\cdot, x) \\in\\mathcal{H}\\), then \\(k(\\cdot, x)\\in\\mathcal{H}\\)\nFor all \\(x\\in\\mathcal{X}\\), \\(\\langle{f(\\cdot), k(\\cdot,x)\\rangle}_\\mathcal{H} = f(x)\\)\n\n\nGiven the defintion, one can see that:\n\\[\n\\langle k(\\cdot, x), k(\\cdot, y)\\rangle_\\mathcal{H} = k(x, y)\n\\]\nwhich means that \\(k(\\cdot,x)\\) for any \\(x\\in\\mathcal{X}\\) can be seen as the feature map (recall that it doesn’t have to be unique), we will call this a canonical feature map.\nWe also have the follows result that illustrate why RKHS is preferable compared to the normal HS of functions.\n\nAdvanced Topics: Intuitively, we just say that the functions in RKHS acts “smoothly” and “predictably”, in the sense that: - If the distance between functions \\(\\|f-g\\|_\\mathcal{H}\\) is close to each other then its pointwise evaluation \\(|f(x)-g(x)|\\) for any \\(x\\) would also be close to each other. - This can be shown by the fact that the HS \\(\\mathcal{H}\\) has reproducing kernel iff the evaluation operator (defined as \\(\\delta_x : \\mathcal{H}\\rightarrow\\mathbb{R}\\) where \\(\\delta_x(f)=f(x)\\)) is bounded i.e \\(|\\delta_x(f)|\\le \\lambda_x\\|f\\|_\\mathcal{H}\\) for positive constant \\(\\lambda_x\\in\\mathbb{R}\\) (the proof uses the Riesz representation theorem)\n\nFurthermore, if the kernel satisfies the special property, then there is going to be an RHKS that is equipped with the given kernel as:\n\nTheorem (Moore-Aronszajn): A symmetric function \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow \\mathbb{R}\\) is positive definite if: for all \\(a_1,a_2,\\dots,a_n\\in \\mathbb{R}\\) and for all \\(x_1,x_2,\\dots,x_n\\in\\mathcal{X}\\): \\[\\sum^n_{i=1}\\sum^n_{j=1}a_ia_jk(x_i, x_j)\\ge0\\] If the kernel is positive definite, then there is a unique RKHS with the reproducing kernel \\(k\\).\n\n\n\n\nGiven the sample \\((x_i)^m_{i=1}\\sim p\\) and \\((y_i)^m_{i=1}\\sim q\\). Given any feature extraction function \\(\\phi\\), one can find related kernel \\(k(\\cdot,\\cdot)\\) to be: \\(k(a, b)=\\langle \\phi(a), \\phi(b)\\rangle\\). Therefore, the distance between their mean in a feature space of the kernel \\(k(\\cdot,\\cdot)\\) can be computed as:\n\\[\n\\begin{aligned}\n\\Bigg\\| \\frac{1}{m}&\\sum^m_{i=1}\\phi(x_i) - \\frac{1}{n}\\sum^n_{i=1}\\phi(y_i) \\Bigg\\|^2 \\\\\n&= \\frac{1}{m^2}\\sum^m_{i=1}\\sum^m_{j=1}k(x_i,x_j) + \\frac{1}{n^2}\\sum^n_{i=1}\\sum^n_{j=1}k(y_i, y_j) - \\frac{2}{mn}\\sum^m_{i=1}\\sum^n_{j=1}k(x_i, y_i)\n\\end{aligned}\n\\]\nWe can observe 2 things here: 1. If we set the feature extraction function to be \\(\\phi(a)=[a \\ a^2]\\), then we are able to compare both means and variance. 2. One can set the feature extraction function to be arbitrary, as long as one can find the appropriate corresponding kernel (that should be easier to compute than just an inner product of each other). For instance, with RBF, one can have feature extraction function with infinite features! (via Taylor series).\nTherefore, intuitively, we can perform a more power/non-linear relationship between samples. Let’s now move to the actual formulation of the statistical testing.\n\n\n\nIn this section, we are going to given the description of 2 main statistical testing technique that relies on the kernel method: MMD and HSIC (together with its variations). Let’s start with some operators that will be useful for both.\n\n\nGiven the example above in the interlude, we can generalizes the mean of the features map given an element \\(x\\sim P\\), as follows.\n\nDefinition (Mean Embedding): Given positive definite kernel \\(k(x,x')\\) with probability distribution \\(P\\) and \\(Q\\), we define \\(\\mu_P\\) and \\(\\mu_Q\\) such that: \\[\\langle{\\mu_P, \\mu_Q\\rangle} = \\mathbb{E}_{P, Q}[k(x, y)]\\] where \\(x\\sim P\\) and \\(y \\sim Q\\). We can consider the expectation in an RKHS as \\(\\mathbb{E}_P[f(x)] = \\langle{f, \\mu_P}\\rangle_\\mathcal{H}\\) for any function \\(f\\in\\mathcal{H}\\), the function in the corresponding RKHS\n\nWith this, one can see that the empirical mean embedding can be given in the form of:\n\\[\n\\hat{\\mu}_P = \\frac{1}{m}\\sum^m_{i=1}\\phi(x_i) \\qquad \\text{ where } \\qquad x_i\\sim P\n\\]\nIn which, one can show that this element exists.\n\nTheorem: The element \\(\\mu_P\\in\\mathcal{F}\\) defined as \\[\\mathbb{E}_P[f(x)] = \\langle{f, \\mu_P}\\rangle_\\mathcal{H}\\] if the kernel \\(k\\) of RKHS has the property that \\(\\mathbb{E}_P[\\sqrt{k(x, x)}]&lt;\n\\infty\\)\n\nProof Sketch: We can use the Riesz representation theorem by showing that the operator \\(T_Pf=\\mathbb{E}_P[f(x)]\\) is boudned, and thus there is \\(\\mu_P\\) such that \\(T_Pf=\\langle f,\\mu_P\\rangle\\)).\n\n\n\nLet’s formally define the notion of MMD, which tries to answer the question, does the samples \\(\\{x_i\\}^n_{i=1}\\) and \\(\\{y_i\\}^n_{i=1}\\) comes from the same distribution or not ?\n\nDefinition (MMD): Now, we define the quantity of MMD being the distance between \\(2\\) probability distributions \\(P\\) and \\(Q\\) as (together with its, more computable form) \\[ \\begin{aligned} \\operatorname{MMD}^2&(P, Q) = \\|\\mu_P-\\mu_Q\\|^2_\\mathcal{F} \\\\ &= \\mathbb{E}_P[k(x, x')] + \\mathbb{E}_Q[k(y, y')] - 2\\mathbb{E}_{P, Q}[k(x, y)] \\end{aligned} \\] whereby, we have the following unbiased estimate of its quantity: \\[ \\widehat{\\operatorname{MMD}}^2(P, Q) = \\frac{1}{n(n-1)}\\sum_{i\\ne j}k(x_i, x_j) + \\frac{1}{n(n-1)}\\sum_{i\\ne j}k(y_i, y_j) - \\frac{2}{n^2}\\sum_{i,j}k(x_i, y_j) \\] for \\(x_i\\sim P\\) and \\(y_i\\sim Q\\)\n\nYou may wonder, why does MMD is called maximum mean discrepancy ? One can show MMD can be written in an alternative form of:\n\nTheorem: We can show that the MMD can be written in an alternative form of: \\[\\operatorname{MMD}(P, Q) = \\sup_{\\|f\\|\\le1}\\big(\\mathbb{E}_P[f(x)] - \\mathbb{E}_Q[f(x)]\\big)\\]\n\nThis can be interpreted as, given “smooth” function within a ball (therefore not being too extream), we find such a function that maximally distingush the sample of \\(P\\) and \\(Q\\), and the maximum disagreement is the MMD value.\nNow, back to the statistical testing, we can show that the value of MMD will have the following asympototics distribution of:\n\nTheorem: We have the following distribution of the empirical MMD statistics as follows: - When \\(P\\ne Q\\), we have: \\[\\frac{\\widehat{\\operatorname{MMD}}^2 - \\operatorname{MMD}(P, Q)^2}{\\sqrt{V_n(P, Q)}} \\xrightarrow{D} \\mathcal{N}(0, 1)\\]where the variance \\(V_n(P, Q) = \\mathcal{O}(n^{-1})\\) but depends on the chosen kernel. - When \\(P=Q\\), we have: \\[n\\widehat{\\operatorname{MMD}}^2 \\sim \\sum^\\infty_{l=1} \\lambda_l[z^2_l - 2] \\qquad \\text{ where } \\qquad \\lambda_i\\phi_i(x) = \\int_\\mathcal{X}\\widetilde{k}(x,\\widetilde{x})\\phi_i(x)\\text{ d}P(x)\\]where \\(\\widetilde{k}\\) is a centered kernel and \\(z_l\\sim\\mathcal{N}(0, 2)\\)\n\nHowever, to compute such a distribution with null-hypothesis \\(P=Q\\) in closed form is hard, therefore:\n\nWe have to rely on using a boostrap method which is done by permuting the set \\(X\\) and \\(Y\\) before testing (i.e mixing them up)\nThis would gives us the estimate of the MMD statistics when \\(P=Q\\), which can them be used to compute the threshold for statistical test.\n\nNow, to find a best kernel, we have that:\n\nRemark (Finding a best kernek): Given the distribution when \\(P=Q\\), one can see that the power of the test is given to be: \\[\\text{Pr}_1\\left({n\\widehat{\\operatorname{MMD}} &gt; \\hat{c}_\\alpha }\\right) \\rightarrow  \\Phi\\left({\\frac{\\operatorname{MMD}^2(P, Q)}{\\sqrt{V_n(P, Q)}} - \\frac{c_\\alpha}{n\\sqrt{V_n(P, Q)}} }\\right)\\] To find the best kernel, we can find the kernel that maximize the test power. We would like to note the following: \\[\\frac{\\operatorname{MMD}^2(P, Q)}{\\sqrt{V_n(P, Q)}} = \\mathcal{O}(\\sqrt{n}) \\qquad \\frac{c_\\alpha}{n\\sqrt{V_n(P, Q)}} =\\mathcal{O}(n^{-1/2})\\] therefore, we can ignore the second term, and we can maximize the first term only, by setting this to be the objective of the neural network (that perform the feature extraction of the kernel). Note that we can derive the estimator for \\(V_n\\) too.\n\nFor example, one can use the following kernel from\n\\[\nk_\\theta(x, y) = \\big[(1-\\varepsilon)\\kappa(\\Phi_\\theta(x), \\Phi_\\theta(y))+\\varepsilon\\big]q(x, y)\n\\]\nwhere \\(\\Phi_\\theta\\) is a neural network and \\(\\kappa\\) and \\(q\\) are Gaussian kernel, which is able to distinguish between CIFAR-10 vs CIFAR-10 (image dataset).\nNow come the more important question, can we use any kernel to give us the appropriate MMD test ? The answer is obviously no, but what kind of kernel would be approriate ? Starting with a defintion of a good kernel (or we will call it characteristic):\n\nDefinition (Charateristic kernel): A RKHS (with corresponding kernel) is called characteristic if \\(\\operatorname{MMD}(P, Q; \\mathcal{F}) = 0\\) iff \\(P = Q\\)\n\nThat is when \\(P\\) and \\(Q\\) are the same, the value of MMD should be zero. What would be an appropriate kernel ? In this case, we would like to assume that kernel that we are working on is Translation Invariance i.e\n\nDefinition (Translation Invariance): The kernel \\(k\\) is called Translation Invariance if there is a function \\(f\\) such that: \\[k(x,y)=f(x-y)\\] for any \\(x\\) and \\(y\\)\n\nThen, one can have a fourier representation/coefficient of the kernel to be (assume we are within the domain of \\([-\\pi,\\pi]\\)) the multiple within the fourier series expansion:\n\\[\nk(x, y) = \\sum^\\infty_{l=-\\infty} \\hat{k}_l \\exp(il(x-y)) = \\sum^\\infty_{l=-\\infty}\\underbrace{\\left[{\\sqrt{\\hat{k}_l} \\exp(ilx) }\\right]}_{\\phi_l(x)}\\underbrace{\\left[{\\sqrt{\\hat{k}_l}\\exp(-ily)}\\right]}_{\\overline{\\phi_l(y)}}\n\\]\n\\(\\hat{k}_l\\) is called the fourier coefficient of the kernel. For the probability distribution, one can also have a similar way to find the fourier coefficient of them. We have the following result.\n\nTheorem: The value of MMD can be written as: \\[\\operatorname{MMD}^2(P, Q;\\mathcal{F}) = \\sum^\\infty_{l=-\\infty} |\\phi_{P,l} - \\phi_{Q, l}|^2\\hat{k}_l\\] for \\(\\hat{k}_l\\) being the fourier coefficient of the kernel, \\(\\phi_{P,l}\\) and \\(\\phi_{Q,l}\\) are fourier coefficient of the probability distributions \\(P\\) and \\(Q\\), respectively.\n\nTherefore, the kernel is characterisic iff none of the \\(\\hat{k}_l\\) is equal to zero.\nOn the other hand, instead of considering within specific range \\([\\pi,-\\pi]\\), one can also define the RKHS to be universal, which is when:\n\nDefinition (Universal RKHS): Given RKHS, it is universal if when: - \\(k(x, x')\\) is continuous - \\(\\mathcal{X}\\) is compact. - \\(\\mathcal{F}\\) is dense in \\(C(\\mathcal{X})\\) wrt. \\(L_\\infty\\) i.e for \\(\\varepsilon&gt;0\\) and \\(f\\in C(\\mathcal{X})\\), there is \\(g\\in\\mathcal{F}\\) such that: \\[\\|f-g\\|_\\infty\\le\\varepsilon\\]\n\nin which we can show that:\n\nTheorem: If \\(\\mathcal{F}\\) is universal then \\(\\operatorname{MMD}(P, Q;\\mathcal{F}) = 0\\) iff \\(P = Q\\)\n\n\n\n\n\nNow, we are interested in given a pair of variables \\(\\{(x_i, y_i)\\}^n_{i=1}\\sim P_{XY}\\) are they dependent of each other ? - Usually one can use the MMD to find the differences whether this sample is sampled from the \\(P_XP_Y\\) (i.e product of marginal distribution). However, we don’t have an access to this. - Another question is: which kind of kernel would we be use ? is it a product kernel ? or different kind of kernels\n\n\nWe start off by defining the tensor product between elements in the Hilber space.\n\nDefinition (Tensor Product): Given element \\(a,b,c\\in\\mathcal{H}\\) of the Hilbert space, the tensor product between \\(a\\) and \\(b\\) is denoted as \\(a\\otimes b\\) such that: \\[(a\\otimes b)c = \\langle b,c\\rangle_\\mathcal{H}a\\] Note that this is analogous to when \\(a,b\\) and \\(c\\) are vector, then \\((ab^\\top)c=b^\\top ca\\)\n\nNow, we would like to extends the notion of the inner product (and norm) to the linear transformation between Hilbert space. This would gives us Hilbert-Schmidt Operators i.e\n\nDefinition (Hilbert-Schmidt Operators): Given a separable (countable orthonormal basis Hilbert spaces \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\) with orthonormal basis \\((f_i)_{i\\in I}\\) and \\((g_j)_{j\\in I}\\), respectively and 2 linear transformation between them: \\(L:\\mathcal{G}\\rightarrow\\mathcal{F}\\) and \\(M:\\mathcal{G}\\rightarrow\\mathcal{F}\\): \\[\\langle{L, M}\\rangle_{\\operatorname{HS}} = \\sum_{j\\in J}\\langle{Lg_j, Mg_j}\\rangle_\\mathcal{F}\\]\n\nNow, we can define the covariance operator (in similar manners to the mean embedding) as:\n\nDefinition (Covariance Operator): The covariance operators \\(C_{xy} : \\mathcal{G} \\rightarrow \\mathcal{F}\\) is given by: \\[\\langle{f, C_{xy}g}\\rangle_\\mathcal{F} = \\mathbb{E}_{xy}[f(x)g(y)]\\] which we can show to exists if the kernel associated \\(\\mathcal{G}\\) and \\(\\mathcal{F}\\): \\(k_1\\) and \\(k_2\\), respectively, are such that \\(k_1(x,x) &lt; \\infty\\) and \\(k_2(y,y)&lt;\\infty\\)\n\nThe existances can be proven by observe that, for any linear operator \\(A:\\mathcal{G} \\rightarrow \\mathcal{F}\\), we have:\n\\[\n\\langle{C_{xy}, A}\\rangle_{\\operatorname{HS}} = \\mathbb{E}_{xy}\\big[\\langle\\psi(x)\\otimes\\phi(y), A\\rangle_{\\operatorname{HS}}\\big]\n\\]\nand so we can use Riesz representation thoerem to proof the existence. Then, we are ready to define the HSIC\n\n\n\n\nDefinition (Hilbert-Schmidt Indepdent Criterion): The HSIC can be seen as the norm of the centered covariance operator i.e: \\[\\operatorname{HSIC}(P_{XY};\\mathcal{F}, \\mathcal{G}) = \\|{C_{xy} - \\mu_x\\otimes\\mu_y}\\|_{\\operatorname{HS}} = \\|{\\widetilde{C}_{xy}}\\|_{\\operatorname{HS}}\\]\n\nIn relation to MMD, we can show that"
  },
  {
    "objectID": "posts/2-kernel-test/index.html#quick-introduction-to-rkhs",
    "href": "posts/2-kernel-test/index.html#quick-introduction-to-rkhs",
    "title": "Kernel Statistical Test",
    "section": "",
    "text": "We recall that the Hilbert space (HS) is a vector space that are equipped with an inner product between vectors and returns a scalar result. Reproducing Kernel HS (RKHS) is the Hilbert spaces that is equipped with the a kernel (that is constructed by the non-unique feature maps). Let’s unpack this, by starting from the definition of kernel\n\nDefinition (Kernel): Given the non-empty set \\(\\mathcal{X}\\), we define a kernel to be \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}\\) such that there is a Hilber space \\(\\mathcal{H}\\) and a function (called feature map) \\(\\phi:\\mathcal{X}\\rightarrow\\mathcal{H}\\) where: \\[k(x, y) = \\langle \\phi(x), \\phi(y)\\rangle_\\mathcal{H}\\] Noted that vector space \\(\\mathcal{H}\\) doesn’t need to be finite dimension (and so it can have infinite dimension i.e a function like object).\n\nThe good example of feature maps that doesn’t have to be unique is when:\n\\[\n\\phi_1(x) = x \\qquad \\phi_2(x) = \\begin{bmatrix}x/\\sqrt{2} \\\\ x/\\sqrt{2}\\end{bmatrix}\n\\]\nWe have the following way to construct a new kernel from the old one, given the fact that \\(k_1\\) and \\(k_2\\) are kernels, then we can show that, for any \\(x, y\\in\\mathcal{X}\\):\n\n\\(k_1(x, y)+k_2(x, y)\\)\n\\(k_1(x, y)*k_2(x, y)\\)\nFor \\(a\\in\\mathbb{R}\\), such that \\(ak_1(x,y)\\)\nFor any function \\(f:\\mathcal{X}\\rightarrow\\mathcal{X}'\\) (can be neural network or any kind of functions) and kernel \\(k':\\mathcal{X}'\\times\\mathcal{X}'\\rightarrow\\mathbb{R}\\), such that \\(k'(\\phi(x), \\phi(y))\\)\n\nare all kernel. With this would means that the following function \\(k(x, x') = (c + \\langle x, x'\\rangle)^m\\) is also a kernel, or if we have a function that admits Taylor series \\(f\\) (with convergences properties etc.), then \\(f(\\langle x, x'\\rangle)\\) is also a kernel.\nNow, we are ready to define the RKHS, in which it is a special Hilbert space with a special kind of kernel that satisfies additional\n\nDefinition (Reproducing Kernel Hilber Space): Given a Hilbert space \\(\\mathcal{H}\\) of \\(\\mathbb{R}\\) valued functions on non-empty set \\(\\mathcal{X}\\), the kernel \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}\\) is called reproducing and \\(\\mathcal{H}\\) is called RKHS if:\n\nFor all \\(x\\in\\mathcal{X}\\), \\(k(\\cdot, x) \\in\\mathcal{H}\\), then \\(k(\\cdot, x)\\in\\mathcal{H}\\)\nFor all \\(x\\in\\mathcal{X}\\), \\(\\langle{f(\\cdot), k(\\cdot,x)\\rangle}_\\mathcal{H} = f(x)\\)\n\n\nGiven the defintion, one can see that:\n\\[\n\\langle k(\\cdot, x), k(\\cdot, y)\\rangle_\\mathcal{H} = k(x, y)\n\\]\nwhich means that \\(k(\\cdot,x)\\) for any \\(x\\in\\mathcal{X}\\) can be seen as the feature map (recall that it doesn’t have to be unique), we will call this a canonical feature map.\nWe also have the follows result that illustrate why RKHS is preferable compared to the normal HS of functions.\n\nAdvanced Topics: Intuitively, we just say that the functions in RKHS acts “smoothly” and “predictably”, in the sense that: - If the distance between functions \\(\\|f-g\\|_\\mathcal{H}\\) is close to each other then its pointwise evaluation \\(|f(x)-g(x)|\\) for any \\(x\\) would also be close to each other. - This can be shown by the fact that the HS \\(\\mathcal{H}\\) has reproducing kernel iff the evaluation operator (defined as \\(\\delta_x : \\mathcal{H}\\rightarrow\\mathbb{R}\\) where \\(\\delta_x(f)=f(x)\\)) is bounded i.e \\(|\\delta_x(f)|\\le \\lambda_x\\|f\\|_\\mathcal{H}\\) for positive constant \\(\\lambda_x\\in\\mathbb{R}\\) (the proof uses the Riesz representation theorem)\n\nFurthermore, if the kernel satisfies the special property, then there is going to be an RHKS that is equipped with the given kernel as:\n\nTheorem (Moore-Aronszajn): A symmetric function \\(k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow \\mathbb{R}\\) is positive definite if: for all \\(a_1,a_2,\\dots,a_n\\in \\mathbb{R}\\) and for all \\(x_1,x_2,\\dots,x_n\\in\\mathcal{X}\\): \\[\\sum^n_{i=1}\\sum^n_{j=1}a_ia_jk(x_i, x_j)\\ge0\\] If the kernel is positive definite, then there is a unique RKHS with the reproducing kernel \\(k\\)."
  },
  {
    "objectID": "posts/2-kernel-test/index.html#interlude-why-kernel-in-statistical-testing",
    "href": "posts/2-kernel-test/index.html#interlude-why-kernel-in-statistical-testing",
    "title": "Kernel Statistical Test",
    "section": "",
    "text": "Given the sample \\((x_i)^m_{i=1}\\sim p\\) and \\((y_i)^m_{i=1}\\sim q\\). Given any feature extraction function \\(\\phi\\), one can find related kernel \\(k(\\cdot,\\cdot)\\) to be: \\(k(a, b)=\\langle \\phi(a), \\phi(b)\\rangle\\). Therefore, the distance between their mean in a feature space of the kernel \\(k(\\cdot,\\cdot)\\) can be computed as:\n\\[\n\\begin{aligned}\n\\Bigg\\| \\frac{1}{m}&\\sum^m_{i=1}\\phi(x_i) - \\frac{1}{n}\\sum^n_{i=1}\\phi(y_i) \\Bigg\\|^2 \\\\\n&= \\frac{1}{m^2}\\sum^m_{i=1}\\sum^m_{j=1}k(x_i,x_j) + \\frac{1}{n^2}\\sum^n_{i=1}\\sum^n_{j=1}k(y_i, y_j) - \\frac{2}{mn}\\sum^m_{i=1}\\sum^n_{j=1}k(x_i, y_i)\n\\end{aligned}\n\\]\nWe can observe 2 things here: 1. If we set the feature extraction function to be \\(\\phi(a)=[a \\ a^2]\\), then we are able to compare both means and variance. 2. One can set the feature extraction function to be arbitrary, as long as one can find the appropriate corresponding kernel (that should be easier to compute than just an inner product of each other). For instance, with RBF, one can have feature extraction function with infinite features! (via Taylor series).\nTherefore, intuitively, we can perform a more power/non-linear relationship between samples. Let’s now move to the actual formulation of the statistical testing."
  },
  {
    "objectID": "posts/2-kernel-test/index.html#statistical-testing-with-kernel-method",
    "href": "posts/2-kernel-test/index.html#statistical-testing-with-kernel-method",
    "title": "Kernel Statistical Test",
    "section": "",
    "text": "In this section, we are going to given the description of 2 main statistical testing technique that relies on the kernel method: MMD and HSIC (together with its variations). Let’s start with some operators that will be useful for both.\n\n\nGiven the example above in the interlude, we can generalizes the mean of the features map given an element \\(x\\sim P\\), as follows.\n\nDefinition (Mean Embedding): Given positive definite kernel \\(k(x,x')\\) with probability distribution \\(P\\) and \\(Q\\), we define \\(\\mu_P\\) and \\(\\mu_Q\\) such that: \\[\\langle{\\mu_P, \\mu_Q\\rangle} = \\mathbb{E}_{P, Q}[k(x, y)]\\] where \\(x\\sim P\\) and \\(y \\sim Q\\). We can consider the expectation in an RKHS as \\(\\mathbb{E}_P[f(x)] = \\langle{f, \\mu_P}\\rangle_\\mathcal{H}\\) for any function \\(f\\in\\mathcal{H}\\), the function in the corresponding RKHS\n\nWith this, one can see that the empirical mean embedding can be given in the form of:\n\\[\n\\hat{\\mu}_P = \\frac{1}{m}\\sum^m_{i=1}\\phi(x_i) \\qquad \\text{ where } \\qquad x_i\\sim P\n\\]\nIn which, one can show that this element exists.\n\nTheorem: The element \\(\\mu_P\\in\\mathcal{F}\\) defined as \\[\\mathbb{E}_P[f(x)] = \\langle{f, \\mu_P}\\rangle_\\mathcal{H}\\] if the kernel \\(k\\) of RKHS has the property that \\(\\mathbb{E}_P[\\sqrt{k(x, x)}]&lt;\n\\infty\\)\n\nProof Sketch: We can use the Riesz representation theorem by showing that the operator \\(T_Pf=\\mathbb{E}_P[f(x)]\\) is boudned, and thus there is \\(\\mu_P\\) such that \\(T_Pf=\\langle f,\\mu_P\\rangle\\)).\n\n\n\nLet’s formally define the notion of MMD, which tries to answer the question, does the samples \\(\\{x_i\\}^n_{i=1}\\) and \\(\\{y_i\\}^n_{i=1}\\) comes from the same distribution or not ?\n\nDefinition (MMD): Now, we define the quantity of MMD being the distance between \\(2\\) probability distributions \\(P\\) and \\(Q\\) as (together with its, more computable form) \\[ \\begin{aligned} \\operatorname{MMD}^2&(P, Q) = \\|\\mu_P-\\mu_Q\\|^2_\\mathcal{F} \\\\ &= \\mathbb{E}_P[k(x, x')] + \\mathbb{E}_Q[k(y, y')] - 2\\mathbb{E}_{P, Q}[k(x, y)] \\end{aligned} \\] whereby, we have the following unbiased estimate of its quantity: \\[ \\widehat{\\operatorname{MMD}}^2(P, Q) = \\frac{1}{n(n-1)}\\sum_{i\\ne j}k(x_i, x_j) + \\frac{1}{n(n-1)}\\sum_{i\\ne j}k(y_i, y_j) - \\frac{2}{n^2}\\sum_{i,j}k(x_i, y_j) \\] for \\(x_i\\sim P\\) and \\(y_i\\sim Q\\)\n\nYou may wonder, why does MMD is called maximum mean discrepancy ? One can show MMD can be written in an alternative form of:\n\nTheorem: We can show that the MMD can be written in an alternative form of: \\[\\operatorname{MMD}(P, Q) = \\sup_{\\|f\\|\\le1}\\big(\\mathbb{E}_P[f(x)] - \\mathbb{E}_Q[f(x)]\\big)\\]\n\nThis can be interpreted as, given “smooth” function within a ball (therefore not being too extream), we find such a function that maximally distingush the sample of \\(P\\) and \\(Q\\), and the maximum disagreement is the MMD value.\nNow, back to the statistical testing, we can show that the value of MMD will have the following asympototics distribution of:\n\nTheorem: We have the following distribution of the empirical MMD statistics as follows: - When \\(P\\ne Q\\), we have: \\[\\frac{\\widehat{\\operatorname{MMD}}^2 - \\operatorname{MMD}(P, Q)^2}{\\sqrt{V_n(P, Q)}} \\xrightarrow{D} \\mathcal{N}(0, 1)\\]where the variance \\(V_n(P, Q) = \\mathcal{O}(n^{-1})\\) but depends on the chosen kernel. - When \\(P=Q\\), we have: \\[n\\widehat{\\operatorname{MMD}}^2 \\sim \\sum^\\infty_{l=1} \\lambda_l[z^2_l - 2] \\qquad \\text{ where } \\qquad \\lambda_i\\phi_i(x) = \\int_\\mathcal{X}\\widetilde{k}(x,\\widetilde{x})\\phi_i(x)\\text{ d}P(x)\\]where \\(\\widetilde{k}\\) is a centered kernel and \\(z_l\\sim\\mathcal{N}(0, 2)\\)\n\nHowever, to compute such a distribution with null-hypothesis \\(P=Q\\) in closed form is hard, therefore:\n\nWe have to rely on using a boostrap method which is done by permuting the set \\(X\\) and \\(Y\\) before testing (i.e mixing them up)\nThis would gives us the estimate of the MMD statistics when \\(P=Q\\), which can them be used to compute the threshold for statistical test.\n\nNow, to find a best kernel, we have that:\n\nRemark (Finding a best kernek): Given the distribution when \\(P=Q\\), one can see that the power of the test is given to be: \\[\\text{Pr}_1\\left({n\\widehat{\\operatorname{MMD}} &gt; \\hat{c}_\\alpha }\\right) \\rightarrow  \\Phi\\left({\\frac{\\operatorname{MMD}^2(P, Q)}{\\sqrt{V_n(P, Q)}} - \\frac{c_\\alpha}{n\\sqrt{V_n(P, Q)}} }\\right)\\] To find the best kernel, we can find the kernel that maximize the test power. We would like to note the following: \\[\\frac{\\operatorname{MMD}^2(P, Q)}{\\sqrt{V_n(P, Q)}} = \\mathcal{O}(\\sqrt{n}) \\qquad \\frac{c_\\alpha}{n\\sqrt{V_n(P, Q)}} =\\mathcal{O}(n^{-1/2})\\] therefore, we can ignore the second term, and we can maximize the first term only, by setting this to be the objective of the neural network (that perform the feature extraction of the kernel). Note that we can derive the estimator for \\(V_n\\) too.\n\nFor example, one can use the following kernel from\n\\[\nk_\\theta(x, y) = \\big[(1-\\varepsilon)\\kappa(\\Phi_\\theta(x), \\Phi_\\theta(y))+\\varepsilon\\big]q(x, y)\n\\]\nwhere \\(\\Phi_\\theta\\) is a neural network and \\(\\kappa\\) and \\(q\\) are Gaussian kernel, which is able to distinguish between CIFAR-10 vs CIFAR-10 (image dataset).\nNow come the more important question, can we use any kernel to give us the appropriate MMD test ? The answer is obviously no, but what kind of kernel would be approriate ? Starting with a defintion of a good kernel (or we will call it characteristic):\n\nDefinition (Charateristic kernel): A RKHS (with corresponding kernel) is called characteristic if \\(\\operatorname{MMD}(P, Q; \\mathcal{F}) = 0\\) iff \\(P = Q\\)\n\nThat is when \\(P\\) and \\(Q\\) are the same, the value of MMD should be zero. What would be an appropriate kernel ? In this case, we would like to assume that kernel that we are working on is Translation Invariance i.e\n\nDefinition (Translation Invariance): The kernel \\(k\\) is called Translation Invariance if there is a function \\(f\\) such that: \\[k(x,y)=f(x-y)\\] for any \\(x\\) and \\(y\\)\n\nThen, one can have a fourier representation/coefficient of the kernel to be (assume we are within the domain of \\([-\\pi,\\pi]\\)) the multiple within the fourier series expansion:\n\\[\nk(x, y) = \\sum^\\infty_{l=-\\infty} \\hat{k}_l \\exp(il(x-y)) = \\sum^\\infty_{l=-\\infty}\\underbrace{\\left[{\\sqrt{\\hat{k}_l} \\exp(ilx) }\\right]}_{\\phi_l(x)}\\underbrace{\\left[{\\sqrt{\\hat{k}_l}\\exp(-ily)}\\right]}_{\\overline{\\phi_l(y)}}\n\\]\n\\(\\hat{k}_l\\) is called the fourier coefficient of the kernel. For the probability distribution, one can also have a similar way to find the fourier coefficient of them. We have the following result.\n\nTheorem: The value of MMD can be written as: \\[\\operatorname{MMD}^2(P, Q;\\mathcal{F}) = \\sum^\\infty_{l=-\\infty} |\\phi_{P,l} - \\phi_{Q, l}|^2\\hat{k}_l\\] for \\(\\hat{k}_l\\) being the fourier coefficient of the kernel, \\(\\phi_{P,l}\\) and \\(\\phi_{Q,l}\\) are fourier coefficient of the probability distributions \\(P\\) and \\(Q\\), respectively.\n\nTherefore, the kernel is characterisic iff none of the \\(\\hat{k}_l\\) is equal to zero.\nOn the other hand, instead of considering within specific range \\([\\pi,-\\pi]\\), one can also define the RKHS to be universal, which is when:\n\nDefinition (Universal RKHS): Given RKHS, it is universal if when: - \\(k(x, x')\\) is continuous - \\(\\mathcal{X}\\) is compact. - \\(\\mathcal{F}\\) is dense in \\(C(\\mathcal{X})\\) wrt. \\(L_\\infty\\) i.e for \\(\\varepsilon&gt;0\\) and \\(f\\in C(\\mathcal{X})\\), there is \\(g\\in\\mathcal{F}\\) such that: \\[\\|f-g\\|_\\infty\\le\\varepsilon\\]\n\nin which we can show that:\n\nTheorem: If \\(\\mathcal{F}\\) is universal then \\(\\operatorname{MMD}(P, Q;\\mathcal{F}) = 0\\) iff \\(P = Q\\)"
  },
  {
    "objectID": "posts/2-kernel-test/index.html#hilbert-schmidt-indepdent-criterion",
    "href": "posts/2-kernel-test/index.html#hilbert-schmidt-indepdent-criterion",
    "title": "Kernel Statistical Test",
    "section": "",
    "text": "Now, we are interested in given a pair of variables \\(\\{(x_i, y_i)\\}^n_{i=1}\\sim P_{XY}\\) are they dependent of each other ? - Usually one can use the MMD to find the differences whether this sample is sampled from the \\(P_XP_Y\\) (i.e product of marginal distribution). However, we don’t have an access to this. - Another question is: which kind of kernel would we be use ? is it a product kernel ? or different kind of kernels\n\n\nWe start off by defining the tensor product between elements in the Hilber space.\n\nDefinition (Tensor Product): Given element \\(a,b,c\\in\\mathcal{H}\\) of the Hilbert space, the tensor product between \\(a\\) and \\(b\\) is denoted as \\(a\\otimes b\\) such that: \\[(a\\otimes b)c = \\langle b,c\\rangle_\\mathcal{H}a\\] Note that this is analogous to when \\(a,b\\) and \\(c\\) are vector, then \\((ab^\\top)c=b^\\top ca\\)\n\nNow, we would like to extends the notion of the inner product (and norm) to the linear transformation between Hilbert space. This would gives us Hilbert-Schmidt Operators i.e\n\nDefinition (Hilbert-Schmidt Operators): Given a separable (countable orthonormal basis Hilbert spaces \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\) with orthonormal basis \\((f_i)_{i\\in I}\\) and \\((g_j)_{j\\in I}\\), respectively and 2 linear transformation between them: \\(L:\\mathcal{G}\\rightarrow\\mathcal{F}\\) and \\(M:\\mathcal{G}\\rightarrow\\mathcal{F}\\): \\[\\langle{L, M}\\rangle_{\\operatorname{HS}} = \\sum_{j\\in J}\\langle{Lg_j, Mg_j}\\rangle_\\mathcal{F}\\]\n\nNow, we can define the covariance operator (in similar manners to the mean embedding) as:\n\nDefinition (Covariance Operator): The covariance operators \\(C_{xy} : \\mathcal{G} \\rightarrow \\mathcal{F}\\) is given by: \\[\\langle{f, C_{xy}g}\\rangle_\\mathcal{F} = \\mathbb{E}_{xy}[f(x)g(y)]\\] which we can show to exists if the kernel associated \\(\\mathcal{G}\\) and \\(\\mathcal{F}\\): \\(k_1\\) and \\(k_2\\), respectively, are such that \\(k_1(x,x) &lt; \\infty\\) and \\(k_2(y,y)&lt;\\infty\\)\n\nThe existances can be proven by observe that, for any linear operator \\(A:\\mathcal{G} \\rightarrow \\mathcal{F}\\), we have:\n\\[\n\\langle{C_{xy}, A}\\rangle_{\\operatorname{HS}} = \\mathbb{E}_{xy}\\big[\\langle\\psi(x)\\otimes\\phi(y), A\\rangle_{\\operatorname{HS}}\\big]\n\\]\nand so we can use Riesz representation thoerem to proof the existence. Then, we are ready to define the HSIC\n\n\n\n\nDefinition (Hilbert-Schmidt Indepdent Criterion): The HSIC can be seen as the norm of the centered covariance operator i.e: \\[\\operatorname{HSIC}(P_{XY};\\mathcal{F}, \\mathcal{G}) = \\|{C_{xy} - \\mu_x\\otimes\\mu_y}\\|_{\\operatorname{HS}} = \\|{\\widetilde{C}_{xy}}\\|_{\\operatorname{HS}}\\]\n\nIn relation to MMD, we can show that"
  },
  {
    "objectID": "posts/3-yoneda-explore/index.html",
    "href": "posts/3-yoneda-explore/index.html",
    "title": "Yoneda Lemma: An Exploration (WIP)",
    "section": "",
    "text": "Required Knowledge: some conformation with what category is, for the latter section, you would need to understand the notion of functor and natural transformation.\n\nProbing Action"
  }
]